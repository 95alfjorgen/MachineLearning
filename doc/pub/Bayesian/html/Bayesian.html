<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Data Analysis and Machine Learning: Elements of Bayesian theory">

<title>Data Analysis and Machine Learning: Elements of Bayesian theory</title>


<style type="text/css">
/* bloodish style */

body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em;  color: #8A0808; }
h2 { font-size: 1.6em;  color: #8A0808; }
h3 { font-size: 1.4em;  color: #8A0808; }
h4 { color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
/* pre style removed because it will interfer with pygments */
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('What is Bayesian Statistics', 2, None, '___sec0'),
              ('Inference', 2, None, '___sec1'),
              ('Statistical Inference', 2, None, '___sec2'),
              ('Some history', 2, None, '___sec3'),
              ('The Bayesian recipe', 2, None, '___sec4'),
              ("Bayes' theorem", 2, None, '___sec5'),
              ("The friends of Bayes' theorem", 2, None, '___sec6'),
              ('Inference With Parametric Models', 2, None, '___sec7'),
              ('Illustrative examples with python code', 2, None, '___sec8')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>Data Analysis and Machine Learning: Elements of Bayesian theory</h1></center>  <!-- document title -->

<p>
<!-- author(s): Christian Forss&#233;n, and Morten Hjorth-Jensen -->

<center>
<b>Christian Forss&#233;n</b> [1]
</center>

<center>
<b>Morten Hjorth-Jensen</b> [2, 3]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, Chalmers University of Technology, Sweden</b></center>
<center>[2] <b>Department of Physics, University of Oslo</b></center>
<center>[3] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>Mar 10, 2018</h4></center> <!-- date -->
<br>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec0">What is Bayesian Statistics  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Morten's original plan: Reminder about probabilities from the statistics section

<ol>
<li> Product rule</li>
<li> Binomial distribution</li>
<li> Gaussian PDF</li>
<li> other PDFs</li>
<li> Bayesian regression analysis</li>
</ol>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec1">Inference  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<dl>
<dt>Inference:<dd> 
  &quot;the act of passing from one proposition, statement or judgment considered as true to another whose truth is believed to follow from that of the former&quot; (Webster) <br />
  Do premises \( A, B, \ldots \to \) hypothesis, \( H \)? 
<dt>Deductive inference:<dd> 
  Premises allow definite determination of truth/falsity of H (syllogisms, symbolic logic, Boolean algebra) <br />
  \( B(H|A,B,...) = 0 \) or \( 1 \)
<dt>Inductive inference:<dd> 
  Premises bear on truth/falsity of H, but don&#8217;t allow its definite determination (weak syllogisms, analogies)<br />
  \( A, B, C, D \) share properties \( x, y, z \); \( E \) has properties \( x, y \)<br />
  \( \to \) $E$ probably has property \( z \).
</dl>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec2">Statistical Inference  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> Quantify the strength of inductive inferences from facts, in the form of data (\( D \)), and other premises, e.g. models, to hypotheses about the phenomena producing the data.</li>
<li> Quantify via probabilities, or averages calculated using probabilities. Frequentists (\( \mathcal{F} \)) and Bayesians (\( \mathcal{B} \)) use probabilities very differently for this.</li>
<li> To the pioneers such as Bernoulli, Bayes and Laplace, a probability represented a <em>degree-of-belief</em> or plausability: how much they thought that something as true based on the evidence at hand. This is the Bayesian approach.</li>
<li> To the 19th century scholars, this seemed too vague and subjective. They redefined probability as the <em>long run relative frequency</em> with which an event occurred, given (infinitely) many repeated (experimental) trials.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec3">Some history </h2>
Adapted from D.S. Sivia [<a id="link_footnote_1" href="#def_footnote_1">1</a>]:

<p id="def_footnote_1"><a href="#link_footnote_1"><b>1:</b></a> Sivia, Devinderjit, and John Skilling. Data Analysis : A Bayesian Tutorial, OUP Oxford, 2006</p>

<p>
<blockquote>
    Although the frequency definition appears to be more objective, its range of validity is also far more limited. For example, Laplace used (his) probability theory to estimate the mass of Saturn, given orbital data that were available to him from various astronomical observatories. In essence, he computed the posterior pdf for the mass M , given the data and all the relevant background information I (such as a knowledge of the laws of classical mechanics): prob(M|{data},I); this is shown schematically in the figure [Fig. 1.2].
</blockquote>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<br /><br /><center><p><img src="fig/sivia_fig_1_2.png" align="bottom" width=700></p></center><br /><br />

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<blockquote>
    To Laplace, the (shaded) area under the posterior pdf curve between \( m_1 \) and \( m_2 \) was a measure of how much he believed that the mass of Saturn lay in the range \( m_1 \le M \le m_2 \). As such, the position of the maximum of the posterior pdf represents a best estimate of the mass; its width, or spread, about this optimal value gives an indication of the uncertainty in the estimate. Laplace stated that: &#8216; . . . it is a bet of 11,000 to 1 that the error of this result is not 1/100th of its value.&#8217; He would have won the bet, as another 150 years&#8217; accumulation of data has changed the estimate by only 0.63%!
</blockquote>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<blockquote>
    According to the frequency definition, however, we are not permitted to use probability theory to tackle this problem. This is because the mass of Saturn is a constant and not a random variable; therefore, it has no frequency distribution and so probability theory cannot be used.
    
    <p>
    If the pdf [of Fig. 1.2] had to be interpreted in terms of the frequency definition, we would have to imagine a large ensemble of universes in which everything remains constant apart from the mass of Saturn.
</blockquote>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<blockquote>
    As this scenario appears quite far-fetched, we might be inclined to think of [Fig. 1.2] in terms of the distribution of the measurements of the mass in many repetitions of the experiment. Although we are at liberty to think about a problem in any way that facilitates its solution, or our understanding of it, having to seek a frequency interpretation for every data analysis problem seems rather perverse.
    For example, what do we mean by the &#8216;measurement of the mass&#8217; when the data consist of orbital periods? Besides, why should we have to think about many repetitions of an experiment that never happened? What we really want to do is to make the best inference of the mass given the (few) data that we actually have; this is precisely the Bayes and Laplace view of probability.
</blockquote>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<blockquote>
    Faced with the realization that the frequency definition of probability theory did not permit most real-life scientific problems to be addressed, a new subject was invented &#8212; statistics! To estimate the mass of Saturn, for example, one has to relate the mass to the data through some function called the statistic; since the data are subject to &#8216;random&#8217; noise, the statistic becomes the random variable to which the rules of probability the- ory can be applied. But now the question arises: How should we choose the statistic? The frequentist approach does not yield a natural way of doing this and has, therefore, led to the development of several alternative schools of orthodox or conventional statis- tics. The masters, such as Fisher, Neyman and Pearson, provided a variety of different principles, which has merely resulted in a plethora of tests and procedures without any clear underlying rationale. This lack of unifying principles is, perhaps, at the heart of the shortcomings of the cook-book approach to statistics that students are often taught even today.
</blockquote>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec4">The Bayesian recipe </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Assess hypotheses by calculating their probabilities \( p(H_i | \ldots) \) conditional on known and/or presumed information using the rules of probability theory.
</div>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Probability Theory Axioms:

<dl>
<dt>Product (AND) rule :<dd> 
  \( p(A, B | I) = p(A|I) p(B|A, I) = p(B|I)p(A|B,I) \)<br />
  Should read \( p(A,B|I) \) as the probability for propositions \( A \) AND \( B \) being true given that \( I \) is true.
<dt>Sum (OR) rule:<dd> 
  \( p(A + B | I) = p(A | I) + p(B | I) - p(A, B | I) \)<br />
  \( p(A+B|I) \) is the probability that proposition \( A \) OR \( B \) is true given that \( I \) is true.
<dt>Normalization:<dd> 
  \( p(A|I) + p(\bar{A}|I) = 1 \)<br />
  \( \bar{A} \) denotes the proposition that \( A \) is false.
</dl>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec5">Bayes' theorem </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Bayes' theorem follows directly from the product rule
$$
$$
p(A|B,I) = \frac{p(B|A,I) p(A|I)}{p(B|I)}.
$$
$$

The importance of this property to data analysis becomes apparent if we replace \( A \) and \( B \) by hypothesis(\( H \)) and data(\( D \)):
$$
$$
p(H|D,I) = \frac{p(D|H,I) p(H|I)}{p(D|I)}.
$$
$$

The power of Bayes&#8217; theorem lies in the fact that it relates the quantity of interest, the probability that the hypothesis is true given the data, to the term we have a better chance of being able to assign, the probability that we would have observed the measured data if the hypothesis was true.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The various terms in Bayes&#8217; theorem have formal names. 

<ul>
<li> The quantity on the far right, \( p(H|I) \), is called the <em>prior</em> probability; it represents our state of knowledge (or ignorance) about the truth of the hypothesis before we have analysed the current data.</li> 
<li> This is modified by the experimental measurements through \( p(D|H,I) \), the <em>likelihood</em> function,</li> 
<li> The denominator \( p(D|I) \) is called the <em>evidence</em>. It does not depend on the hypothesis and can be regarded as a normalization constant.</li>
<li> Together, these yield the <em>posterior</em> probability, \( p(H|D, I ) \), representing our state of knowledge about the truth of the hypothesis in the light of the data.</li> 
</ul>

In a sense, Bayes&#8217; theorem encapsulates the process of learning.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec6">The friends of Bayes' theorem </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<dl>
<dt>Normalization:<dd> 
  \( \sum_i p(H_i|\ldots) = 1 \).
<dt>Marginalization:<dd> 
  \( \sum_i p(A,H_i|I) = \sum_i p(H_i|A,I) p(A|I) = p(A|I) \).
<dt>Marginalization (continuum limit):<dd> 
  \( \int dx p(A,H(x)|I) = p(A|I) \).
</dl>

In the above, \( H_i \) is an exclusive and exhaustive list of hypotheses. For example,let&#8217;s imagine that there are five candidates in a presidential election; then \( H_1 \) could be the proposition that the first candidate will win, and so on. The probability that \( A \) is true, for example that unemployment will be lower in a year&#8217;s time (given all relevant information \( I \), but irrespective of whoever becomes president) is then given by \( \sum_i p(A,H_i|I) \).

<p>
In the continuum limit of propositions we must understand \( p(\ldots) \) as a pdf (probability density function).

<p>
Marginalization is a very powerful device in data analysis because it enables us to deal with nuisance parameters; that is, quantities which necessarily enter the analysis but are of no intrinsic interest. The unwanted background signal present in many experimental measurements are examples of nuisance parameters.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec7">Inference With Parametric Models  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Inductive inference with parametric models is a very important tool in the natural sciences.

<ul>
<li> Consider \( N \) different models \( M_i \) (\( i = 1, \ldots, N \)), each with parameters \( \boldsymbol{\alpha}_i \). Each of them implies a sampling distribution (conditional predictive distribution for possible data)</li>
</ul>

$$
$$
p(D|\boldsymbol{\alpha}_i, M_i)
$$
$$


<ul>
<li> The \( \boldsymbol{\alpha}_i \) dependence when we fix attention on the actual, observed data (\( D_\mathrm{obs} \)) is the likelihood function:</li>
</ul>

$$
$$
\mathcal{L}_i (\boldsymbol{\alpha}_i) \equiv p(D_\mathrm{obs}|\boldsymbol{\alpha}_i, M_i)
$$
$$


<ul>
<li> We may be uncertain about \( i \) (model uncertainty),</li>
<li> or uncertain about \( \boldsymbol{\alpha}_i \) (parameter uncertainty).</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<dl>
<dt>Parameter Estimation:<dd> 
  Premise = choice of model (pick specific \( i \))<br />
  \( \Rightarrow \) What can we say about \( \boldsymbol{\alpha}_i \)?
<dt>Model comparison:<dd> 
  Premise = \( \{M_i\} \)<br />
  \( \Rightarrow \) What can we say about \( i \)?
<dt>Model adequacy:<dd> 
  Premise = \( M_1 \)<br />
  \( \Rightarrow \) Is \( M_1 \) adequate?
<dt>Hybrid Uncertainty:<dd> 
  Models share some common params: \( \boldsymbol{\alpha}_1 = \{ \boldsymbol{\varphi}, \boldsymbol{\eta}_i\} \)<br />
  \( \Rightarrow \) What can we say about \( \boldsymbol{\varphi} \)? (Systematic error is an example)
</dl>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec8">Illustrative examples with python code  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> Is this a fair coin? (analytical)</li>
<li> Flux from a star (single parameter, MCMC)</li>
<li> The lighthouse problem (two parameters, MCMC)</li>
<li> Linear fit with outliers (nuisance parameters)</li>
<li> ...</li>
</ul>
</div>


<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2018, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

