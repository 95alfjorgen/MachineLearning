{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Data Analysis and Machine Learning: Support Vector Machines -->\n",
    "# Data Analysis and Machine Learning: Support Vector Machines\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Nov 3, 2018**\n",
    "\n",
    "Copyright 1999-2018, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license\n",
    "\n",
    "\n",
    "\n",
    "## Support Vector Machines, overarching aims\n",
    "\n",
    "A Support Vector Machine (SVM) is a very powerful and versatile\n",
    "Machine Learning model, capable of performing linear or nonlinear\n",
    "classification, regression, and even outlier detection. It is one of\n",
    "the most popular models in Machine Learning, and anyone interested in\n",
    "Machine Learning should have it in their toolbox. SVMs are\n",
    "particularly well suited for classification of complex but small-sized or\n",
    "medium-sized datasets.  \n",
    "\n",
    "The case with two well-separated classes only can be understood in an intuitive way in terms of lines in a two-dimensional space separating the two classes (see figure below).  \n",
    "\n",
    "The basic mathematics behind the SVM is however less familiar to most of us. \n",
    "It relies on the definition of hyperplanes and the\n",
    "definition of a **margin** which separates classes (in case of\n",
    "classification problems) of variables. It is also used for regression\n",
    "problems.\n",
    "\n",
    "With SVMs we distinguish between hard margin and soft margins. The latter introduces a so-called softening parameter to be discussed below.\n",
    "We distringuish also between linear and non-linear approaches. The latter are the most frequent ones since it is rather unlikely that we can separate classes easily by say straight lines. \n",
    "\n",
    "\n",
    "\n",
    "## Strength and weakness\n",
    "\n",
    "When we implement a linear support vector machine, the main parameter is the constant $C$. Small values of $C$ mean simple models.\n",
    "These models are fast to train and also fast to predict and scale to very large data sets and work well with sparse data. Linear support vector machines make it easy to understand how a prediction is made, however it is often not easy to understand why coefficients are the way they are.  These models work also well in higer dimensions. \n",
    "\n",
    "\n",
    "## Hyperplanes and all that\n",
    "\n",
    "The theory behind support vector machines (SVM hereafter) is based on\n",
    "the mathematical description of so-called hyperplanes. Let us start\n",
    "with a two-dimensional case. This will also allow us to introduce our\n",
    "first SVM examples. These will be tailored to the case of two specific\n",
    "classes, as displayed in the figure here.\n",
    "\n",
    "We assume here that our data set can be well separated into two\n",
    "domains, where a straight line does the job in the separating the two\n",
    "classes. Here the two classes are represented by either crosses or\n",
    "circles.\n",
    "\n",
    "## What is a hyperplane\n",
    "\n",
    "The aim of the SVM algorithm is to find a hyperplane in an $n$-dimensional space, where $n$ is the number of features  that distinctly classifies the data points.  \n",
    "\n",
    "In an $n$-dimensional space, a hyperplane is what we call an affine subspace of dimension of $n-1$.\n",
    "As an example, in two dimension, a hyperplane is simply as straight line while in three dimensions it is \n",
    "a two-dimensional subspace, or stated simply, a plane. \n",
    "\n",
    "In two dimensions, with the variables $x_1$ and $x_2$, the hyperplane is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_0+\\beta_1x_1+\\beta_2x_2=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an $n$-dimensional space we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_0+\\beta_1x_1+\\beta_2x_2+\\dots +\\beta_nx_n=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\hat{x}=\\left[x_1,x_2,\\dots, x_n\\right]$, if the above condition is not met and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_0+\\beta_1x_1+\\beta_2x_2+\\dots +\\beta_nx_n<0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we say that $\\hat{x}$ lies on one of the sides of the hyperplane and if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_0+\\beta_1x_1+\\beta_2x_2+\\dots +\\beta_nx_n>0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then $\\hat{x}$ lies on the other side. \n",
    "\n",
    "<!-- !split  -->\n",
    "## The two-dimensional case\n",
    "\n",
    "Let us try to develop our intuition about SVMs by limiting ourselves to a two-dimensional\n",
    "plane.  To separate the two classes of data points, there are many\n",
    "possible lines (hyperplanes if you prefer a more strict naming)  \n",
    "that could be chosen. Our objective is to find a\n",
    "plane that has the maximum margin, i.e the maximum distance between\n",
    "data points of both classes. Maximizing the margin distance provides\n",
    "some reinforcement so that future data points can be classified with\n",
    "more confidence.\n",
    "\n",
    "What a linear classifier attempts to accomplish is to split the\n",
    "feature space into two half spaces by placing a hyperplane between the\n",
    "data points.  This hyperplane will be our decision boundary.  All\n",
    "points on one side of the plane will belong to class one and all points\n",
    "on the other side of the plane will belong to the second class two.\n",
    "\n",
    "Unfortunately there are many ways in which we can place a hyperplane\n",
    "to divide the data.  Below is an example of two candidate hyperplanes\n",
    "for our data sample.\n",
    "\n",
    "## Examples with kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "from cycler import cycler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "y = y % 2\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add the squared first feature\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "figure = plt.figure()\n",
    "# visualize in 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "# plot first all the points with y==0, then all with y == 1\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# show linear decision boundary\n",
    "figure = plt.figure()\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")\n",
    "\n",
    "ZZ = YY ** 2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "             cmap=mglearn.cm2, alpha=0.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X, y = mglearn.tools.make_handcrafted_dataset()                                                                  \n",
    "svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n",
    "mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "# plot support vectors\n",
    "sv = svm.support_vectors_\n",
    "# class labels of support vectors are given by the sign of the dual coefficients\n",
    "sv_labels = svm.dual_coef_.ravel() > 0\n",
    "mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, C in zip(axes, [-1, 0, 3]):\n",
    "    for a, gamma in zip(ax, range(-1, 2)):\n",
    "        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "        \n",
    "axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n",
    "                  ncol=4, loc=(.9, 1.2))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
