{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Data Analysis and Machine Learning: Getting started, our first data and Machine Learning encounters -->\n",
    "# Data Analysis and Machine Learning: Getting started, our first data and Machine Learning encounters\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Dec 25, 2019**\n",
    "\n",
    "Copyright 1999-2019, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Our emphasis throughout this series of lectures  \n",
    "is on understanding the mathematical aspects of\n",
    "different algorithms used in the fields of data analysis and machine learning. \n",
    "\n",
    "However, where possible we will emphasize the\n",
    "importance of using available software. We start thus with a hands-on\n",
    "and top-down approach to machine learning. The aim is thus to start with\n",
    "relevant data or data we have produced \n",
    "and use these to introduce statistical data analysis\n",
    "concepts and machine learning algorithms before we delve into the\n",
    "algorithms themselves. The examples we will use in the beginning, start with simple\n",
    "polynomials with random noise added. We will use the Python\n",
    "software package [Scikit-Learn](http://scikit-learn.org/stable/) and\n",
    "introduce various machine learning algorithms to make fits of\n",
    "the data and predictions. We move thereafter to more interesting\n",
    "cases such as data from say experiments (below we will look at experimental nuclear binding energies as an example).\n",
    "These are examples where we can easily set up the data and\n",
    "then use machine learning algorithms included in for example\n",
    "**Scikit-Learn**. \n",
    "\n",
    "These examples will serve us the purpose of getting\n",
    "started. Furthermore, they allow us to catch more than two birds with\n",
    "a stone. They will allow us to bring in some programming specific\n",
    "topics and tools as well as showing the power of various Python \n",
    "libraries for machine learning and statistical data analysis.  \n",
    "\n",
    "Here, we will mainly focus on two\n",
    "specific Python packages for Machine Learning, Scikit-Learn and\n",
    "Tensorflow (see below for links etc).  Moreover, the examples we\n",
    "introduce will serve as inputs to many of our discussions later, as\n",
    "well as allowing you to set up models and produce your own data and\n",
    "get started with programming.\n",
    "\n",
    "\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Statistics, data science and machine learning form important fields of\n",
    "research in modern science.  They describe how to learn and make\n",
    "predictions from data, as well as allowing us to extract important\n",
    "correlations about physical process and the underlying laws of motion\n",
    "in large data sets. The latter, big data sets, appear frequently in\n",
    "essentially all disciplines, from the traditional Science, Technology,\n",
    "Mathematics and Engineering fields to Life Science, Law, education\n",
    "research, the Humanities and the Social Sciences. \n",
    "\n",
    "It has become more\n",
    "and more common to see research projects on big data in for example\n",
    "the Social Sciences where extracting patterns from complicated survey\n",
    "data is one of many research directions.  Having a solid grasp of data\n",
    "analysis and machine learning is thus becoming central to scientific\n",
    "computing in many fields, and competences and skills within the fields\n",
    "of machine learning and scientific computing are nowadays strongly\n",
    "requested by many potential employers. The latter cannot be\n",
    "overstated, familiarity with machine learning has almost become a\n",
    "prerequisite for many of the most exciting employment opportunities,\n",
    "whether they are in bioinformatics, life science, physics or finance,\n",
    "in the private or the public sector. This author has had several\n",
    "students or met students who have been hired recently based on their\n",
    "skills and competences in scientific computing and data science, often\n",
    "with marginal knowledge of machine learning.\n",
    "\n",
    "Machine learning is a subfield of computer science, and is closely\n",
    "related to computational statistics.  It evolved from the study of\n",
    "pattern recognition in artificial intelligence (AI) research, and has\n",
    "made contributions to AI tasks like computer vision, natural language\n",
    "processing and speech recognition. Many of the methods we will study are also \n",
    "strongly rooted in basic mathematics and physics research. \n",
    "\n",
    "Ideally, machine learning represents the science of giving computers\n",
    "the ability to learn without being explicitly programmed.  The idea is\n",
    "that there exist generic algorithms which can be used to find patterns\n",
    "in a broad class of data sets without having to write code\n",
    "specifically for each problem. The algorithm will build its own logic\n",
    "based on the data.  You should however always keep in mind that\n",
    "machines and algorithms are to a large extent developed by humans. The\n",
    "insights and knowledge we have about a specific system, play a central\n",
    "role when we develop a specific machine learning algorithm. \n",
    "\n",
    "Machine learning is an extremely rich field, in spite of its young\n",
    "age. The increases we have seen during the last three decades in\n",
    "computational capabilities have been followed by developments of\n",
    "methods and techniques for analyzing and handling large date sets,\n",
    "relying heavily on statistics, computer science and mathematics.  The\n",
    "field is rather new and developing rapidly. Popular software packages\n",
    "written in Python for machine learning like\n",
    "[Scikit-learn](http://scikit-learn.org/stable/),\n",
    "[Tensorflow](https://www.tensorflow.org/),\n",
    "[PyTorch](http://pytorch.org/) and [Keras](https://keras.io/), all\n",
    "freely available at their respective GitHub sites, encompass\n",
    "communities of developers in the thousands or more. And the number of\n",
    "code developers and contributors keeps increasing. Not all the\n",
    "algorithms and methods can be given a rigorous mathematical\n",
    "justification, opening up thereby large rooms for experimenting and\n",
    "trial and error and thereby exciting new developments.  However, a\n",
    "solid command of linear algebra, multivariate theory, probability\n",
    "theory, statistical data analysis, understanding errors and Monte\n",
    "Carlo methods are central elements in a proper understanding of many\n",
    "of algorithms and methods we will discuss.\n",
    "\n",
    "\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "\n",
    "The approaches to machine learning are many, but are often split into\n",
    "two main categories.  In *supervised learning* we know the answer to a\n",
    "problem, and let the computer deduce the logic behind it. On the other\n",
    "hand, *unsupervised learning* is a method for finding patterns and\n",
    "relationship in data sets without any prior knowledge of the system.\n",
    "Some authours also operate with a third category, namely\n",
    "*reinforcement learning*. This is a paradigm of learning inspired by\n",
    "behavioral psychology, where learning is achieved by trial-and-error,\n",
    "solely from rewards and punishment.\n",
    "\n",
    "Another way to categorize machine learning tasks is to consider the\n",
    "desired output of a system.  Some of the most common tasks are:\n",
    "\n",
    "  * Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.\n",
    "\n",
    "  * Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.\n",
    "\n",
    "  * Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.\n",
    "\n",
    "The methods we cover have three main topics in common, irrespective of\n",
    "whether we deal with supervised or unsupervised learning. The first\n",
    "ingredient is normally our data set (which can be subdivided into\n",
    "training and test data), the second item is a model which is normally a\n",
    "function of some parameters.  The model reflects our knowledge of the system (or lack thereof). As an example, if we know that our data show a behavior similar to what would be predicted by a polynomial, fitting our data to a polynomial of some degree would then determin our model. \n",
    "\n",
    "The last ingredient is a so-called **cost**\n",
    "function which allows us to present an estimate on how good our model\n",
    "is in reproducing the data it is supposed to train.  \n",
    "At the heart of basically all ML algorithms there are so-called minimization algorithms, often we end up with various variants of **gradient** methods.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Software and needed installations\n",
    "\n",
    "We will make extensive use of Python as programming language and its\n",
    "myriad of available libraries.  You will find\n",
    "Jupyter notebooks invaluable in your work.  You can run **R**\n",
    "codes in the Jupyter/IPython notebooks, with the immediate benefit of\n",
    "visualizing your data. You can also use compiled languages like C++,\n",
    "Rust, Julia, Fortran etc if you prefer. The focus in these lectures will be\n",
    "on Python.\n",
    "\n",
    "\n",
    "If you have Python installed (we strongly recommend Python3) and you feel\n",
    "pretty familiar with installing different packages, we recommend that\n",
    "you install the following Python packages via **pip** as \n",
    "\n",
    "1. pip install numpy scipy matplotlib ipython scikit-learn mglearn sympy pandas pillow \n",
    "\n",
    "For Python3, replace **pip** with **pip3**.\n",
    "\n",
    "For OSX users we recommend, after having installed Xcode, to\n",
    "install **brew**. Brew allows for a seamless installation of additional\n",
    "software via for example \n",
    "\n",
    "1. brew install python3\n",
    "\n",
    "For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,\n",
    "you can use **pip** as well and simply install Python as \n",
    "\n",
    "1. sudo apt-get install python3  (or python for pyhton2.7)\n",
    "\n",
    "etc etc. \n",
    "\n",
    "\n",
    "\n",
    "## Python installers\n",
    "\n",
    "If you don't want to perform these operations separately and venture\n",
    "into the hassle of exploring how to set up dependencies and paths, we\n",
    "recommend two widely used distrubutions which set up all relevant\n",
    "dependencies for Python, namely \n",
    "\n",
    "* [Anaconda](https://docs.anaconda.com/), \n",
    "\n",
    "which is an open source\n",
    "distribution of the Python and R programming languages for large-scale\n",
    "data processing, predictive analytics, and scientific computing, that\n",
    "aims to simplify package management and deployment. Package versions\n",
    "are managed by the package management system **conda**. \n",
    "\n",
    "* [Enthought canopy](https://www.enthought.com/product/canopy/) \n",
    "\n",
    "is a Python\n",
    "distribution for scientific and analytic computing distribution and\n",
    "analysis environment, available for free and under a commercial\n",
    "license.\n",
    "\n",
    "Furthermore, [Google's Colab](https://colab.research.google.com/notebooks/welcome.ipynb) is a free Jupyter notebook environment that requires \n",
    "no setup and runs entirely in the cloud. Try it out!\n",
    "\n",
    "## Useful Python libraries\n",
    "Here we list several useful Python libraries we strongly recommend (if you use anaconda many of these are already there)\n",
    "\n",
    "* [NumPy](https://www.numpy.org/) is a highly popular library for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "\n",
    "* [The pandas](https://pandas.pydata.org/) library provides high-performance, easy-to-use data structures and data analysis tools \n",
    "\n",
    "* [Xarray](http://xarray.pydata.org/en/stable/) is a Python package that makes working with labelled multi-dimensional arrays simple, efficient, and fun!\n",
    "\n",
    "* [Scipy](https://www.scipy.org/) (pronounced “Sigh Pie”) is a Python-based ecosystem of open-source software for mathematics, science, and engineering. \n",
    "\n",
    "* [Matplotlib](https://matplotlib.org/) is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "\n",
    "* [Autograd](https://github.com/HIPS/autograd) can automatically differentiate native Python and Numpy code. It can handle a large subset of Python's features, including loops, ifs, recursion and closures, and it can even take derivatives of derivatives of derivatives\n",
    "\n",
    "* [SymPy](https://www.sympy.org/en/index.html) is a Python library for symbolic mathematics. \n",
    "\n",
    "* [scikit-learn](https://scikit-learn.org/stable/) has simple and efficient tools for machine learning, data mining and data analysis\n",
    "\n",
    "* [TensorFlow](https://www.tensorflow.org/) is a Python library for fast numerical computing created and released by Google\n",
    "\n",
    "* [Keras](https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano\n",
    "\n",
    "* And many more such as [pytorch](https://pytorch.org/),  [Theano](https://pypi.org/project/Theano/) etc \n",
    "\n",
    "## Installing R, C++, cython or Julia\n",
    "\n",
    "You will also find it convenient to utilize **R**. We will mainly\n",
    "use Python during our lectures and in various projects and exercises.\n",
    "Those of you\n",
    "already familiar with **R** should feel free to continue using **R**, keeping\n",
    "however an eye on the parallel Python set ups. Similarly, if you are a\n",
    "Python afecionado, feel free to explore **R** as well.  Jupyter/Ipython\n",
    "notebook allows you to run **R** codes interactively in your\n",
    "browser. The software library **R** is really tailored  for statistical data analysis\n",
    "and allows for an easy usage of the tools and algorithms we will discuss in these\n",
    "lectures.\n",
    "\n",
    "To install **R** with Jupyter notebook \n",
    "[follow the link here](https://mpacer.org/maths/r-kernel-for-ipython-notebook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Installing R, C++, cython, Numba etc\n",
    "\n",
    "\n",
    "For the C++ aficionados, Jupyter/IPython notebook allows you also to\n",
    "install C++ and run codes written in this language interactively in\n",
    "the browser. Since we will emphasize writing many of the algorithms\n",
    "yourself, you can thus opt for either Python or C++ (or Fortran or other compiled languages) as programming\n",
    "languages.\n",
    "\n",
    "To add more entropy, **cython** can also be used when running your\n",
    "notebooks. It means that Python with the jupyter notebook\n",
    "setup allows you to integrate widely popular softwares and tools for\n",
    "scientific computing. Similarly, the \n",
    "[Numba Python package](https://numba.pydata.org/) delivers increased performance\n",
    "capabilities with minimal rewrites of your codes.  With its\n",
    "versatility, including symbolic operations, Python offers a unique\n",
    "computational environment. Your jupyter notebook can easily be\n",
    "converted into a nicely rendered **PDF** file or a Latex file for\n",
    "further processing. For example, convert to latex as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        pycod jupyter nbconvert filename.ipynb --to latex \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to add more versatility, the Python package [SymPy](http://www.sympy.org/en/index.html) is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS)  and is entirely written in Python. \n",
    "\n",
    "Finally, if you wish to use the light mark-up language \n",
    "[doconce](https://github.com/hplgit/doconce) you can convert a standard ascii text file into various HTML \n",
    "formats, ipython notebooks, latex files, pdf files etc with minimal edits. These lectures were generated using **doconce**.\n",
    "\n",
    "\n",
    "\n",
    "## Numpy examples and Important Matrix and vector handling packages\n",
    "\n",
    "There are several central software libraries for linear algebra and eigenvalue problems. Several of the more\n",
    "popular ones have been wrapped into ofter software packages like those from the widely used text **Numerical Recipes**. The original source codes in many of the available packages are often taken from the widely used\n",
    "software package LAPACK, which follows two other popular packages\n",
    "developed in the 1970s, namely EISPACK and LINPACK.  We describe them shortly here.\n",
    "\n",
    "  * LINPACK: package for linear equations and least square problems.\n",
    "\n",
    "  * LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website <http://www.netlib.org> it is possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.\n",
    "\n",
    "  * BLAS (I, II and III): (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. Highly parallelized and efficient codes, all available for download from <http://www.netlib.org>.\n",
    "\n",
    "## Basic Matrix Features\n",
    "\n",
    "**Matrix properties reminder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A} =\n",
    "      \\begin{bmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\\\\n",
    "                                 a_{21} & a_{22} & a_{23} & a_{24} \\\\\n",
    "                                   a_{31} & a_{32} & a_{33} & a_{34} \\\\\n",
    "                                  a_{41} & a_{42} & a_{43} & a_{44}\n",
    "             \\end{bmatrix}\\qquad\n",
    "\\mathbf{I} =\n",
    "      \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\\n",
    "                                 0 & 1 & 0 & 0 \\\\\n",
    "                                 0 & 0 & 1 & 0 \\\\\n",
    "                                 0 & 0 & 0 & 1\n",
    "             \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of a matrix is defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}^{-1} \\cdot \\mathbf{A} = I\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">              Relations               </th> <th align=\"center\">      Name     </th> <th align=\"center\">                            matrix elements                            </th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   $A = A^{T}$                               </td> <td align=\"center\">   symmetric          </td> <td align=\"center\">   $a_{ij} = a_{ji}$                                                          </td> </tr>\n",
    "<tr><td align=\"center\">   $A = \\left (A^{T} \\right )^{-1}$          </td> <td align=\"center\">   real orthogonal    </td> <td align=\"center\">   $\\sum_k a_{ik} a_{jk} = \\sum_k a_{ki} a_{kj} = \\delta_{ij}$                </td> </tr>\n",
    "<tr><td align=\"center\">   $A = A^{ * }$                             </td> <td align=\"center\">   real matrix        </td> <td align=\"center\">   $a_{ij} = a_{ij}^{ * }$                                                    </td> </tr>\n",
    "<tr><td align=\"center\">   $A = A^{\\dagger}$                         </td> <td align=\"center\">   hermitian          </td> <td align=\"center\">   $a_{ij} = a_{ji}^{ * }$                                                    </td> </tr>\n",
    "<tr><td align=\"center\">   $A = \\left (A^{\\dagger} \\right )^{-1}$    </td> <td align=\"center\">   unitary            </td> <td align=\"center\">   $\\sum_k a_{ik} a_{jk}^{ * } = \\sum_k a_{ki}^{ * } a_{kj} = \\delta_{ij}$    </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Some famous Matrices\n",
    "\n",
    "  * Diagonal if $a_{ij}=0$ for $i\\ne j$\n",
    "\n",
    "  * Upper triangular if $a_{ij}=0$ for $i > j$\n",
    "\n",
    "  * Lower triangular if $a_{ij}=0$ for $i < j$\n",
    "\n",
    "  * Upper Hessenberg if $a_{ij}=0$ for $i > j+1$\n",
    "\n",
    "  * Lower Hessenberg if $a_{ij}=0$ for $i < j+1$\n",
    "\n",
    "  * Tridiagonal if $a_{ij}=0$ for $|i -j| > 1$\n",
    "\n",
    "  * Lower banded with bandwidth $p$: $a_{ij}=0$ for $i > j+p$\n",
    "\n",
    "  * Upper banded with bandwidth $p$: $a_{ij}=0$ for $i < j+p$\n",
    "\n",
    "  * Banded, block upper triangular, block lower triangular....\n",
    "\n",
    "### More Basic Matrix Features\n",
    "\n",
    "**Some Equivalent Statements.**\n",
    "\n",
    "For an $N\\times N$ matrix  $\\mathbf{A}$ the following properties are all equivalent\n",
    "\n",
    "  * If the inverse of $\\mathbf{A}$ exists, $\\mathbf{A}$ is nonsingular.\n",
    "\n",
    "  * The equation $\\mathbf{Ax}=0$ implies $\\mathbf{x}=0$.\n",
    "\n",
    "  * The rows of $\\mathbf{A}$ form a basis of $R^N$.\n",
    "\n",
    "  * The columns of $\\mathbf{A}$ form a basis of $R^N$.\n",
    "\n",
    "  * $\\mathbf{A}$ is a product of elementary matrices.\n",
    "\n",
    "  * $0$ is not eigenvalue of $\\mathbf{A}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Numpy and arrays\n",
    "[Numpy](http://www.numpy.org/) provides an easy way to handle arrays in Python. The standard way to import this library is as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows a simple example where we set up an array of ten elements, all determined by random numbers drawn according to the normal distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x = np.random.normal(size=n)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a vector $x$ with $n=10$ elements with its values given by the Normal distribution $N(0,1)$.\n",
    "Another alternative is to declare a vector as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([1, 2, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined a vector with three elements, with $x_0=1$, $x_1=2$ and $x_2=3$. Note that both Python and C++\n",
    "start numbering array elements from $0$ and on. This means that a vector with $n$ elements has a sequence of entities $x_0, x_1, x_2, \\dots, x_{n-1}$. We could also let (recommended) Numpy to compute the logarithms of a specific array as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.log(np.array([4, 7, 8]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example we used Numpy's unary function $np.log$. This function is\n",
    "highly tuned to compute array elements since the code is vectorized\n",
    "and does not require looping. We normaly recommend that you use the\n",
    "Numpy intrinsic functions instead of the corresponding **log** function\n",
    "from Python's **math** module. The looping is done explicitely by the\n",
    "**np.log** function. The alternative, and slower way to compute the\n",
    "logarithms of a vector would be to write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "x = np.array([4, 7, 8])\n",
    "for i in range(0, len(x)):\n",
    "    x[i] = log(x[i])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that our code is much longer already and we need to import the **log** function from the **math** module. \n",
    "The attentive reader will also notice that the output is $[1, 1, 2]$. Python interprets automagically our numbers as integers (like the **automatic** keyword in C++). To change this we could define our array elements to be double precision numbers as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.log(np.array([4, 7, 8], dtype = np.float64))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or simply write them as double precision numbers (Python uses 64 bits as default for floating point type variables), that is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.log(np.array([4.0, 7.0, 8.0])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the number of bytes (remember that one byte contains eight bits for double precision variables), you can use simple use the **itemsize** functionality (the array $x$ is actually an object which inherits the functionalities defined in Numpy) as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.log(np.array([4.0, 7.0, 8.0])\n",
    "print(x.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices in Python\n",
    "\n",
    "Having defined vectors, we are now ready to try out matrices. We can\n",
    "define a $3 \\times 3 $ real matrix $\\hat{A}$ as (recall that we user\n",
    "lowercase letters for vectors and uppercase letters for matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the **shape** function we would get $(3, 3)$ as output, that is verifying that our matrix is a $3\\times 3$ matrix. We can slice the matrix and print for example the first column (Python organized matrix elements in a row-major order, see below) as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))\n",
    "# print the first column, row-major order and elements start with 0\n",
    "print(A[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue this was by printing out other columns or rows. The example here prints out the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))\n",
    "# print the first column, row-major order and elements start with 0\n",
    "print(A[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy contains many other functionalities that allow us to slice, subdivide etc etc arrays. We strongly recommend that you look up the [Numpy website for more details](http://www.numpy.org/). Useful functions when defining a matrix are the **np.zeros** function which declares a matrix of a given dimension and sets all elements to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "# define a matrix of dimension 10 x 10 and set all elements to zero\n",
    "A = np.zeros( (n, n) )\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or initializing all elements to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "# define a matrix of dimension 10 x 10 and set all elements to one\n",
    "A = np.ones( (n, n) )\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as unitarily distributed random numbers (see the material on random number generators in the statistics part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "# define a matrix of dimension 10 x 10 and set all elements to random numbers with x \\in [0, 1]\n",
    "A = np.random.rand(n, n)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see throughout these lectures, there are several extremely useful functionalities in Numpy.\n",
    "As an example, consider the discussion of the covariance matrix. Suppose we have defined three vectors\n",
    "$\\hat{x}, \\hat{y}, \\hat{z}$ with $n$ elements each. The covariance matrix is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\Sigma} = \\begin{bmatrix} \\sigma_{xx} & \\sigma_{xy} & \\sigma_{xz} \\\\\n",
    "                              \\sigma_{yx} & \\sigma_{yy} & \\sigma_{yz} \\\\\n",
    "                              \\sigma_{zx} & \\sigma_{zy} & \\sigma_{zz} \n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma_{xy} =\\frac{1}{n} \\sum_{i=0}^{n-1}(x_i- \\overline{x})(y_i- \\overline{y}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Numpy function **np.cov** calculates the covariance elements using the factor $1/(n-1)$ instead of $1/n$ since it assumes we do not have the exact mean values. \n",
    "The following simple function uses the **np.vstack** function which takes each vector of dimension $1\\times n$ and produces a $3\\times n$ matrix $\\hat{W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{W} = \\begin{bmatrix} x_0 & y_0 & z_0 \\\\\n",
    "                          x_1 & y_1 & z_1 \\\\\n",
    "                          x_2 & y_2 & z_2 \\\\\n",
    "                          \\dots & \\dots & \\dots \\\\\n",
    "                          x_{n-2} & y_{n-2} & z_{n-2} \\\\\n",
    "                          x_{n-1} & y_{n-1} & z_{n-1}\n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in turn is converted into into the $3\\times 3$ covariance matrix\n",
    "$\\hat{\\Sigma}$ via the Numpy function **np.cov()**. We note that we can also calculate\n",
    "the mean value of each set of samples $\\hat{x}$ etc using the Numpy\n",
    "function **np.mean(x)**. We can also extract the eigenvalues of the\n",
    "covariance matrix through the **np.linalg.eig()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "\n",
    "n = 100\n",
    "x = np.random.normal(size=n)\n",
    "print(np.mean(x))\n",
    "y = 4+3*x+np.random.normal(size=n)\n",
    "print(np.mean(y))\n",
    "z = x**3+np.random.normal(size=n)\n",
    "print(np.mean(z))\n",
    "W = np.vstack((x, y, z))\n",
    "Sigma = np.cov(W)\n",
    "print(Sigma)\n",
    "Eigvals, Eigvecs = np.linalg.eig(Sigma)\n",
    "print(Eigvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "eye = np.eye(4)\n",
    "print(eye)\n",
    "sparse_mtx = sparse.csr_matrix(eye)\n",
    "print(sparse_mtx)\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.sin(x)\n",
    "plt.plot(x,y,marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet the Pandas\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig/pandas.jpg, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"fig/pandas.jpg\" width=600>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Another useful Python package is\n",
    "[pandas](https://pandas.pydata.org/), which is an open source library\n",
    "providing high-performance, easy-to-use data structures and data\n",
    "analysis tools for Python. **pandas** stands for panel data, a term borrowed from econometrics and is an efficient library for data analysis with an emphasis on tabular data.\n",
    "**pandas** has two major classes, the **DataFrame** class with two-dimensional data objects and tabular data organized in columns and the class **Series** with a focus on one-dimensional data objects. Both classes allow you to index data easily as we will see in the examples below. \n",
    "**pandas** allows you also to perform mathematical operations on the data, spanning from simple reshapings of vectors and matrices to statistical operations. \n",
    "\n",
    "The following simple example shows how we can, in an easy way make tables of our data. Here we define a data set which includes names, place of birth and date of birth, and displays the data in an easy to read way. We will see repeated use of **pandas**, in particular in connection with classification of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "data = {'First Name': [\"Frodo\", \"Bilbo\", \"Aragorn II\", \"Samwise\"],\n",
    "        'Last Name': [\"Baggins\", \"Baggins\",\"Elessar\",\"Gamgee\"],\n",
    "        'Place of birth': [\"Shire\", \"Shire\", \"Eriador\", \"Shire\"],\n",
    "        'Date of Birth T.A.': [2968, 2890, 2931, 2980]\n",
    "        }\n",
    "data_pandas = pd.DataFrame(data)\n",
    "display(data_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we have imported **pandas** with the shorthand **pd**, the latter has become the standard way we import **pandas**. We make then a list of various variables\n",
    "and reorganize the aboves lists into a **DataFrame** and then print out  a neat table with specific column labels as *Name*, *place of birth* and *date of birth*.\n",
    "Displaying these results, we see that the indices are given by the default numbers from zero to three.\n",
    "**pandas** is extremely flexible and we can easily change the above indices by defining a new type of indexing as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pandas = pd.DataFrame(data,index=['Frodo','Bilbo','Aragorn','Sam'])\n",
    "display(data_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereafter we display the content of the row which begins with the index **Aragorn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_pandas.loc['Aragorn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily append data to this, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hobbit = {'First Name': [\"Peregrin\"],\n",
    "              'Last Name': [\"Took\"],\n",
    "              'Place of birth': [\"Shire\"],\n",
    "              'Date of Birth T.A.': [2990]\n",
    "              }\n",
    "data_pandas=data_pandas.append(pd.DataFrame(new_hobbit, index=['Pippin']))\n",
    "display(data_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are other examples where we use the **DataFrame** functionality to handle arrays, now with more interesting features for us, namely numbers. We set up a matrix \n",
    "of dimensionality $10\\times 5$ and compute the mean value and standard deviation of each column. Similarly, we can perform mathematial operations like squaring the matrix elements and many other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "np.random.seed(100)\n",
    "# setting up a 10 x 5 matrix\n",
    "rows = 10\n",
    "cols = 5\n",
    "a = np.random.randn(rows,cols)\n",
    "df = pd.DataFrame(a)\n",
    "display(df)\n",
    "print(df.mean())\n",
    "print(df.std())\n",
    "display(df**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereafter we can select specific columns only and plot final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['First', 'Second', 'Third', 'Fourth', 'Fifth']\n",
    "df.index = np.arange(10)\n",
    "\n",
    "display(df)\n",
    "print(df['Second'].mean() )\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "from pylab import plt, mpl\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "df.cumsum().plot(lw=2.0, figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df.plot.bar(figsize=(10,6), rot=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can produce a $4\\times 4$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.arange(16).reshape((4,4))\n",
    "print(b)\n",
    "df1 = pd.DataFrame(b)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and many other operations. \n",
    "\n",
    "The **Series** class is another important class included in\n",
    "**pandas**. You can view it as a specialization of **DataFrame** but where\n",
    "we have just a single column of data. It shares many of the same features as _DataFrame. As with **DataFrame**,\n",
    "most operations are vectorized, achieving thereby a high performance when dealing with computations of arrays, in particular labeled arrays.\n",
    "As we will see below it leads also to a very concice code close to the mathematical operations we may be interested in.\n",
    "For multidimensional arrays, we recommend strongly [xarray](http://xarray.pydata.org/en/stable/). **xarray** has much of the same flexibility as **pandas**, but allows for the extension to higher dimensions than two. We will see examples later of the usage of both **pandas** and **xarray**. \n",
    "\n",
    "\n",
    "\n",
    "## Reading Data and fitting\n",
    "\n",
    "In order to study various Machine Learning algorithms, we need to\n",
    "access data. Acccessing data is an essential step in all machine\n",
    "learning algorithms. In particular, setting up the so-called **design\n",
    "matrix** (to be defined below) is often the first element we need in\n",
    "order to perform our calculations. To set up the design matrix means\n",
    "reading (and later, when the calculations are done, writing) data\n",
    "in various formats, The formats span from reading files from disk,\n",
    "loading data from databases and interacting with online sources\n",
    "like web application programming interfaces (APIs).\n",
    "\n",
    "In handling various input formats, as discussed above, we will mainly stay with **pandas**,\n",
    "a Python package which allows us, in a seamless and painless way, to\n",
    "deal with a multitude of formats, from standard **csv** (comma separated\n",
    "values) files, via **excel**, **html** to **hdf5** formats.  With **pandas**\n",
    "and the **DataFrame**  and **Series** functionalities we are able to convert text data\n",
    "into the calculational formats we need for a specific algorithm. And our code is going to be \n",
    "pretty close the basic mathematical expressions.\n",
    "\n",
    "Our first data set is going to be a classic from nuclear physics, namely all\n",
    "available data on binding energies. Don't be intimidated if you are not familiar with nuclear physics. It serves simply as an example here of a data set. \n",
    "\n",
    "We will show some of the\n",
    "strengths of packages like **Scikit-Learn** in fitting nuclear binding energies to\n",
    "specific functions using linear regression first. Then, as a teaser, we will show you how \n",
    "you can easily implement other algorithms like decision trees and random forests and neural networks.\n",
    "\n",
    "But before we really start with nuclear physics data, let's just look at some simpler polynomial fitting cases, such as,\n",
    "(don't be offended) fitting straight lines!\n",
    "\n",
    "\n",
    "### Simple linear regression model using **scikit-learn**\n",
    "\n",
    "We start with perhaps our simplest possible example, using **Scikit-Learn** to perform linear regression analysis on a data set produced by us. \n",
    "\n",
    "What follows is a simple Python code where we have defined a function\n",
    "$y$ in terms of the variable $x$. Both are defined as vectors with  $100$ entries. \n",
    "The numbers in the vector $\\hat{x}$ are given\n",
    "by random numbers generated with a uniform distribution with entries\n",
    "$x_i \\in [0,1]$ (more about probability distribution functions\n",
    "later). These values are then used to define a function $y(x)$\n",
    "(tabulated again as a vector) with a linear dependence on $x$ plus a\n",
    "random noise added via the normal distribution.\n",
    "\n",
    "\n",
    "The Numpy functions are imported used the **import numpy as np**\n",
    "statement and the random number generator for the uniform distribution\n",
    "is called using the function **np.random.rand()**, where we specificy\n",
    "that we want $100$ random variables.  Using Numpy we define\n",
    "automatically an array with the specified number of elements, $100$ in\n",
    "our case.  With the Numpy function **randn()** we can compute random\n",
    "numbers with the normal distribution (mean value $\\mu$ equal to zero and\n",
    "variance $\\sigma^2$ set to one) and produce the values of $y$ assuming a linear\n",
    "dependence as function of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = 2x+N(0,1),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $N(0,1)$ represents random numbers generated by the normal\n",
    "distribution.  From **Scikit-Learn** we import then the\n",
    "**LinearRegression** functionality and make a prediction $\\tilde{y} =\n",
    "\\alpha + \\beta x$ using the function **fit(x,y)**. We call the set of\n",
    "data $(\\hat{x},\\hat{y})$ for our training data. The Python package\n",
    "**scikit-learn** has also a functionality which extracts the above\n",
    "fitting parameters $\\alpha$ and $\\beta$ (see below). Later we will\n",
    "distinguish between training data and test data.\n",
    "\n",
    "For plotting we use the Python package\n",
    "[matplotlib](https://matplotlib.org/) which produces publication\n",
    "quality figures. Feel free to explore the extensive\n",
    "[gallery](https://matplotlib.org/gallery/index.html) of examples. In\n",
    "this example we plot our original values of $x$ and $y$ as well as the\n",
    "prediction **ypredict** ($\\tilde{y}$), which attempts at fitting our\n",
    "data with a straight line.\n",
    "\n",
    "The Python code follows here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.random.rand(100,1)\n",
    "y = 2*x+np.random.randn(100,1)\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(x,y)\n",
    "xnew = np.array([[0],[1]])\n",
    "ypredict = linreg.predict(xnew)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,1.0,0, 5.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Simple Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example serves several aims. It allows us to demonstrate several\n",
    "aspects of data analysis and later machine learning algorithms. The\n",
    "immediate visualization shows that our linear fit is not\n",
    "impressive. It goes through the data points, but there are many\n",
    "outliers which are not reproduced by our linear regression.  We could\n",
    "now play around with this small program and change for example the\n",
    "factor in front of $x$ and the normal distribution.  Try to change the\n",
    "function $y$ to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = 10x+0.01 \\times N(0,1),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $x$ is defined as before.  Does the fit look better? Indeed, by\n",
    "reducing the role of the noise given by the normal distribution we see immediately that\n",
    "our linear prediction seemingly reproduces better the training\n",
    "set. However, this testing 'by the eye' is obviouly not satisfactory in the\n",
    "long run. Here we have only defined the training data and our model, and \n",
    "have not discussed a more rigorous approach to the **cost** function.\n",
    "\n",
    "We need more rigorous criteria in defining whether we have succeeded or\n",
    "not in modeling our training data.  You will be surprised to see that\n",
    "many scientists seldomly venture beyond this 'by the eye' approach. A\n",
    "standard approach for the *cost* function is the so-called $\\chi^2$\n",
    "function (a variant of the mean-squared error (MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\chi^2 = \\frac{1}{n}\n",
    "\\sum_{i=0}^{n-1}\\frac{(y_i-\\tilde{y}_i)^2}{\\sigma_i^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\sigma_i^2$ is the variance (to be defined later) of the entry\n",
    "$y_i$.  We may not know the explicit value of $\\sigma_i^2$, it serves\n",
    "however the aim of scaling the equations and make the cost function\n",
    "dimensionless.  \n",
    "\n",
    "Minimizing the cost function is a central aspect of\n",
    "our discussions to come. Finding its minima as function of the model\n",
    "parameters ($\\alpha$ and $\\beta$ in our case) will be a recurring\n",
    "theme in these series of lectures. Essentially all machine learning\n",
    "algorithms we will discuss center around the minimization of the\n",
    "chosen cost function. This depends in turn on our specific\n",
    "model for describing the data, a typical situation in supervised\n",
    "learning. Automatizing the search for the minima of the cost function is a\n",
    "central ingredient in all algorithms. Typical methods which are\n",
    "employed are various variants of **gradient** methods. These will be\n",
    "discussed in more detail later. Again, you'll be surprised to hear that\n",
    "many practitioners minimize the above function ''by the eye', popularly dubbed as \n",
    "'chi by the eye'. That is, change a parameter and see (visually and numerically) that \n",
    "the  $\\chi^2$ function becomes smaller. \n",
    "\n",
    "There are many ways to define the cost function. A simpler approach is to look at the relative difference between the training data and the predicted data, that is we define \n",
    "the relative error (why would we prefer the MSE instead of the relative error?) as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\epsilon_{\\mathrm{relative}}= \\frac{\\vert \\hat{y} -\\hat{\\tilde{y}}\\vert}{\\vert \\hat{y}\\vert}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared cost function results in an arithmetic mean-unbiased\n",
    "estimator, and the absolute-value cost function results in a\n",
    "median-unbiased estimator (in the one-dimensional case, and a\n",
    "geometric median-unbiased estimator for the multi-dimensional\n",
    "case). The squared cost function has the disadvantage that it has the tendency\n",
    "to be dominated by outliers.\n",
    "\n",
    "We can modify easily the above Python code and plot the relative error instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.random.rand(100,1)\n",
    "y = 5*x+0.01*np.random.randn(100,1)\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(x,y)\n",
    "ypredict = linreg.predict(x)\n",
    "\n",
    "plt.plot(x, np.abs(ypredict-y)/abs(y), \"ro\")\n",
    "plt.axis([0,1.0,0.0, 0.5])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$\\epsilon_{\\mathrm{relative}}$')\n",
    "plt.title(r'Relative error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the parameter in front of the normal distribution, we may\n",
    "have a small or larger relative error. Try to play around with\n",
    "different training data sets and study (graphically) the value of the\n",
    "relative error.\n",
    "\n",
    "As mentioned above, **Scikit-Learn** has an impressive functionality.\n",
    "We can for example extract the values of $\\alpha$ and $\\beta$ and\n",
    "their error estimates, or the variance and standard deviation and many\n",
    "other properties from the statistical data analysis. \n",
    "\n",
    "Here we show an\n",
    "example of the functionality of **Scikit-Learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_error\n",
    "\n",
    "x = np.random.rand(100,1)\n",
    "y = 2.0+ 5*x+0.5*np.random.randn(100,1)\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(x,y)\n",
    "ypredict = linreg.predict(x)\n",
    "print('The intercept alpha: \\n', linreg.intercept_)\n",
    "print('Coefficient beta : \\n', linreg.coef_)\n",
    "# The mean squared error                               \n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y, ypredict))\n",
    "# Explained variance score: 1 is perfect prediction                                 \n",
    "print('Variance score: %.2f' % r2_score(y, ypredict))\n",
    "# Mean squared log error                                                        \n",
    "print('Mean squared log error: %.2f' % mean_squared_log_error(y, ypredict) )\n",
    "# Mean absolute error                                                           \n",
    "print('Mean absolute error: %.2f' % mean_absolute_error(y, ypredict))\n",
    "plt.plot(x, ypredict, \"r-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0.0,1.0,1.5, 7.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Linear Regression fit ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **coef** gives us the parameter $\\beta$ of our fit while **intercept** yields \n",
    "$\\alpha$. Depending on the constant in front of the normal distribution, we get values near or far from $alpha =2$ and $\\beta =5$. Try to play around with different parameters in front of the normal distribution. The function **meansquarederror** gives us the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(\\hat{y},\\hat{\\tilde{y}}) = \\frac{1}{n}\n",
    "\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller the value, the better the fit. Ideally we would like to\n",
    "have an MSE equal zero.  The attentive reader has probably recognized\n",
    "this function as being similar to the $\\chi^2$ function defined above.\n",
    "\n",
    "The **r2score** function computes $R^2$, the coefficient of\n",
    "determination. It provides a measure of how well future samples are\n",
    "likely to be predicted by the model. Best possible score is 1.0 and it\n",
    "can be negative (because the model can be arbitrarily worse). A\n",
    "constant model that always predicts the expected value of $\\hat{y}$,\n",
    "disregarding the input features, would get a $R^2$ score of $0.0$.\n",
    "\n",
    "If $\\tilde{\\hat{y}}_i$ is the predicted value of the $i-th$ sample and $y_i$ is the corresponding true value, then the score $R^2$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R^2(\\hat{y}, \\tilde{\\hat{y}}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\tilde{y}_i)^2}{\\sum_{i=0}^{n - 1} (y_i - \\bar{y})^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have defined the mean value  of $\\hat{y}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quantity taht we will meet again in our discussions of regression analysis is \n",
    " the mean absolute error (MAE), a risk metric corresponding to the expected value of the absolute error loss or what we call the $l1$-norm loss. In our discussion above we presented the relative error.\n",
    "The MAE is defined as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MAE}(\\hat{y}, \\hat{\\tilde{y}}) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left| y_i - \\tilde{y}_i \\right|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the \n",
    "squared logarithmic (quadratic) error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MSLE}(\\hat{y}, \\hat{\\tilde{y}}) = \\frac{1}{n} \\sum_{i=0}^{n - 1} (\\log_e (1 + y_i) - \\log_e (1 + \\tilde{y}_i) )^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\log_e (x)$ stands for the natural logarithm of $x$. This error\n",
    "estimate is best to use when targets having exponential growth, such\n",
    "as population counts, average sales of a commodity over a span of\n",
    "years etc. \n",
    "\n",
    "\n",
    "Finally, another cost function is the Huber cost function used in robust regression.\n",
    "\n",
    "The rationale behind this possible cost function is its reduced\n",
    "sensitivity to outliers in the data set. In our discussions on\n",
    "dimensionality reduction and normalization of data we will meet other\n",
    "ways of dealing with outliers.\n",
    "\n",
    "The Huber cost function is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_{\\delta}(a)={\\begin{cases}{\\frac {1}{2}}{a^{2}}&{\\text{for }}|a|\\leq \\delta ,\\\\\\delta (|a|-{\\frac {1}{2}}\\delta ),&{\\text{otherwise.}}\\end{cases}}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $a=\\boldsymbol{y} - \\boldsymbol{\\tilde{y}}$.\n",
    "We will discuss in more\n",
    "detail these and other functions in the various lectures.  We conclude this part with another example. Instead of \n",
    "a linear $x$-dependence we study now a cubic polynomial and use the polynomial regression analysis tools of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x=np.linspace(0.02,0.98,200)\n",
    "noise = np.asarray(random.sample((range(200)),200))\n",
    "y=x**3*noise\n",
    "yn=x**3*100\n",
    "poly3 = PolynomialFeatures(degree=3)\n",
    "X = poly3.fit_transform(x[:,np.newaxis])\n",
    "clf3 = LinearRegression()\n",
    "clf3.fit(X,y)\n",
    "\n",
    "Xplot=poly3.fit_transform(x[:,np.newaxis])\n",
    "poly3_plot=plt.plot(x, clf3.predict(Xplot), label='Cubic Fit')\n",
    "plt.plot(x,yn, color='red', label=\"True Cubic\")\n",
    "plt.scatter(x, y, label='Data', color='orange', s=15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def error(a):\n",
    "    for i in y:\n",
    "        err=(y-yn)/yn\n",
    "    return abs(np.sum(err))/len(err)\n",
    "\n",
    "print (error(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To our real data: nuclear binding energies. Brief reminder on masses and binding energies\n",
    "\n",
    "Let us now dive into  nuclear physics and remind ourselves briefly about some basic features about binding\n",
    "energies.  A basic quantity which can be measured for the ground\n",
    "states of nuclei is the atomic mass $M(N, Z)$ of the neutral atom with\n",
    "atomic mass number $A$ and charge $Z$. The number of neutrons is $N$. There are indeed several sophisticated experiments worldwide which allow us to measure this quantity to high precision (parts per million even). \n",
    "\n",
    "Atomic masses are usually tabulated in terms of the mass excess defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta M(N, Z) =  M(N, Z) - uA,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $u$ is the Atomic Mass Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u = M(^{12}\\mathrm{C})/12 = 931.4940954(57) \\hspace{0.1cm} \\mathrm{MeV}/c^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nucleon masses are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "m_p =  1.00727646693(9)u,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "m_n = 939.56536(8)\\hspace{0.1cm} \\mathrm{MeV}/c^2 = 1.0086649156(6)u.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [2016 mass evaluation of by W.J.Huang, G.Audi, M.Wang, F.G.Kondev, S.Naimi and X.Xu](http://nuclearmasses.org/resources_folder/Wang_2017_Chinese_Phys_C_41_030003.pdf)\n",
    "there are data on masses and decays of 3437 nuclei.\n",
    "\n",
    "The nuclear binding energy is defined as the energy required to break\n",
    "up a given nucleus into its constituent parts of $N$ neutrons and $Z$\n",
    "protons. In terms of the atomic masses $M(N, Z)$ the binding energy is\n",
    "defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "BE(N, Z) = ZM_H c^2 + Nm_n c^2 - M(N, Z)c^2 ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $M_H$ is the mass of the hydrogen atom and $m_n$ is the mass of the neutron.\n",
    "In terms of the mass excess the binding energy is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "BE(N, Z) = Z\\Delta_H c^2 + N\\Delta_n c^2 -\\Delta(N, Z)c^2 ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\Delta_H c^2 = 7.2890$ MeV and $\\Delta_n c^2 = 8.0713$ MeV.\n",
    "\n",
    "\n",
    "A popular and physically intuitive model which can be used to parametrize \n",
    "the experimental binding energies as function of $A$, is the so-called \n",
    "**liquid drop model**. The ansatz is based on the following expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "BE(N,Z) = a_1A-a_2A^{2/3}-a_3\\frac{Z^2}{A^{1/3}}-a_4\\frac{(N-Z)^2}{A},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $A$ stands for the number of nucleons and the $a_i$s are parameters which are determined by a fit \n",
    "to the experimental data.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To arrive at the above expression we have assumed that we can make the following assumptions:\n",
    "\n",
    " * There is a volume term $a_1A$ proportional with the number of nucleons (the energy is also an extensive quantity). When an assembly of nucleons of the same size is packed together into the smallest volume, each interior nucleon has a certain number of other nucleons in contact with it. This contribution is proportional to the volume.\n",
    "\n",
    " * There is a surface energy term $a_2A^{2/3}$. The assumption here is that a nucleon at the surface of a nucleus interacts with fewer other nucleons than one in the interior of the nucleus and hence its binding energy is less. This surface energy term takes that into account and is therefore negative and is proportional to the surface area.\n",
    "\n",
    " * There is a Coulomb energy term $a_3\\frac{Z^2}{A^{1/3}}$. The electric repulsion between each pair of protons in a nucleus yields less binding. \n",
    "\n",
    " * There is an asymmetry term $a_4\\frac{(N-Z)^2}{A}$. This term is associated with the Pauli exclusion principle and reflects the fact that the proton-neutron interaction is more attractive on the average than the neutron-neutron and proton-proton interactions.\n",
    "\n",
    "We could also add a so-called pairing term, which is a correction term that\n",
    "arises from the tendency of proton pairs and neutron pairs to\n",
    "occur. An even number of particles is more stable than an odd number. \n",
    "\n",
    "\n",
    "### Organizing our data\n",
    "\n",
    "Let us start with reading and organizing our data. \n",
    "We start with the compilation of masses and binding energies from 2016.\n",
    "After having downloaded this file to our own computer, we are now ready to read the file and start structuring our data.\n",
    "\n",
    "\n",
    "We start with preparing folders for storing our calculations and the data file over masses and binding energies. We import also various modules that we will find useful in order to present various Machine Learning methods. Here we focus mainly on the functionality of **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')\n",
    "\n",
    "infile = open(data_path(\"MassEval2016.dat\"),'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, we define also a function for making our plots. You can obviously avoid this and simply set up various **matplotlib** commands every time you need them. You may however find it convenient to collect all such commands in one function and simply call this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import plt, mpl\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "def MakePlot(x,y, styles, labels, axlabels):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(len(x)):\n",
    "        plt.plot(x[i], y[i], styles[i], label = labels[i])\n",
    "        plt.xlabel(axlabels[0])\n",
    "        plt.ylabel(axlabels[1])\n",
    "    plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to read the data on experimental binding energies and\n",
    "reorganize them as functions of the mass number $A$, the number of\n",
    "protons $Z$ and neutrons $N$ using **pandas**.  Before we do this it is\n",
    "always useful (unless you have a binary file or other types of compressed\n",
    "data) to actually open the file and simply take a look at it!\n",
    "\n",
    "\n",
    "In particular, the program that outputs the final nuclear masses is written in Fortran with a specific format. It means that we need to figure out the format and which columns contain the data we are interested in. Pandas comes with a function that reads formatted output. After having admired the file, we are now ready to start massaging it with **pandas**. The file begins with some basic format information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"                                                                                                                         \n",
    "This is taken from the data file of the mass 2016 evaluation.                                                               \n",
    "All files are 3436 lines long with 124 character per line.                                                                  \n",
    "       Headers are 39 lines long.                                                                                           \n",
    "   col 1     :  Fortran character control: 1 = page feed  0 = line feed                                                     \n",
    "   format    :  a1,i3,i5,i5,i5,1x,a3,a4,1x,f13.5,f11.5,f11.3,f9.3,1x,a2,f11.3,f9.3,1x,i3,1x,f12.5,f11.5                     \n",
    "   These formats are reflected in the pandas widths variable below, see the statement                                       \n",
    "   widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),                                                            \n",
    "   Pandas has also a variable header, with length 39 in this case.                                                          \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are interested in are in columns 2, 3, 4 and 11, giving us\n",
    "the number of neutrons, protons, mass numbers and binding energies,\n",
    "respectively. We add also for the sake of completeness the element name. The data are in fixed-width formatted lines and we will\n",
    "covert them into the **pandas** DataFrame structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the experimental data with Pandas\n",
    "Masses = pd.read_fwf(infile, usecols=(2,3,4,6,11),\n",
    "              names=('N', 'Z', 'A', 'Element', 'Ebinding'),\n",
    "              widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),\n",
    "              header=39,\n",
    "              index_col=False)\n",
    "\n",
    "# Extrapolated values are indicated by '#' in place of the decimal place, so\n",
    "# the Ebinding column won't be numeric. Coerce to float and drop these entries.\n",
    "Masses['Ebinding'] = pd.to_numeric(Masses['Ebinding'], errors='coerce')\n",
    "Masses = Masses.dropna()\n",
    "# Convert from keV to MeV.\n",
    "Masses['Ebinding'] /= 1000\n",
    "\n",
    "# Group the DataFrame by nucleon number, A.\n",
    "Masses = Masses.groupby('A')\n",
    "# Find the rows of the grouped DataFrame with the maximum binding energy.\n",
    "Masses = Masses.apply(lambda t: t[t.Ebinding==t.Ebinding.max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now read in the data, grouped them according to the variables we are interested in. \n",
    "We see how easy it is to reorganize the data using **pandas**. If we\n",
    "were to do these operations in C/C++ or Fortran, we would have had to\n",
    "write various functions/subroutines which perform the above\n",
    "reorganizations for us.  Having reorganized the data, we can now start\n",
    "to make some simple fits using both the functionalities in **numpy** and\n",
    "**Scikit-Learn** afterwards. \n",
    "\n",
    "Now we define five variables which contain\n",
    "the number of nucleons $A$, the number of protons $Z$ and the number of neutrons $N$, the element name and finally the energies themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            N    Z    A Element  Ebinding\n",
      "A                                        \n",
      "1   0       0    1    1       H  0.000000\n",
      "2   1       1    1    2       H  1.112283\n",
      "3   2       2    1    3       H  2.827265\n",
      "4   6       2    2    4      He  7.073915\n",
      "5   9       3    2    5      He  5.512132\n",
      "6   14      3    3    6      Li  5.332331\n",
      "7   19      4    3    7      Li  5.606439\n",
      "8   24      4    4    8      Be  7.062435\n",
      "9   29      5    4    9      Be  6.462668\n",
      "10  34      6    4   10      Be  6.497630\n",
      "11  40      6    5   11       B  6.927732\n",
      "12  46      6    6   12       C  7.680144\n",
      "13  52      7    6   13       C  7.469849\n",
      "14  57      8    6   14       C  7.520319\n",
      "15  64      8    7   15       N  7.699460\n",
      "16  72      8    8   16       O  7.976206\n",
      "17  78      9    8   17       O  7.750728\n",
      "18  85     10    8   18       O  7.767097\n",
      "19  93     10    9   19       F  7.779018\n",
      "20  102    10   10   20      Ne  8.032240\n",
      "21  110    11   10   21      Ne  7.971713\n",
      "22  118    12   10   22      Ne  8.080465\n",
      "23  128    12   11   23      Na  8.111493\n",
      "24  137    12   12   24      Mg  8.260709\n",
      "25  146    13   12   25      Mg  8.223502\n",
      "26  154    14   12   26      Mg  8.333870\n",
      "27  164    14   13   27      Al  8.331553\n",
      "28  174    14   14   28      Si  8.447744\n",
      "29  183    15   14   29      Si  8.448635\n",
      "30  192    16   14   30      Si  8.520654\n",
      "...       ...  ...  ...     ...       ...\n",
      "238 3089  146   92  238       U  7.570125\n",
      "239 3099  146   93  239      Np  7.560567\n",
      "240 3109  146   94  240      Pu  7.556042\n",
      "241 3118  147   94  241      Pu  7.546439\n",
      "242 3127  148   94  242      Pu  7.541327\n",
      "243 3136  149   94  243      Pu  7.531008\n",
      "244 3144  150   94  244      Pu  7.524815\n",
      "245 3154  149   96  245      Cm  7.515767\n",
      "246 3162  150   96  246      Cm  7.511471\n",
      "247 3170  151   96  247      Cm  7.501931\n",
      "248 3177  152   96  248      Cm  7.496728\n",
      "249 3186  152   97  249      Bk  7.486040\n",
      "250 3194  152   98  250      Cf  7.479956\n",
      "251 3201  153   98  251      Cf  7.470500\n",
      "252 3209  154   98  252      Cf  7.465347\n",
      "253 3216  155   98  253      Cf  7.454829\n",
      "254 3224  156   98  254      Cf  7.449225\n",
      "255 3232  156   99  255      Es  7.437821\n",
      "256 3241  156  100  256      Fm  7.431780\n",
      "257 3248  157  100  257      Fm  7.422194\n",
      "258 3256  157  101  258      Md  7.409675\n",
      "259 3264  157  102  259      No  7.399974\n",
      "260 3275  154  106  260      Sg  7.342562\n",
      "261 3280  157  104  261      Rf  7.371384\n",
      "262 3289  156  106  262      Sg  7.341185\n",
      "264 3304  156  108  264      Hs  7.298375\n",
      "265 3310  157  108  265      Hs  7.296247\n",
      "266 3317  158  108  266      Hs  7.298273\n",
      "269 3338  159  110  269      Ds  7.250154\n",
      "270 3344  160  110  270      Ds  7.253775\n",
      "\n",
      "[267 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "A = Masses['A']\n",
    "Z = Masses['Z']\n",
    "N = Masses['N']\n",
    "Element = Masses['Element']\n",
    "Energies = Masses['Ebinding']\n",
    "print(Masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step, and we will define this mathematically later, is to set up the so-called **design matrix**. We will throughout call this matrix $\\boldsymbol{X}$.\n",
    "It has dimensionality $p\\times n$, where $n$ is the number of data points and $p$ are the so-called predictors. In our case here they are given by the number of polynomials in $A$ we wish to include in the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we set up the design matrix X\n",
    "X = np.zeros((len(A),5))\n",
    "X[:,0] = 1\n",
    "X[:,1] = A\n",
    "X[:,2] = A**(2.0/3.0)\n",
    "X[:,3] = A**(-1.0/3.0)\n",
    "X[:,4] = A**(-1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **scikitlearn** we are now ready to use linear regression and fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = skl.LinearRegression().fit(X, Energies)\n",
    "fity = clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty simple!  \n",
    "Now we can print measures of how our fit is doing, the coefficients from the fits and plot the final fit together with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.04\n",
      "Variance score: 0.95\n",
      "Mean absolute error: 0.05\n",
      "[ 0.00000000e+00  7.06492086e-03 -1.73091052e-01 -1.66020213e+01\n",
      "  1.17385778e+00] 15.212327334149492\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The mean squared error                               \n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(Energies, fity))\n",
    "# Explained variance score: 1 is perfect prediction                                 \n",
    "print('Variance score: %.2f' % r2_score(Energies, fity))\n",
    "# Mean absolute error                                                           \n",
    "print('Mean absolute error: %.2f' % mean_absolute_error(Energies, fity))\n",
    "print(clf.coef_, clf.intercept_)\n",
    "\n",
    "Masses['Eapprox']  = fity\n",
    "# Generate a plot comparing the experimental with the fitted values values.\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(r'$A = N + Z$')\n",
    "ax.set_ylabel(r'$E_\\mathrm{bind}\\,/\\mathrm{MeV}$')\n",
    "ax.plot(Masses['A'], Masses['Ebinding'], alpha=0.7, lw=2,\n",
    "            label='Ame2016')\n",
    "ax.plot(Masses['A'], Masses['Eapprox'], alpha=0.7, lw=2, c='m',\n",
    "            label='Fit')\n",
    "ax.legend()\n",
    "save_fig(\"Masses2016\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the wood for the trees\n",
    "\n",
    "As a teaser, let us now see how we can do this with decision trees using **scikit-learn**. Later we will switch to so-called **random forests**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            N    Z    A Element  Ebinding   Eapprox\n",
      "A                                                  \n",
      "1   0       0    1    1       H  0.000000  0.000000\n",
      "2   1       1    1    2       H  1.112283  1.112283\n",
      "3   2       2    1    3       H  2.827265  2.827265\n",
      "4   6       2    2    4      He  7.073915  7.073915\n",
      "5   9       3    2    5      He  5.512132  5.512132\n",
      "6   14      3    3    6      Li  5.332331  5.332331\n",
      "7   19      4    3    7      Li  5.606439  5.606439\n",
      "8   24      4    4    8      Be  7.062435  7.062435\n",
      "9   29      5    4    9      Be  6.462668  6.462668\n",
      "10  34      6    4   10      Be  6.497630  6.497630\n",
      "11  40      6    5   11       B  6.927732  6.927732\n",
      "12  46      6    6   12       C  7.680144  7.680144\n",
      "13  52      7    6   13       C  7.469849  7.469849\n",
      "14  57      8    6   14       C  7.520319  7.520319\n",
      "15  64      8    7   15       N  7.699460  7.699460\n",
      "16  72      8    8   16       O  7.976206  7.976206\n",
      "17  78      9    8   17       O  7.750728  7.750728\n",
      "18  85     10    8   18       O  7.767097  7.773058\n",
      "19  93     10    9   19       F  7.779018  7.773058\n",
      "20  102    10   10   20      Ne  8.032240  8.032240\n",
      "21  110    11   10   21      Ne  7.971713  7.971713\n",
      "22  118    12   10   22      Ne  8.080465  8.080465\n",
      "23  128    12   11   23      Na  8.111493  8.111493\n",
      "24  137    12   12   24      Mg  8.260709  8.260709\n",
      "25  146    13   12   25      Mg  8.223502  8.223502\n",
      "26  154    14   12   26      Mg  8.333870  8.333870\n",
      "27  164    14   13   27      Al  8.331553  8.331553\n",
      "28  174    14   14   28      Si  8.447744  8.447744\n",
      "29  183    15   14   29      Si  8.448635  8.448635\n",
      "30  192    16   14   30      Si  8.520654  8.520654\n",
      "...       ...  ...  ...     ...       ...       ...\n",
      "238 3089  146   92  238       U  7.570125  7.573113\n",
      "239 3099  146   93  239      Np  7.560567  7.558304\n",
      "240 3109  146   94  240      Pu  7.556042  7.558304\n",
      "241 3118  147   94  241      Pu  7.546439  7.546439\n",
      "242 3127  148   94  242      Pu  7.541327  7.541327\n",
      "243 3136  149   94  243      Pu  7.531008  7.527912\n",
      "244 3144  150   94  244      Pu  7.524815  7.527912\n",
      "245 3154  149   96  245      Cm  7.515767  7.513619\n",
      "246 3162  150   96  246      Cm  7.511471  7.513619\n",
      "247 3170  151   96  247      Cm  7.501931  7.499329\n",
      "248 3177  152   96  248      Cm  7.496728  7.499329\n",
      "249 3186  152   97  249      Bk  7.486040  7.482998\n",
      "250 3194  152   98  250      Cf  7.479956  7.482998\n",
      "251 3201  153   98  251      Cf  7.470500  7.470500\n",
      "252 3209  154   98  252      Cf  7.465347  7.465347\n",
      "253 3216  155   98  253      Cf  7.454829  7.452027\n",
      "254 3224  156   98  254      Cf  7.449225  7.452027\n",
      "255 3232  156   99  255      Es  7.437821  7.434800\n",
      "256 3241  156  100  256      Fm  7.431780  7.434800\n",
      "257 3248  157  100  257      Fm  7.422194  7.422194\n",
      "258 3256  157  101  258      Md  7.409675  7.409675\n",
      "259 3264  157  102  259      No  7.399974  7.399974\n",
      "260 3275  154  106  260      Sg  7.342562  7.342562\n",
      "261 3280  157  104  261      Rf  7.371384  7.371384\n",
      "262 3289  156  106  262      Sg  7.341185  7.341185\n",
      "264 3304  156  108  264      Hs  7.298375  7.298375\n",
      "265 3310  157  108  265      Hs  7.296247  7.297260\n",
      "266 3317  158  108  266      Hs  7.298273  7.297260\n",
      "269 3338  159  110  269      Ds  7.250154  7.250154\n",
      "270 3344  160  110  270      Ds  7.253775  7.253775\n",
      "\n",
      "[267 rows x 6 columns]\n",
      "0.009883615646716184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Decision Tree Regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regr_1=DecisionTreeRegressor(max_depth=5)\n",
    "regr_2=DecisionTreeRegressor(max_depth=7)\n",
    "regr_3=DecisionTreeRegressor(max_depth=9)\n",
    "regr_1.fit(X, Energies)\n",
    "regr_2.fit(X, Energies)\n",
    "regr_3.fit(X, Energies)\n",
    "\n",
    "\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "y_3=regr_3.predict(X)\n",
    "Masses['Eapprox'] = y_3\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.plot(A, Energies, color=\"blue\", label=\"Data\", linewidth=2)\n",
    "plt.plot(A, y_1, color=\"red\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.plot(A, y_2, color=\"green\", label=\"max_depth=7\", linewidth=2)\n",
    "plt.plot(A, y_3, color=\"m\", label=\"max_depth=9\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"$A$\")\n",
    "plt.ylabel(\"$E$[MeV]\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "save_fig(\"Masses2016Trees\")\n",
    "plt.show()\n",
    "print(Masses)\n",
    "print(np.mean( (Energies-y_1)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And what about using neural networks?\n",
    "\n",
    "The **seaborn** package allows us to visualize data in an efficient way. Note that we use **scikit-learn**'s multi-layer perceptron (or feed forward neural network) \n",
    "functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "X_train = X\n",
    "Y_train = Energies\n",
    "n_hidden_neurons = 100\n",
    "epochs = 100\n",
    "# store models for later use\n",
    "eta_vals = np.logspace(-5, 1, 7)\n",
    "lmbd_vals = np.logspace(-5, 1, 7)\n",
    "# store the models for later use\n",
    "DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "sns.set()\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = MLPRegressor(hidden_layer_sizes=(n_hidden_neurons), activation='logistic',\n",
    "                            alpha=lmbd, learning_rate_init=eta, max_iter=epochs)\n",
    "        dnn.fit(X_train, Y_train)\n",
    "        DNN_scikit[i][j] = dnn\n",
    "        train_accuracy[i][j] = dnn.score(X_train, Y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=\"viridis\")\n",
    "ax.set_title(\"Training Accuracy\")\n",
    "ax.set_ylabel(\"$\\eta$\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first summary\n",
    "\n",
    "The aim behind these introductory words was to present to you various\n",
    "Python libraries and their functionalities, in particular libraries like\n",
    "**numpy**, **pandas**, **xarray** and **matplotlib** and other that make our life much easier\n",
    "in handling various data sets and visualizing data. \n",
    "\n",
    "Furthermore,\n",
    "**Scikit-Learn** allows us with few lines of code to implement popular\n",
    "Machine Learning algorithms for supervised learning. Later we will meet **Tensorflow**, a powerful library for deep learning. \n",
    "Now it is time to dive more into the details of various methods. We will start with linear regression and try to take a deeper look at what it entails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
