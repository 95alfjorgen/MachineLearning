<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods">

<title>Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</title>


<style type="text/css">
/* bloodish style */

body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em;  color: #8A0808; }
h2 { font-size: 1.6em;  color: #8A0808; }
h3 { font-size: 1.4em;  color: #8A0808; }
h4 { color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
/* pre style removed because it will interfer with pygments */
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Optimization, the central part of any Machine Learning '
               'algortithm',
               2,
               None,
               '___sec0'),
              ('Steepest descent', 2, None, '___sec1'),
              ('More on Steepest descent', 2, None, '___sec2'),
              ('The ideal', 2, None, '___sec3'),
              ('The sensitiveness of the gradient descent', 2, None, '___sec4'),
              ('Gradient Descent Example', 2, None, '___sec5'),
              ('And a corresponding example using _scikit-learn_',
               2,
               None,
               '___sec6'),
              ('Convex functions', 2, None, '___sec7'),
              ('Convex function', 2, None, '___sec8'),
              ('Conditions on convex functions', 2, None, '___sec9'),
              ('More on convex functions', 2, None, '___sec10'),
              ('Some simple problems', 2, None, '___sec11'),
              ('Revisiting our first homework', 2, None, '___sec12'),
              ('Gradient descent example', 2, None, '___sec13'),
              ('The derivative of the cost/loss function', 2, None, '___sec14'),
              ('The Hessian matrix', 2, None, '___sec15'),
              ('Simple program', 2, None, '___sec16'),
              ('Gradient descent and Ridge', 2, None, '___sec17'),
              ('Stochastic Gradient Descent', 2, None, '___sec18'),
              ('Computation of gradients', 2, None, '___sec19'),
              ('SGD example', 2, None, '___sec20'),
              ('The gradient step', 2, None, '___sec21'),
              ('Simple example code', 2, None, '___sec22'),
              ('When do we stop?', 2, None, '___sec23'),
              ('Slightly different approach', 2, None, '___sec24'),
              ('Conjugate gradient (CG) method', 2, None, '___sec25'),
              ('Conjugate gradient method', 2, None, '___sec26'),
              ("Conjugate gradient method, Newton's method first",
               2,
               None,
               '___sec27'),
              ('Simple example and demonstration', 2, None, '___sec28'),
              ('Simple example and demonstration', 2, None, '___sec29'),
              ('Conjugate gradient method', 2, None, '___sec30'),
              ('Conjugate gradient method', 2, None, '___sec31'),
              ('Conjugate gradient method', 2, None, '___sec32'),
              ('Conjugate gradient method', 2, None, '___sec33'),
              ('Conjugate gradient method and iterations', 2, None, '___sec34'),
              ('Conjugate gradient method', 2, None, '___sec35'),
              ('Conjugate gradient method', 2, None, '___sec36'),
              ('Conjugate gradient method', 2, None, '___sec37')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>Sep 20, 2018</h4></center> <!-- date -->
<br>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec0">Optimization, the central part of any Machine Learning algortithm </h2>

<p>
Almost every problem in machine learning and data science starts with
a dataset \( X \), a model \( g(\theta) \), which is a function of the
parameters \( \theta \) and a cost function \( C(X, g(\theta)) \) that allows
us to judge how well the model \( g(\theta) \) explains the observations
\( X \). The model is fit by finding the values of \( \theta \) that minimize
the cost function. Ideally we would be able to solve for \( \theta \)
analytically, however this is not possible in general and we must use
some approximative/numerical method to compute the minimum.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec1">Steepest descent </h2>

<p>
The method of steepest descent The basic idea of gradient descent is
that a function \( F(\mathbf{x}) \), 
\( \mathbf{x} \equiv (x_1,\cdots,x_n) \), decreases fastest if one goes from \( \bf {x} \) in the
direction of the negative gradient \( -\nabla F(\mathbf{x}) \).

<p>
It can be shown that if 
$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k), \ \ \gamma_k &gt; 0
$$

<p>
for \( \gamma_k \) small enough, then \( F(\mathbf{x}_{k+1}) \leq
F(\mathbf{x}_k) \). This means that for a sufficiently small \( \gamma_k \)
we are always moving towards smaller function values, i.e a minimum.

<p>
<!-- !split  -->

<h2 id="___sec2">More on Steepest descent </h2>

<p>
The previous observation is the basis of the method of steepest
descent, which is also referred to as just gradient descent (GD). One
starts with an initial guess \( \mathbf{x}_0 \) for a minimum of \( F \) and
compute new approximations according to

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k), \ \ k \geq 0.
$$

<p>
The parameter \( \gamma_k \) is often referred to as the step length or
the learning rate in the context of Machine Learning.

<p>
<!-- !split  -->

<h2 id="___sec3">The ideal </h2>

<p>
Ideally the sequence \( \{ \mathbf{x}_k \}_{k=0} \) converges to a global minimum of the function \( F \). In general we do not know if we are in a global or local minimum. In the special case when \( F \) is a convex function, all local minima are also global minima, so in this case gradient descent can converge to the global solution. The advantage of this scheme is that it is conceptually simple and straightforward to implement. However the method in this form has some severe limitations:

<p>
In machine learing we are often faced with non-convex high dimensional cost functions with many local minimum. Since GD is deterministic we will get stuck in a local minimum, if the method converges, unless we have a very good intial guess. This also implies that the scheme is sensitive to the chosen initial condition.

<p>
Note that the gradient is a function of \( \mathbf{x} =
(x_1,\cdots,x_n) \) which makes it expensive to compute numerically.

<p>
<!-- !split  -->

<h2 id="___sec4">The sensitiveness of the gradient descent </h2>

<p>
GD is sensitive to the choice of learning rate \( \gamma_k \). This is due
to the fact that we are only guaranteed that \( F(\mathbf{x}_{k+1}) \leq
F(\mathbf{x}_k) \) for sufficiently small \( \gamma_k \). The problem is to
determine an optimal learning rate. If the learning rate is chosen to
small the method will take a long to converge and if it is to large we
can experience erratic behavior.

<p>
Many of these shortcomings can be alleviated by introducing
randomness. One such method is that of Stochastic Gradient Descent
(SGD), see below

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec5">Gradient Descent Example </h2>
We revisit now our simple linear regression example with a linear polynomial.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Importing various packages</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">random</span> <span style="color: #008000; font-weight: bold">import</span> random, seed
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008000; font-weight: bold">import</span> Axes3D
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> cm
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib.ticker</span> <span style="color: #008000; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">sys</span>

x <span style="color: #666666">=</span> <span style="color: #666666">2*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">4+3*</span>x<span style="color: #666666">+</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)

xb <span style="color: #666666">=</span> np<span style="color: #666666">.</span>c_[np<span style="color: #666666">.</span>ones((<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)), x]
theta_linreg <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(xb<span style="color: #666666">.</span>T<span style="color: #666666">.</span>dot(xb))<span style="color: #666666">.</span>dot(xb<span style="color: #666666">.</span>T)<span style="color: #666666">.</span>dot(y)
<span style="color: #008000; font-weight: bold">print</span>(theta_linreg)
theta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">2</span>,<span style="color: #666666">1</span>)

eta <span style="color: #666666">=</span> <span style="color: #666666">0.1</span>
Niterations <span style="color: #666666">=</span> <span style="color: #666666">1000</span>
m <span style="color: #666666">=</span> <span style="color: #666666">100</span>

<span style="color: #008000; font-weight: bold">for</span> <span style="color: #008000">iter</span> <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(Niterations):
    gradients <span style="color: #666666">=</span> <span style="color: #666666">2.0/</span>m<span style="color: #666666">*</span>xb<span style="color: #666666">.</span>T<span style="color: #666666">.</span>dot(xb<span style="color: #666666">.</span>dot(theta)<span style="color: #666666">-</span>y)
    theta <span style="color: #666666">-=</span> eta<span style="color: #666666">*</span>gradients

<span style="color: #008000; font-weight: bold">print</span>(theta)
xnew <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[<span style="color: #666666">0</span>],[<span style="color: #666666">2</span>]])
xbnew <span style="color: #666666">=</span> np<span style="color: #666666">.</span>c_[np<span style="color: #666666">.</span>ones((<span style="color: #666666">2</span>,<span style="color: #666666">1</span>)), xnew]
ypredict <span style="color: #666666">=</span> xbnew<span style="color: #666666">.</span>dot(theta)
ypredict2 <span style="color: #666666">=</span> xbnew<span style="color: #666666">.</span>dot(theta_linreg)
plt<span style="color: #666666">.</span>plot(xnew, ypredict, <span style="color: #BA2121">&quot;r-&quot;</span>)
plt<span style="color: #666666">.</span>plot(xnew, ypredict2, <span style="color: #BA2121">&quot;b-&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, y ,<span style="color: #BA2121">&#39;ro&#39;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>,<span style="color: #666666">2.0</span>,<span style="color: #666666">0</span>, <span style="color: #666666">15.0</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&#39;$x$&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&#39;$y$&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">r&#39;Random numbers &#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec6">And a corresponding example using <b>scikit-learn</b> </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Importing various packages</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">random</span> <span style="color: #008000; font-weight: bold">import</span> random, seed
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> SGDRegressor

x <span style="color: #666666">=</span> <span style="color: #666666">2*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">4+3*</span>x<span style="color: #666666">+</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)

xb <span style="color: #666666">=</span> np<span style="color: #666666">.</span>c_[np<span style="color: #666666">.</span>ones((<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)), x]
theta_linreg <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(xb<span style="color: #666666">.</span>T<span style="color: #666666">.</span>dot(xb))<span style="color: #666666">.</span>dot(xb<span style="color: #666666">.</span>T)<span style="color: #666666">.</span>dot(y)
<span style="color: #008000; font-weight: bold">print</span>(theta_linreg)
sgdreg <span style="color: #666666">=</span> SGDRegressor(n_iter <span style="color: #666666">=</span> <span style="color: #666666">50</span>, penalty<span style="color: #666666">=</span><span style="color: #008000">None</span>, eta0<span style="color: #666666">=0.1</span>)
sgdreg<span style="color: #666666">.</span>fit(x,y<span style="color: #666666">.</span>ravel())
<span style="color: #008000; font-weight: bold">print</span>(sgdreg<span style="color: #666666">.</span>intercept_, sgdreg<span style="color: #666666">.</span>coef_)
</pre></div>
<p>
<!-- !split  -->

<h2 id="___sec7">Convex functions </h2>
Ideally we want our cost/loss function to be convex(concave).

<p>
First we give the definition of a convex set: A set \( C \) in
\( \mathbb{R}^n \) is said to be convex if, for all \( x \) and \( y \) in \( C \) and
all \( t \in (0,1) \) , the point \( (1 &#8722; t)x + ty \) also belongs to
C. Geometrically this means that every point on the line segment
connecting \( x \) and \( y \) is in \( C \) as discussed below.

<p>
The convex subsets of \( \mathbb{R} \) are the intervals of
\( \mathbb{R} \). Examples of convex sets of \( \mathbb{R}^2 \) are the
regular polygons (triangles, rectangles, pentagons, etc...).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec8">Convex function </h2>

<p>
Convex function: Let \( X \subset \mathbb{R}^n \) be a convex set. Assume that the function \( f: X \rightarrow \mathbb{R} \) is continuous, then \( f \) is said to be convex if $$f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t)f(x_2) $$ for all \( x_1, x_2 \in X \) and for all \( t \in [0,1] \). If \( \leq \) is replaced with a strict inequaltiy in the definition, we demand \( x_1 \neq x_2 \) and \( t\in(0,1) \) then \( f \) is said to be strictly convex. For a single variable function, convexity means that if you draw a straight line connecting \( f(x_1) \) and \( f(x_2) \), the value of the function on the interval \( [x_1,x_2] \) is always below the line as illustrated below.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec9">Conditions on convex functions </h2>

<p>
In the following we state first and second-order conditions which
ensures convexity of a function \( f \). We write \( D_f \) to denote the
domain of \( f \), i.e the subset of \( R^n \) where \( f \) is defined. For more
details and proofs we refer to: S. Boyd and L. Vandenberghe. Convex
Optimization. Cambridge University Press, <a href="http://stanford.edu/" target="_blank"><tt>http://stanford.edu/</tt></a>
boyd/cvxbook/, 2004.

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>First order condition.</b>
<p>
Suppose \( f \) is differentiable (i.e \( \nabla f(x) \) is well defined for
all \( x \) in the domain of \( f \)). Then \( f \) is convex if and only if \( D_f \)
is a convex set and $$f(y) \geq f(x) + \nabla f(x)^T (y-x) $$ holds
for all \( x,y \in D_f \). This condition means that for a convex function
the first order Taylor expansion (right hand side above) at any point
a global under estimator of the function. To convince yourself you can
make a drawing of f(x) = x^2+1 and draw the tangent line to \( f(x) \) and
note that it is always below the graph.
</div>


<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Second order condition.</b>
<p>
Assume that \( f \) is twice
differentiable, i.e the Hessian matrix exists at each point in
\( D_f \). Then \( f \) is convex if and only if \( D_f \) is a convex set and its
Hessian is positive semi-definite for all \( x\in D_f \). For a
single-variable function this reduces to \( f''(x) \geq
0 \). Geometrically this means that \( f \) has nonnegative curvature
everywhere.
</div>


<p>
This condition is particularly useful since it gives us an procedure for determining if the function under consideration is convex, apart from using the definition.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec10">More on convex functions </h2>

<p>
The next result is of great importance to us and the reason why we are
going on about convex functions. In machine learning we frequently
have to minimize a loss/cost function in order to find the best
parameters for the model we are considering. Ideally we want the
global minimum, however for high-dimensional models it is hard to know
if we have local or global minimum. However, if the cost/loss function
is convex the following result provides invaluable information:

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Any minimum is global for convex functions.</b>
<p>
Consider the problem of finding \( x \in \mathbb{R}^n \) such that \( f(x) \)
is minimal, where \( f \) is convex and differentiable. Then, any point
\( x^* \) that satisfies \( \nabla f(x^*) = 0 \) is a global minimum.
</div>

This result means that if we know that the cost/loss function is convex and we are able to find a minimum, we are guaranteed that it is a global minimum.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec11">Some simple problems </h2>

<ol>
<li> Show that \( f(x)=x^2 \) is convex for \( x \in \mathbb{R} \) using the definition of convexity.</li>
</ol>

Hint: If you re-write the definition, \( f \) is convex if the following holds for all \( x,y \in D_f \) and any \( \lambda \in [0,1]  \) $$\lambda f(x) + (1-\lambda)f(y) - f(\lambda x + (1-\lambda) y ) \geq 0. $$

<ol>
<li> Using the second order condition show that the following functions are convex on the specified domain.</li>
</ol>

\( f(x) = e^x \) is convex for \( x \in \mathbb{R} \).
\( g(x) = -\ln(x) \) is convex for \( x \in (0,\infty) \).

<ol>
<li> Let \( f(x) = x^2 \) and \( g(x) = e^x \). Show that \( f(g(x)) \) and \( g(f(x)) \) is convex for \( x \in \mathbb{R} \). Also show that if \( f(x) \) is any convex function than \( h(x) = e^{f(x)} \) is convex.</li>
<li> A norm is any function that satisfy the following properties</li>
</ol>

\( f(\alpha x) = |\alpha| f(x) \) for all \( \alpha \in \mathbb{R} \).
\( f(x+y) \leq f(x) + f(y) \)
\( f(x) \leq 0 \) for all \( x \in \mathbb{R}^n \) with equality if and only if \( x = 0 \)
Using the definition of convexity, show that a function satisfying the properties above is convex (the third condition is not needed to show this).

<p>
<!-- !split  -->

<h2 id="___sec12">Revisiting our first homework </h2>

<p>
We will use linear regression as a case study for the gradient descent methods. Linear regression is a great test case for the gradient descent methods discussed in the lectures since it has several desirable properties such as:

<ol>
<li> An analytical solution (recall homework set 1).</li>
<li> The gradient can be computed analytically.</li>
<li> The cost function is convex which guarantees that gradient descent converges for small enough learning rates</li>
</ol>

We revisit the example from homework set 1 where we had 
$$
y_i = 5x_i^2 + 0.1\xi_i, \ i=1,\cdots,100
$$

with \( x_i \in [0,1]  \) chosen randomly with a uniform distribution. Additionally \( \xi_i \) represents stochastic noise chosen according to a normal distribution \( \cal {N}(0,1) \). 
The linear regression model is given by 
$$
h_\theta(x) = \hat{y} = \theta_0 + \theta_1 x,
$$

such that 
$$
\hat{y}_i = \theta_0 + \theta_1 x_i.
$$

<p>
<!-- !split  -->

<h2 id="___sec13">Gradient descent example </h2>

<p>
Let \( \mathbf{y} = (y_1,\cdots,y_n)^T \), \( \mathbf{\hat{y}} = (\hat{y}_1,\cdots,\hat{y}_n)^T \) and \( \theta = (\theta_0, \theta_1)^T \)

<p>
t is convenient to write \( \mathbf{\hat{y}} = X\theta \) where \( X \in \mathbb{R}^{100 \times 2}  \) is the design matrix given by
$$
\begin{equation}
X \equiv \begin{bmatrix}
1 &amp; x_1  \\
\vdots &amp; \vdots  \\
1 &amp; x_{100} &amp;  \\
\end{bmatrix}.
\label{_auto1}
\end{equation}
$$

The loss function is given by 
$$
C(\theta) = ||X\theta-\mathbf{y}||^2 = ||X\theta||^2 - 2 \mathbf{y}^T X\theta + ||\mathbf{y}||^2 = \sum_{i=1}^{100} (\theta_0 + \theta_1 x_i)^2 - 2 y_i (\theta_0 + \theta_1 x_i) + y_i^2 
$$

and we want to find \( \theta \) such that \( C(\theta) \) is minimized.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec14">The derivative of the cost/loss function </h2>

<p>
Computing \( \partial C(\theta) / \partial \theta_0 \) and \( \partial C(\theta) / \partial \theta_1 \) we can show  that the gradient can be written as
$$
\nabla_\theta C(\theta) = (\partial C(\theta) / \partial \theta_0, \partial C(\theta) / \partial \theta_1)^T = 2\begin{bmatrix} \sum_{i=1}^{100} \left(\theta_0+\theta_1x_i-y_i\right) \\
\sum_{i=1}^{100}\left( x_i (\theta_0+\theta_1x_i)-y_ix_i\right) \\
\end{bmatrix} = 2X^T(X\theta - \mathbf{y}), 
$$

where \( X \) is the design matrix defined above.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec15">The Hessian matrix </h2>
The Hessian matrix of \( C(\theta) \) is given by 
$$
\hat{H} \equiv \begin{bmatrix}
\frac{\partial^2 C(\theta)}{\partial \theta_0^2} &amp; \frac{\partial^2 C(\theta)}{\partial \theta_0 \partial \theta_1}  \\
\frac{\partial^2 C(\theta)}{\partial \theta_0 \partial \theta_1} &amp; \frac{\partial^2 C(\theta)}{\partial \theta_1^2} &amp;  \\
\end{bmatrix} = 2X^T X.
$$

This result implies that \( C(\theta) \) is a convex function since the matrix \( X^T X \) always is positive semi-definite.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec16">Simple program </h2>

<p>
We can now write a program that minimizes \( C(\theta) \) using the gradient descent method with a constant learning rate \( \gamma \) according to 
$$
\theta_{k+1} = \theta_k - \gamma \nabla_\theta C(\theta_k), \ k=0,1,\cdots 
$$

<p>
We can use the expression we computed for the gradient and let use a
\( \theta_0 \) be chosen randomly and let \( \gamma = 0.001 \). Stop iterating
when \( ||\nabla_\theta C(\theta_k) || &lt; \epsilon = 10^{-8} \).

<p>
And finally we can compare our solution for \( \theta \) with the analytic result given by 
\( \theta= (X^TX)^{-1} X^T \mathbf{y} \).
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">The following setup is just a suggestion, feel free to write it the way you like.</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #408080; font-style: italic">#Setup problem described in the exercise</span>
N  <span style="color: #666666">=</span> <span style="color: #666666">100</span> <span style="color: #408080; font-style: italic">#Nr of datapoints</span>
M  <span style="color: #666666">=</span> <span style="color: #666666">2</span> <span style="color: #408080; font-style: italic">#Nr of features</span>
x  <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(N) <span style="color: #408080; font-style: italic">#Uniformly generated x-values in [0,1]</span>
y  <span style="color: #666666">=</span> <span style="color: #666666">5*</span>x<span style="color: #666666">**2</span> <span style="color: #666666">+</span> <span style="color: #666666">0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(N)
X  <span style="color: #666666">=</span> np<span style="color: #666666">.</span>c_[np<span style="color: #666666">.</span>ones(N),x] <span style="color: #408080; font-style: italic">#Construct design matrix</span>

<span style="color: #408080; font-style: italic">#Compute theta according to normal equations to compare with GD solution</span>
Xt_X_inv <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(np<span style="color: #666666">.</span>dot(X<span style="color: #666666">.</span>T,X))
Xt_y     <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(X<span style="color: #666666">.</span>transpose(),y)
theta_NE <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(Xt_X_inv,Xt_y)
<span style="color: #008000; font-weight: bold">print</span>(theta_NE)
</pre></div>
<p>
<!-- !split  -->

<h2 id="___sec17">Gradient descent and Ridge </h2>

<p>
We have also discussed Ridge regression where the loss function contains a regularized given by the \( L_2 \) norm of \( \theta \), 
$$
C_{\text{ridge}}(\theta) = ||X\theta -\mathbf{y}||^2 + \lambda ||\theta||^2, \ \lambda \geq 0.
$$

<p>
In order to minimize \( C_{\text{ridge}}(\theta) \) using GD we only have adjust the gradient as follows 
$$
\nabla_\theta C_{\text{ridge}}(\theta)  = 2\begin{bmatrix} \sum_{i=1}^{100} \left(\theta_0+\theta_1x_i-y_i\right) \\
\sum_{i=1}^{100}\left( x_i (\theta_0+\theta_1x_i)-y_ix_i\right) \\
\end{bmatrix} + 2\lambda\begin{bmatrix} \theta_0 \\ \theta_1\end{bmatrix} = 2 (X^T(X\theta - \mathbf{y})+\lambda \theta).
$$

<p>
We can now extend our program to minimize \( C_{\text{ridge}}(\theta) \) using gradient descent and compare with the analytical solution given by 
$$
\theta_{\text{ridge}} = \left(X^T X + \lambda I_{2 \times 2} \right)^{-1} X^T \mathbf{y},
$$

for \( \lambda = {0,1,10,50,100} \) (\( \lambda = 0 \) corresponds to ordinary least squares). 
We can then compute \( ||\theta_{\text{ridge}}|| \) for each \( \lambda \).

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">The following setup is just a suggestion, feel free to write it the way you like.</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #408080; font-style: italic">#Setup problem described in the exercise</span>
N  <span style="color: #666666">=</span> <span style="color: #666666">100</span> <span style="color: #408080; font-style: italic">#Nr of datapoints</span>
M  <span style="color: #666666">=</span> <span style="color: #666666">2</span>   <span style="color: #408080; font-style: italic">#Nr of features</span>
x  <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(N)
y  <span style="color: #666666">=</span> <span style="color: #666666">5*</span>x<span style="color: #666666">**2</span> <span style="color: #666666">+</span> <span style="color: #666666">0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(N)


<span style="color: #408080; font-style: italic">#Compute analytic theta for Ridge regression </span>
X    <span style="color: #666666">=</span> np<span style="color: #666666">.</span>c_[np<span style="color: #666666">.</span>ones(N),x]
XT_X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(X<span style="color: #666666">.</span>T,X)

l  <span style="color: #666666">=</span> <span style="color: #666666">0.1</span> <span style="color: #408080; font-style: italic">#Ridge parameter lambda</span>
Id <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(XT_X<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>])

Z <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(XT_X<span style="color: #666666">+</span>l<span style="color: #666666">*</span>Id)
theta_ridge <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(Z,np<span style="color: #666666">.</span>dot(X<span style="color: #666666">.</span>T,y))

<span style="color: #008000; font-weight: bold">print</span>(theta_ridge)
<span style="color: #008000; font-weight: bold">print</span>(np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>norm(theta_ridge)) <span style="color: #408080; font-style: italic">#||theta||</span>
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec18">Stochastic Gradient Descent </h2>

<p>
Stochastic gradient descent (SGD) and variants thereof address some of
the shortcomings of the Gradient descent method discussed above.

<p>
The underlying idea of SGD comes from the observation that the cost
function, which we want to minimize, can almost always be written as a
sum over \( n \) datapoints \( \{\mathbf{x}_i\}_{i=1}^n \),
$$
C(\mathbf{\theta}) = \sum_{i=1}^n c_i(\mathbf{x}_i,
\mathbf{\theta}). 
$$

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec19">Computation of gradients </h2>

<p>
This in turn means that the gradient can be
computed as a sum over \( i \)-gradients 
$$
\nabla_\theta C(\mathbf{\theta}) = \sum_i^n \nabla_\theta c_i(\mathbf{x}_i,
\mathbf{\theta}).
$$

<p>
Stochasticity/randomness is introduced by only taking the
gradient on a subset of the data called minibatches.  If there are \( n \)
datapoints and the size of each minibatch is \( M \), there will be \( n/M \)
minibatches. We denote these minibatches by \( B_k \) where
\( k=1,\cdots,n/M \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec20">SGD example </h2>
As an example, suppose we have \( 10 \) datapoints \( ( \mathbf{x}_1,
\cdots, \mathbf{x}_{10} ) \) and we choose to have \( M=5 \) minibathces,
then each minibatch contains two datapoints. In particular we have
\( B_1 = (\mathbf{x}_1,\mathbf{x}_2), \cdots, B_5 =
(\mathbf{x}_9,\mathbf{x}_{10}) \). Note that if you choose \( M=1 \) you
have only a single batch with all datapoints and on the other extreme,
you may choose \( M=n \) resulting in a minibatch for each datapoint, i.e
\( B_k = \mathbf{x}_k \).

<p>
The idea is now to approximate the gradient by replacing the sum over
all datapoints with a sum over the datapoints in one the minibatches
picked at random in each gradient descent step 
$$
\nabla_\theta
C(\mathbf{\theta}) = \sum_{i=1}^n \nabla_\theta c_i(\mathbf{x}_i,
\mathbf{\theta}) \rightarrow \sum_{i \in B_k}^n \nabla_\theta
c_i(\mathbf{x}_i, \mathbf{\theta}).
$$

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec21">The gradient step </h2>

<p>
Thus a gradient descent step now looks like 
$$
\theta_{j+1} = \theta_j - \gamma_j \sum_{i \in B_k}^n \nabla_\theta c_i(\mathbf{x}_i,
\mathbf{\theta})
$$

<p>
where \( k \) is picked at random with equal
probability from \( [1,n/M] \). An iteration over the number of
minibathces (n/M) is commonly referred to as an epoch. Thus it is
typical to choose a number of epochs and for each epoch iterate over
the number of minibatches, as exemplified in the code below.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec22">Simple example code </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span> 

n <span style="color: #666666">=</span> <span style="color: #666666">100</span> <span style="color: #408080; font-style: italic">#100 datapoints </span>
M <span style="color: #666666">=</span> <span style="color: #666666">5</span>   <span style="color: #408080; font-style: italic">#size of each minibatch</span>
m <span style="color: #666666">=</span> <span style="color: #008000">int</span>(n<span style="color: #666666">/</span>M) <span style="color: #408080; font-style: italic">#number of minibatches</span>
n_epochs <span style="color: #666666">=</span> <span style="color: #666666">10</span> <span style="color: #408080; font-style: italic">#number of epochs</span>

j <span style="color: #666666">=</span> <span style="color: #666666">0</span>
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,n_epochs<span style="color: #666666">+1</span>):
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(m):
        k <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randint(m) <span style="color: #408080; font-style: italic">#Pick the k-th minibatch at random</span>
        <span style="color: #408080; font-style: italic">#Compute the gradient using the data in minibatch Bk</span>
        <span style="color: #408080; font-style: italic">#Compute new suggestion for theta</span>
        j <span style="color: #666666">+=</span> <span style="color: #666666">1</span>
</pre></div>
<p>
Taking the gradient only on a subset of the data has two important
benefits. First, it introduces randomness which decreases the chance
that our opmization scheme gets stuck in a local minima. Second, if
the size of the minibatches are small relative to the number of
datapoints (\( M &lt; n \)), the computation of the gradient is much
cheaper since we sum over the datapoints in the k-th minibatch and not
all \( n \) datapoints.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec23">When do we stop? </h2>

<p>
A natural question is when do we stop the search for a new minimum?
One possibility is to compute the full gradient after a given number
of epochs and check if the norm of the gradient is smaller than some
threshold and stop if true. However, the condition that the gradient
is zero is valid also for local minima, so this would only tell us
that we are close to a local/global minimum. However, we could also
evaluate the cost function at this point, store the result and
continue the search. If the test kicks in at a later stage we can
compare the values of the cost function and keep the \( \theta \) that
gave the lowest value.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec24">Slightly different approach </h2>

<p>
Another approach is to let the step length \( \gamma_j \) depend on the
number of epochs in such a way that it becomes very small after a
reasonable time such that we do not move at all.

<p>
As an example, let \( e = 0,1,2,3,\cdots \) denote the current epoch and let \( t_0, t_1 &gt; 0 \) be two fixed numbers. Furthermore, let \( t = e \cdot m + i \) where \( m \) is the number of minibatches and \( i=0,\cdots,m-1 \). Then the function $$\gamma_j(t; t_0, t_1) = \frac{t_0}{t+t_1} $$ goes to zero as the number of epochs gets large. I.e. we start with a step length \( \gamma_j (0; t_0, t_1) = t_0/t_1 \) which decays in <em>time</em> \( t \).

<p>
In this way we can fix the number of epochs, compute \( \theta \) and
evaluate the cost function at the end. Repeating the computation will
give a different result since the scheme is random by design. Then we
pick the final \( \theta \) that gives the lowest value of the cost
function.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span> 

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">step_length</span>(t,t0,t1):
    <span style="color: #008000; font-weight: bold">return</span> t0<span style="color: #666666">/</span>(t<span style="color: #666666">+</span>t1)

n <span style="color: #666666">=</span> <span style="color: #666666">100</span> <span style="color: #408080; font-style: italic">#100 datapoints </span>
M <span style="color: #666666">=</span> <span style="color: #666666">5</span>   <span style="color: #408080; font-style: italic">#size of each minibatch</span>
m <span style="color: #666666">=</span> <span style="color: #008000">int</span>(n<span style="color: #666666">/</span>M) <span style="color: #408080; font-style: italic">#number of minibatches</span>
n_epochs <span style="color: #666666">=</span> <span style="color: #666666">500</span> <span style="color: #408080; font-style: italic">#number of epochs</span>
t0 <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
t1 <span style="color: #666666">=</span> <span style="color: #666666">10</span>

gamma_j <span style="color: #666666">=</span> t0<span style="color: #666666">/</span>t1
j <span style="color: #666666">=</span> <span style="color: #666666">0</span>
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,n_epochs<span style="color: #666666">+1</span>):
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(m):
        k <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randint(m) <span style="color: #408080; font-style: italic">#Pick the k-th minibatch at random</span>
        <span style="color: #408080; font-style: italic">#Compute the gradient using the data in minibatch Bk</span>
        <span style="color: #408080; font-style: italic">#Compute new suggestion for theta</span>
        t <span style="color: #666666">=</span> epoch<span style="color: #666666">*</span>m<span style="color: #666666">+</span>i
        gamma_j <span style="color: #666666">=</span> step_length(t,t0,t1)
        j <span style="color: #666666">+=</span> <span style="color: #666666">1</span>

<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&quot;gamma_j after </span><span style="color: #BB6688; font-weight: bold">%d</span><span style="color: #BA2121"> epochs: </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> (n_epochs,gamma_j))
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec25">Conjugate gradient (CG) method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The success of the CG method  for finding solutions of non-linear problems is based
on the theory of conjugate gradients for linear systems of equations. It belongs
to the class of iterative methods for solving problems from linear algebra of the type
$$
\begin{equation*}
  \hat{A}\hat{x} = \hat{b}.
\end{equation*}
$$

In the iterative process we end up with a problem like

$$
\begin{equation*}
  \hat{r}= \hat{b}-\hat{A}\hat{x},
\end{equation*}
$$

where \( \hat{r} \) is the so-called residual or error in the iterative process.

<p>
When we have found the exact solution, \( \hat{r}=0 \).
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec26">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
The residual is zero when we reach the minimum of the quadratic equation
$$
\begin{equation*}
  P(\hat{x})=\frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T\hat{b},
\end{equation*}
$$

with the constraint that the matrix \( \hat{A} \) is positive definite and symmetric.
If we search for a minimum of the quantum mechanical  variance, then the matrix 
\( \hat{A} \), which is called the Hessian, is given by the second-derivative of the function we want to minimize.  This quantity is always positive definite.  In our case this corresponds normally to the second derivative of the energy.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec27">Conjugate gradient method, Newton's method first </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We seek the minimum of the energy or the variance as function of various variational parameters. 
In our case we have thus a function \( f \) whose minimum we are seeking.
In Newton's method we set \( \nabla f = 0 \) and we can thus compute the next iteration point
$$
\begin{equation*}
\hat{x}-\hat{x}_i=\hat{A}^{-1}\nabla f(\hat{x}_i).
\end{equation*}
$$

Subtracting this equation from that of \( \hat{x}_{i+1} \) we have
$$
\begin{equation*}
\hat{x}_{i+1}-\hat{x}_i=\hat{A}^{-1}(\nabla f(\hat{x}_{i+1})-\nabla f(\hat{x}_i)).
\end{equation*}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec28">Simple example and demonstration </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The function \( f \) can be either the energy or the variance.  If we choose the energy then we have
$$
\begin{equation*}
\hat{\alpha}_{i+1}-\hat{\alpha}_i=\hat{A}^{-1}(\nabla E(\hat{\alpha}_{i+1})-\nabla E(\hat{\alpha}_i)).
\end{equation*}
$$

In the simple harmonic oscillator model, the gradient and the Hessian \( \hat{A} \) are
$$
\begin{equation*}
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\end{equation*}
$$

and a second derivative which is always positive (meaning that we find a minimum)
$$
\begin{equation*}
\hat{A}= \frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\end{equation*}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec29">Simple example and demonstration </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We get then
$$
\begin{equation*}
\alpha_{i+1}=\frac{4}{3}\alpha_i-\frac{\alpha_i^4}{3\alpha_{i+1}^3},
\end{equation*}
$$

which can be rewritten as
$$
\begin{equation*}
\alpha_{i+1}^4-\frac{4}{3}\alpha_i\alpha_{i+1}^4+\frac{1}{3}\alpha_i^4.
\end{equation*}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec30">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In the CG method we define so-called conjugate directions and two vectors 
\( \hat{s} \) and \( \hat{t} \)
are said to be
conjugate if
$$
\begin{equation*}
\hat{s}^T\hat{A}\hat{t}= 0.
\end{equation*}
$$

The philosophy of the CG method is to perform searches in various conjugate directions
of our vectors \( \hat{x}_i \) obeying the above criterion, namely
$$
\begin{equation*}
\hat{x}_i^T\hat{A}\hat{x}_j= 0.
\end{equation*}
$$

Two vectors are conjugate if they are orthogonal with respect to 
this inner product. Being conjugate is a symmetric relation: if \( \hat{s} \) is conjugate to \( \hat{t} \), then \( \hat{t} \) is conjugate to \( \hat{s} \).
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec31">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
An example is given by the eigenvectors of the matrix
$$
\begin{equation*}
\hat{v}_i^T\hat{A}\hat{v}_j= \lambda\hat{v}_i^T\hat{v}_j,
\end{equation*}
$$

which is zero unless \( i=j \).
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec32">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Assume now that we have a symmetric positive-definite matrix \( \hat{A} \) of size
\( n\times n \). At each iteration \( i+1 \) we obtain the conjugate direction of a vector
$$
\begin{equation*}
\hat{x}_{i+1}=\hat{x}_{i}+\alpha_i\hat{p}_{i}. 
\end{equation*}
$$

We assume that \( \hat{p}_{i} \) is a sequence of \( n \) mutually conjugate directions. 
Then the \( \hat{p}_{i} \)  form a basis of \( R^n \) and we can expand the solution 
$  \hat{A}\hat{x} = \hat{b}$ in this basis, namely

$$
\begin{equation*}
  \hat{x}  = \sum^{n}_{i=1} \alpha_i \hat{p}_i.
\end{equation*}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec33">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The coefficients are given by
$$
\begin{equation*}
    \mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\end{equation*}
$$

Multiplying with \( \hat{p}_k^T \)  from the left gives

$$
\begin{equation*}
  \hat{p}_k^T \hat{A}\hat{x} = \sum^{n}_{i=1} \alpha_i\hat{p}_k^T \hat{A}\hat{p}_i= \hat{p}_k^T \hat{b},
\end{equation*}
$$

and we can define the coefficients \( \alpha_k \) as

$$
\begin{equation*}
    \alpha_k = \frac{\hat{p}_k^T \hat{b}}{\hat{p}_k^T \hat{A} \hat{p}_k}
\end{equation*}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec34">Conjugate gradient method and iterations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
If we choose the conjugate vectors \( \hat{p}_k \) carefully, 
then we may not need all of them to obtain a good approximation to the solution 
\( \hat{x} \). 
We want to regard the conjugate gradient method as an iterative method. 
This will us to solve systems where \( n \) is so large that the direct 
method would take too much time.

<p>
We denote the initial guess for \( \hat{x} \) as \( \hat{x}_0 \). 
We can assume without loss of generality that
$$
\begin{equation*}
\hat{x}_0=0,
\end{equation*}
$$

or consider the system
$$
\begin{equation*}
\hat{A}\hat{z} = \hat{b}-\hat{A}\hat{x}_0,
\end{equation*}
$$

instead.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec35">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
One can show that the solution \( \hat{x} \) is also the unique minimizer of the quadratic form
$$
\begin{equation*}
  f(\hat{x}) = \frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T \hat{x} , \quad \hat{x}\in\mathbf{R}^n. 
\end{equation*}
$$

This suggests taking the first basis vector \( \hat{p}_1 \) 
to be the gradient of \( f \) at \( \hat{x}=\hat{x}_0 \), 
which equals
$$
\begin{equation*}
\hat{A}\hat{x}_0-\hat{b},
\end{equation*}
$$

and 
\( \hat{x}_0=0 \) it is equal \( -\hat{b} \).
The other vectors in the basis will be conjugate to the gradient, 
hence the name conjugate gradient method.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec36">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let  \( \hat{r}_k \) be the residual at the \( k \)-th step:
$$
\begin{equation*}
\hat{r}_k=\hat{b}-\hat{A}\hat{x}_k.
\end{equation*}
$$

Note that \( \hat{r}_k \) is the negative gradient of \( f \) at 
\( \hat{x}=\hat{x}_k \), 
so the gradient descent method would be to move in the direction \( \hat{r}_k \). 
Here, we insist that the directions \( \hat{p}_k \) are conjugate to each other, 
so we take the direction closest to the gradient \( \hat{r}_k \)  
under the conjugacy constraint. 
This gives the following expression
$$
\begin{equation*}
\hat{p}_{k+1}=\hat{r}_k-\frac{\hat{p}_k^T \hat{A}\hat{r}_k}{\hat{p}_k^T\hat{A}\hat{p}_k} \hat{p}_k.
\end{equation*}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec37">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can also  compute the residual iteratively as
$$
\begin{equation*}
\hat{r}_{k+1}=\hat{b}-\hat{A}\hat{x}_{k+1},
 \end{equation*}
$$

which equals
$$
\begin{equation*}
\hat{b}-\hat{A}(\hat{x}_k+\alpha_k\hat{p}_k),
 \end{equation*}
$$

or
$$
\begin{equation*}
(\hat{b}-\hat{A}\hat{x}_k)-\alpha_k\hat{A}\hat{p}_k,
 \end{equation*}
$$

which gives

$$
\begin{equation*}
\hat{r}_{k+1}=\hat{r}_k-\hat{A}\hat{p}_{k},
 \end{equation*}
$$
</div>


<p>

<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2018, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

