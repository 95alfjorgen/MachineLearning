<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods">

<title>Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Optimization, the central part of any Machine Learning '
               'algortithm',
               2,
               None,
               '___sec0'),
              ('Steepest descent', 2, None, '___sec1'),
              ('More on Steepest descent', 2, None, '___sec2'),
              ('The ideal', 2, None, '___sec3'),
              ('The sensitiveness of the gradient descent', 2, None, '___sec4'),
              ('Convex functions', 2, None, '___sec5'),
              ('Convex function', 2, None, '___sec6'),
              ('Conditions on convex functions', 2, None, '___sec7'),
              ('More on convex functions', 2, None, '___sec8'),
              ('Some simple problems', 2, None, '___sec9'),
              ('Revisiting our first homework', 2, None, '___sec10'),
              ('Gradient descent example', 2, None, '___sec11'),
              ('The derivative of the cost/loss function', 2, None, '___sec12'),
              ('The Hessian matrix', 2, None, '___sec13'),
              ('Simple program', 2, None, '___sec14'),
              ('Gradient Descent Example', 2, None, '___sec15'),
              ('And a corresponding example using _scikit-learn_',
               2,
               None,
               '___sec16'),
              ('Gradient descent and Ridge', 2, None, '___sec17'),
              ('Stochastic Gradient Descent', 2, None, '___sec18'),
              ('Computation of gradients', 2, None, '___sec19'),
              ('SGD example', 2, None, '___sec20'),
              ('The gradient step', 2, None, '___sec21'),
              ('Simple example code', 2, None, '___sec22'),
              ('When do we stop?', 2, None, '___sec23'),
              ('Slightly different approach', 2, None, '___sec24'),
              ('Conjugate gradient (CG) method', 2, None, '___sec25'),
              ('Conjugate gradient method', 2, None, '___sec26'),
              ("Conjugate gradient method, Newton's method first",
               2,
               None,
               '___sec27'),
              ('Conjugate gradient method', 2, None, '___sec28'),
              ('Conjugate gradient method', 2, None, '___sec29'),
              ('Conjugate gradient method', 2, None, '___sec30'),
              ('Conjugate gradient method', 2, None, '___sec31'),
              ('Conjugate gradient method and iterations', 2, None, '___sec32'),
              ('Conjugate gradient method', 2, None, '___sec33'),
              ('Conjugate gradient method', 2, None, '___sec34'),
              ('Conjugate gradient method', 2, None, '___sec35')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="Splines-bs.html">Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._Splines-bs001.html#___sec0" style="font-size: 80%;">Optimization, the central part of any Machine Learning algortithm</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs002.html#___sec1" style="font-size: 80%;">Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs003.html#___sec2" style="font-size: 80%;">More on Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs004.html#___sec3" style="font-size: 80%;">The ideal</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs005.html#___sec4" style="font-size: 80%;">The sensitiveness of the gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs006.html#___sec5" style="font-size: 80%;">Convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs007.html#___sec6" style="font-size: 80%;">Convex function</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs008.html#___sec7" style="font-size: 80%;">Conditions on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs009.html#___sec8" style="font-size: 80%;">More on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs010.html#___sec9" style="font-size: 80%;">Some simple problems</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs011.html#___sec10" style="font-size: 80%;">Revisiting our first homework</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs012.html#___sec11" style="font-size: 80%;">Gradient descent example</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs013.html#___sec12" style="font-size: 80%;">The derivative of the cost/loss function</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs014.html#___sec13" style="font-size: 80%;">The Hessian matrix</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs015.html#___sec14" style="font-size: 80%;">Simple program</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs016.html#___sec15" style="font-size: 80%;">Gradient Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs017.html#___sec16" style="font-size: 80%;">And a corresponding example using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec17" style="font-size: 80%;">Gradient descent and Ridge</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs019.html#___sec18" style="font-size: 80%;">Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs020.html#___sec19" style="font-size: 80%;">Computation of gradients</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs021.html#___sec20" style="font-size: 80%;">SGD example</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs022.html#___sec21" style="font-size: 80%;">The gradient step</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs023.html#___sec22" style="font-size: 80%;">Simple example code</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs024.html#___sec23" style="font-size: 80%;">When do we stop?</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs025.html#___sec24" style="font-size: 80%;">Slightly different approach</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs026.html#___sec25" style="font-size: 80%;">Conjugate gradient (CG) method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs027.html#___sec26" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs028.html#___sec27" style="font-size: 80%;">Conjugate gradient method, Newton's method first</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs029.html#___sec28" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs030.html#___sec29" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs031.html#___sec30" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs032.html#___sec31" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs033.html#___sec32" style="font-size: 80%;">Conjugate gradient method and iterations</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs034.html#___sec33" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs035.html#___sec34" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs036.html#___sec35" style="font-size: 80%;">Conjugate gradient method</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0018"></a>
<!-- !split  -->

<h2 id="___sec17" class="anchor">Gradient descent and Ridge </h2>

<p>
We have also discussed Ridge regression where the loss function contains a regularized given by the \( L_2 \) norm of \( \beta \), 
$$
C_{\text{ridge}}(\beta) = ||X\beta -\mathbf{y}||^2 + \lambda ||\beta||^2, \ \lambda \geq 0.
$$

<p>
In order to minimize \( C_{\text{ridge}}(\beta) \) using GD we only have adjust the gradient as follows 
$$
\nabla_\beta C_{\text{ridge}}(\beta)  = 2\begin{bmatrix} \sum_{i=1}^{100} \left(\beta_0+\beta_1x_i-y_i\right) \\
\sum_{i=1}^{100}\left( x_i (\beta_0+\beta_1x_i)-y_ix_i\right) \\
\end{bmatrix} + 2\lambda\begin{bmatrix} \beta_0 \\ \beta_1\end{bmatrix} = 2 (X^T(X\beta - \mathbf{y})+\lambda \beta).
$$

<p>
We can now extend our program to minimize \( C_{\text{ridge}}(\beta) \) using gradient descent and compare with the analytical solution given by 
$$
\beta_{\text{ridge}} = \left(X^T X + \lambda I_{2 \times 2} \right)^{-1} X^T \mathbf{y},
$$

for \( \lambda = {0,1,10,50,100} \) (\( \lambda = 0 \) corresponds to ordinary least squares). 
We can then compute \( ||\beta_{\text{ridge}}|| \) for each \( \lambda \).

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">The following setup is just a suggestion, feel free to write it the way you like.</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #408080; font-style: italic">#Setup problem described in the exercise</span>
N  <span style="color: #666666">=</span> <span style="color: #666666">100</span> <span style="color: #408080; font-style: italic">#Nr of datapoints</span>
M  <span style="color: #666666">=</span> <span style="color: #666666">2</span>   <span style="color: #408080; font-style: italic">#Nr of features</span>
x  <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(N)
y  <span style="color: #666666">=</span> <span style="color: #666666">5*</span>x<span style="color: #666666">**2</span> <span style="color: #666666">+</span> <span style="color: #666666">0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(N)


<span style="color: #408080; font-style: italic">#Compute analytic beta for Ridge regression </span>
X    <span style="color: #666666">=</span> np<span style="color: #666666">.</span>c_[np<span style="color: #666666">.</span>ones(N),x]
XT_X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(X<span style="color: #666666">.</span>T,X)

l  <span style="color: #666666">=</span> <span style="color: #666666">0.1</span> <span style="color: #408080; font-style: italic">#Ridge parameter lambda</span>
Id <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(XT_X<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>])

Z <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(XT_X<span style="color: #666666">+</span>l<span style="color: #666666">*</span>Id)
beta_ridge <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(Z,np<span style="color: #666666">.</span>dot(X<span style="color: #666666">.</span>T,y))

<span style="color: #008000; font-weight: bold">print</span>(beta_ridge)
<span style="color: #008000; font-weight: bold">print</span>(np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>norm(beta_ridge)) <span style="color: #408080; font-style: italic">#||beta||</span>
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._Splines-bs017.html">&laquo;</a></li>
  <li><a href="._Splines-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._Splines-bs010.html">11</a></li>
  <li><a href="._Splines-bs011.html">12</a></li>
  <li><a href="._Splines-bs012.html">13</a></li>
  <li><a href="._Splines-bs013.html">14</a></li>
  <li><a href="._Splines-bs014.html">15</a></li>
  <li><a href="._Splines-bs015.html">16</a></li>
  <li><a href="._Splines-bs016.html">17</a></li>
  <li><a href="._Splines-bs017.html">18</a></li>
  <li class="active"><a href="._Splines-bs018.html">19</a></li>
  <li><a href="._Splines-bs019.html">20</a></li>
  <li><a href="._Splines-bs020.html">21</a></li>
  <li><a href="._Splines-bs021.html">22</a></li>
  <li><a href="._Splines-bs022.html">23</a></li>
  <li><a href="._Splines-bs023.html">24</a></li>
  <li><a href="._Splines-bs024.html">25</a></li>
  <li><a href="._Splines-bs025.html">26</a></li>
  <li><a href="._Splines-bs026.html">27</a></li>
  <li><a href="._Splines-bs027.html">28</a></li>
  <li><a href="">...</a></li>
  <li><a href="._Splines-bs036.html">37</a></li>
  <li><a href="._Splines-bs019.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    

