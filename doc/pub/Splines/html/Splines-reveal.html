\
<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Data Analysis and Machine Learning  Lectures: Cubic Splines and Gradient Methods">

<title>Data Analysis and Machine Learning  Lectures: Cubic Splines and Gradient Methods</title>







<!-- reveal.js: http://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
    hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .reveal .alert-text-small   { font-size: 80%;  }
    .reveal .alert-text-large   { font-size: 130%; }
    .reveal .alert-text-normal  { font-size: 90%;  }
    .reveal .alert {
             padding:8px 35px 8px 14px; margin-bottom:18px;
             text-shadow:0 1px 0 rgba(255,255,255,0.5);
             border:5px solid #bababa;
             -webkit-border-radius: 14px; -moz-border-radius: 14px;
             border-radius:14px;
             background-position: 10px 10px;
             background-repeat: no-repeat;
             background-size: 38px;
             padding-left: 30px; /* 55px; if icon */
     }
     .reveal .alert-block {padding-top:14px; padding-bottom:14px}
     .reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
     /*.reveal .alert li {margin-top: 1em}*/
     .reveal .alert-block p+p {margin-top:5px}
     /*.reveal .alert-notice { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
     .reveal .alert-summary  { background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
     .reveal .alert-warning { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
     .reveal .alert-question {background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */

</style>



<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>

<body>
<div class="reveal">

<!-- Any section element inside the <div class="slides"> container
     is displayed as a slide -->

<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    



<section>
<!-- ------------------- main content ---------------------- -->



<center><h1 style="text-align: center;">Data Analysis and Machine Learning  Lectures: Cubic Splines and Gradient Methods</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>&nbsp;<br>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>&nbsp;<br>
<center><h4>May 22, 2018</h4></center> <!-- date -->
<br>
<p>

<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2018, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>


<section>
<h2 id="___sec0">Cubic Splines  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Cubic spline interpolation is among one of the most used 
methods for interpolating between data points where the arguments
are organized as ascending series. In the library program we supply
such a function, based on the so-called cubic spline method to be 
described below.

<p>
A spline function consists of polynomial pieces defined on
subintervals. The different subintervals are connected via
various continuity relations.

<p>
Assume we have at our disposal \( n+1 \) points \( x_0, x_1, \dots x_n \) 
arranged so that \( x_0 < x_1 < x_2 < \dots x_{n-1} < x_n \) (such points are called
knots). A spline function \( s \) of degree \( k \) with \( n+1 \) knots is defined
as follows

<ul>
 <p><li> On every subinterval \( [x_{i-1},x_i) \) <em>s</em> is a polynomial of degree \( \le k \).</li>
 <p><li> \( s \) has \( k-1 \) continuous derivatives in the whole interval \( [x_0,x_n] \).</li>
</ul>
</div>
</section>


<section>
<h2 id="___sec1">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
As an example, consider a spline function of degree \( k=1 \) defined as follows
<p>&nbsp;<br>
$$
    s(x)=\begin{bmatrix} s_0(x)=a_0x+b_0 & x\in [x_0, x_1) \\   
                             s_1(x)=a_1x+b_1 & x\in [x_1, x_2) \\   
                             \dots & \dots \\
                             s_{n-1}(x)=a_{n-1}x+b_{n-1} & x\in 
                             [x_{n-1}, x_n] \end{bmatrix}.
$$
<p>&nbsp;<br>

In this case the polynomial consists of series of straight lines 
connected to each other at every endpoint. The number of continuous
derivatives is then \( k-1=0 \), as expected when we deal with straight lines.
Such a polynomial is quite easy to construct given
\( n+1 \) points \( x_0, x_1, \dots x_n \) and their corresponding 
function values.
</div>
</section>


<section>
<h2 id="___sec2">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The most commonly used spline function is the one with \( k=3 \), the so-called
cubic spline function. 
Assume that we have in adddition to the \( n+1 \) knots a series of
functions values \( y_0=f(x_0), y_1=f(x_1), \dots y_n=f(x_n) \).
By definition, the polynomials \( s_{i-1} \) and \( s_i \) 
are thence supposed to interpolate the same point \( i \), that is
<p>&nbsp;<br>
$$
    s_{i-1}(x_i)= y_i = s_i(x_i),
$$
<p>&nbsp;<br>

with \( 1 \le i \le n-1 \). In total we have \( n \) polynomials of the 
type
<p>&nbsp;<br>
$$
    s_i(x)=a_{i0}+a_{i1}x+a_{i2}x^2+a_{i2}x^3,
$$
<p>&nbsp;<br>

yielding \( 4n \) coefficients to determine.
</div>
</section>


<section>
<h2 id="___sec3">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Every subinterval provides in addition the \( 2n \) conditions 
<p>&nbsp;<br>
$$
    y_i = s(x_i),
$$
<p>&nbsp;<br>

and 
<p>&nbsp;<br>
$$
    s(x_{i+1})= y_{i+1},
$$
<p>&nbsp;<br>

to be fulfilled. If we also assume that \( s' \) and \( s'' \) are continuous,
then
<p>&nbsp;<br>
$$
       s'_{i-1}(x_i)= s'_i(x_i),
$$
<p>&nbsp;<br>

yields \( n-1 \) conditions. Similarly,
<p>&nbsp;<br>
$$
       s''_{i-1}(x_i)= s''_i(x_i),
$$
<p>&nbsp;<br>

results in additional \( n-1 \) conditions. In total we have \( 4n \) coefficients
and \( 4n-2 \) equations to determine them, leaving us with \( 2 \) degrees of 
freedom to be determined.
</div>
</section>


<section>
<h2 id="___sec4">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Using the last equation we define two values for the second derivative, namely
<p>&nbsp;<br>
$$
       s''_{i}(x_i)= f_i,
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
       s''_{i}(x_{i+1})= f_{i+1},
$$
<p>&nbsp;<br>

and setting up a straight line between \( f_i \) and \( f_{i+1} \) we have
<p>&nbsp;<br>
$$
   s_i''(x) = \frac{f_i}{x_{i+1}-x_i}(x_{i+1}-x)+
               \frac{f_{i+1}}{x_{i+1}-x_i}(x-x_i),
$$
<p>&nbsp;<br>

and integrating twice one obtains
<p>&nbsp;<br>
$$
   s_i(x) = \frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+
               \frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3
             +c(x-x_i)+d(x_{i+1}-x).
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec5">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Using the conditions \( s_i(x_i)=y_i \) and \( s_i(x_{i+1})=y_{i+1} \) 
we can in turn determine the constants \( c \) and \( d \) resulting in
<p>&nbsp;<br>
$$
\begin{align}
   s_i(x) =&\frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+
               \frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3 \nonumber  \\ 
            +&(\frac{y_{i+1}}{x_{i+1}-x_i}-\frac{f_{i+1}(x_{i+1}-x_i)}{6})
              (x-x_i)+
             (\frac{y_{i}}{x_{i+1}-x_i}-\frac{f_{i}(x_{i+1}-x_i)}{6})
             (x_{i+1}-x).
\tag{1}
\end{align}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec6">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
How to determine the values of the second
derivatives \( f_{i} \) and \( f_{i+1} \)? We use the continuity assumption 
of the first derivatives 
<p>&nbsp;<br>
$$
    s'_{i-1}(x_i)= s'_i(x_i),
$$
<p>&nbsp;<br>

and set \( x=x_i \). Defining \( h_i=x_{i+1}-x_i \) we obtain finally
the following expression
<p>&nbsp;<br>
$$
   h_{i-1}f_{i-1}+2(h_{i}+h_{i-1})f_i+h_if_{i+1}=
   \frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_{i-1}}(y_{i}-y_{i-1}),
$$
<p>&nbsp;<br>

and introducing the shorthands \( u_i=2(h_{i}+h_{i-1}) \), 
\( v_i=\frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_{i-1}}(y_{i}-y_{i-1}) \),
we can reformulate the problem as a set of linear equations to be 
solved  through e.g., Gaussian elemination
</div>
</section>


<section>
<h2 id="___sec7">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Gaussian elimination
<p>&nbsp;<br>
$$
   \begin{bmatrix} u_1 & h_1 &0 &\dots & & & & \\
                                 h_1 & u_2 & h_2 &0 &\dots & & & \\
                                  0   & h_2 & u_3 & h_3 &0 &\dots & & \\
                               \dots& & \dots &\dots &\dots &\dots &\dots & \\
                                 &\dots & & &0 &h_{n-3} &u_{n-2} &h_{n-2} \\
                                 & && & &0 &h_{n-2} &u_{n-1} \end{bmatrix}
   \begin{bmatrix} f_1 \\ 
                          f_2 \\
                          f_3\\
                          \dots \\
                          f_{n-2} \\ 
                          f_{n-1} \end{bmatrix} =
   \begin{bmatrix} v_1 \\ 
                          v_2 \\
                          v_3\\
                          \dots \\
                          v_{n-2}\\
                          v_{n-1} \end{bmatrix}.
$$
<p>&nbsp;<br>

Note that this is a set of tridiagonal equations and can be solved 
through only \( O(n) \) operations.
</div>
</section>


<section>
<h2 id="___sec8">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The functions supplied in the program library are <em>spline</em> and <em>splint</em>.
In order to use cubic spline interpolation you need first to call

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>spline(<span style="color: #00688B; font-weight: bold">double</span> x[], <span style="color: #00688B; font-weight: bold">double</span> y[], <span style="color: #00688B; font-weight: bold">int</span> n, <span style="color: #00688B; font-weight: bold">double</span> yp1,  <span style="color: #00688B; font-weight: bold">double</span> yp2, <span style="color: #00688B; font-weight: bold">double</span> y2[])
</pre></div>
<p>
This function takes as
input \( x[0,..,n - 1] \) and \( y[0,..,n - 1] \) containing a tabulation
\( y_i = f(x_i) \) with \( x_0 < x_1 < .. < x_{n - 1} \) 
together with the 
first derivatives  of \( f(x) \) at \( x_0 \) and \( x_{n-1} \), respectively. Then the
function returns \( y2[0,..,n-1] \) which contains the second derivatives of
\( f(x_i) \) at each point \( x_i \). \( n \) is the number of points.
This function provides the cubic spline interpolation for all subintervals
and is called only once.
</div>
</section>


<section>
<h2 id="___sec9">Splines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Thereafter, if you wish to make  various interpolations, you need to call the function 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>splint(<span style="color: #00688B; font-weight: bold">double</span> x[], <span style="color: #00688B; font-weight: bold">double</span> y[], <span style="color: #00688B; font-weight: bold">double</span> y2a[], <span style="color: #00688B; font-weight: bold">int</span> n, <span style="color: #00688B; font-weight: bold">double</span> x, <span style="color: #00688B; font-weight: bold">double</span> *y)
</pre></div>
<p>
which takes as input
the tabulated values \( x[0,..,n - 1] \) and \( y[0,..,n - 1] \) and the output 
y2a[0,..,n - 1] from <em>spline</em>. It returns the value \( y \) corresponding
to the point \( x \).
</div>
</section>


<section>
<h2 id="___sec10">Conjugate gradient (CG) method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The success of the CG method  for finding solutions of non-linear problems is based
on the theory of conjugate gradients for linear systems of equations. It belongs
to the class of iterative methods for solving problems from linear algebra of the type
<p>&nbsp;<br>
$$
\begin{equation*}
  \hat{A}\hat{x} = \hat{b}.
\end{equation*}
$$
<p>&nbsp;<br>

In the iterative process we end up with a problem like

<p>&nbsp;<br>
$$
\begin{equation*}
  \hat{r}= \hat{b}-\hat{A}\hat{x},
\end{equation*}
$$
<p>&nbsp;<br>

where \( \hat{r} \) is the so-called residual or error in the iterative process.

<p>
When we have found the exact solution, \( \hat{r}=0 \).
</div>
</section>


<section>
<h2 id="___sec11">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The residual is zero when we reach the minimum of the quadratic equation
<p>&nbsp;<br>
$$
\begin{equation*}
  P(\hat{x})=\frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T\hat{b},
\end{equation*}
$$
<p>&nbsp;<br>

with the constraint that the matrix \( \hat{A} \) is positive definite and symmetric.
If we search for a minimum of the quantum mechanical  variance, then the matrix 
\( \hat{A} \), which is called the Hessian, is given by the second-derivative of the function we want to minimize.  This quantity is always positive definite.  In our case this corresponds normally to the second derivative of the energy.
</div>
</section>


<section>
<h2 id="___sec12">Conjugate gradient method, Newton's method first </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We seek the minimum of the energy or the variance as function of various variational parameters. 
In our case we have thus a function \( f \) whose minimum we are seeking.
In Newton's method we set \( \nabla f = 0 \) and we can thus compute the next iteration point
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{x}-\hat{x}_i=\hat{A}^{-1}\nabla f(\hat{x}_i).
\end{equation*}
$$
<p>&nbsp;<br>

Subtracting this equation from that of \( \hat{x}_{i+1} \) we have
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{x}_{i+1}-\hat{x}_i=\hat{A}^{-1}(\nabla f(\hat{x}_{i+1})-\nabla f(\hat{x}_i)).
\end{equation*}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec13">Simple example and demonstration </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The function \( f \) can be either the energy or the variance.  If we choose the energy then we have
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{\alpha}_{i+1}-\hat{\alpha}_i=\hat{A}^{-1}(\nabla E(\hat{\alpha}_{i+1})-\nabla E(\hat{\alpha}_i)).
\end{equation*}
$$
<p>&nbsp;<br>

In the simple harmonic oscillator model, the gradient and the Hessian \( \hat{A} \) are
<p>&nbsp;<br>
$$
\begin{equation*}
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\end{equation*}
$$
<p>&nbsp;<br>

and a second derivative which is always positive (meaning that we find a minimum)
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{A}= \frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\end{equation*}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec14">Simple example and demonstration </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We get then
<p>&nbsp;<br>
$$
\begin{equation*}
\alpha_{i+1}=\frac{4}{3}\alpha_i-\frac{\alpha_i^4}{3\alpha_{i+1}^3},
\end{equation*}
$$
<p>&nbsp;<br>

which can be rewritten as
<p>&nbsp;<br>
$$
\begin{equation*}
\alpha_{i+1}^4-\frac{4}{3}\alpha_i\alpha_{i+1}^4+\frac{1}{3}\alpha_i^4.
\end{equation*}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec15">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In the CG method we define so-called conjugate directions and two vectors 
\( \hat{s} \) and \( \hat{t} \)
are said to be
conjugate if
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{s}^T\hat{A}\hat{t}= 0.
\end{equation*}
$$
<p>&nbsp;<br>

The philosophy of the CG method is to perform searches in various conjugate directions
of our vectors \( \hat{x}_i \) obeying the above criterion, namely
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{x}_i^T\hat{A}\hat{x}_j= 0.
\end{equation*}
$$
<p>&nbsp;<br>

Two vectors are conjugate if they are orthogonal with respect to 
this inner product. Being conjugate is a symmetric relation: if \( \hat{s} \) is conjugate to \( \hat{t} \), then \( \hat{t} \) is conjugate to \( \hat{s} \).
</div>
</section>


<section>
<h2 id="___sec16">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
An example is given by the eigenvectors of the matrix
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{v}_i^T\hat{A}\hat{v}_j= \lambda\hat{v}_i^T\hat{v}_j,
\end{equation*}
$$
<p>&nbsp;<br>

which is zero unless \( i=j \).
</div>
</section>


<section>
<h2 id="___sec17">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Assume now that we have a symmetric positive-definite matrix \( \hat{A} \) of size
\( n\times n \). At each iteration \( i+1 \) we obtain the conjugate direction of a vector
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{x}_{i+1}=\hat{x}_{i}+\alpha_i\hat{p}_{i}. 
\end{equation*}
$$
<p>&nbsp;<br>

We assume that \( \hat{p}_{i} \) is a sequence of \( n \) mutually conjugate directions. 
Then the \( \hat{p}_{i} \)  form a basis of \( R^n \) and we can expand the solution 
$  \hat{A}\hat{x} = \hat{b}$ in this basis, namely

<p>&nbsp;<br>
$$
\begin{equation*}
  \hat{x}  = \sum^{n}_{i=1} \alpha_i \hat{p}_i.
\end{equation*}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec18">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The coefficients are given by
<p>&nbsp;<br>
$$
\begin{equation*}
    \mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\end{equation*}
$$
<p>&nbsp;<br>

Multiplying with \( \hat{p}_k^T \)  from the left gives

<p>&nbsp;<br>
$$
\begin{equation*}
  \hat{p}_k^T \hat{A}\hat{x} = \sum^{n}_{i=1} \alpha_i\hat{p}_k^T \hat{A}\hat{p}_i= \hat{p}_k^T \hat{b},
\end{equation*}
$$
<p>&nbsp;<br>

and we can define the coefficients \( \alpha_k \) as

<p>&nbsp;<br>
$$
\begin{equation*}
    \alpha_k = \frac{\hat{p}_k^T \hat{b}}{\hat{p}_k^T \hat{A} \hat{p}_k}
\end{equation*}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec19">Conjugate gradient method and iterations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If we choose the conjugate vectors \( \hat{p}_k \) carefully, 
then we may not need all of them to obtain a good approximation to the solution 
\( \hat{x} \). 
We want to regard the conjugate gradient method as an iterative method. 
This will us to solve systems where \( n \) is so large that the direct 
method would take too much time.

<p>
We denote the initial guess for \( \hat{x} \) as \( \hat{x}_0 \). 
We can assume without loss of generality that
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{x}_0=0,
\end{equation*}
$$
<p>&nbsp;<br>

or consider the system
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{A}\hat{z} = \hat{b}-\hat{A}\hat{x}_0,
\end{equation*}
$$
<p>&nbsp;<br>

instead.
</div>
</section>


<section>
<h2 id="___sec20">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
One can show that the solution \( \hat{x} \) is also the unique minimizer of the quadratic form
<p>&nbsp;<br>
$$
\begin{equation*}
  f(\hat{x}) = \frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T \hat{x} , \quad \hat{x}\in\mathbf{R}^n. 
\end{equation*}
$$
<p>&nbsp;<br>

This suggests taking the first basis vector \( \hat{p}_1 \) 
to be the gradient of \( f \) at \( \hat{x}=\hat{x}_0 \), 
which equals
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{A}\hat{x}_0-\hat{b},
\end{equation*}
$$
<p>&nbsp;<br>

and 
\( \hat{x}_0=0 \) it is equal \( -\hat{b} \).
The other vectors in the basis will be conjugate to the gradient, 
hence the name conjugate gradient method.
</div>
</section>


<section>
<h2 id="___sec21">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let  \( \hat{r}_k \) be the residual at the \( k \)-th step:
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{r}_k=\hat{b}-\hat{A}\hat{x}_k.
\end{equation*}
$$
<p>&nbsp;<br>

Note that \( \hat{r}_k \) is the negative gradient of \( f \) at 
\( \hat{x}=\hat{x}_k \), 
so the gradient descent method would be to move in the direction \( \hat{r}_k \). 
Here, we insist that the directions \( \hat{p}_k \) are conjugate to each other, 
so we take the direction closest to the gradient \( \hat{r}_k \)  
under the conjugacy constraint. 
This gives the following expression
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{p}_{k+1}=\hat{r}_k-\frac{\hat{p}_k^T \hat{A}\hat{r}_k}{\hat{p}_k^T\hat{A}\hat{p}_k} \hat{p}_k.
\end{equation*}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec22">Conjugate gradient method </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can also  compute the residual iteratively as
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{r}_{k+1}=\hat{b}-\hat{A}\hat{x}_{k+1},
 \end{equation*}
$$
<p>&nbsp;<br>

which equals
<p>&nbsp;<br>
$$
\begin{equation*}
\hat{b}-\hat{A}(\hat{x}_k+\alpha_k\hat{p}_k),
 \end{equation*}
$$
<p>&nbsp;<br>

or
<p>&nbsp;<br>
$$
\begin{equation*}
(\hat{b}-\hat{A}\hat{x}_k)-\alpha_k\hat{A}\hat{p}_k,
 \end{equation*}
$$
<p>&nbsp;<br>

which gives

<p>&nbsp;<br>
$$
\begin{equation*}
\hat{r}_{k+1}=\hat{r}_k-\hat{A}\hat{p}_{k},
 \end{equation*}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec23">Gradient Descent codes </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22"># Importing various packages</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.mplot3d</span> <span style="color: #8B008B; font-weight: bold">import</span> Axes3D
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> cm
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>

x = <span style="color: #B452CD">2</span>*np.random.rand(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
y = <span style="color: #B452CD">4</span>+<span style="color: #B452CD">3</span>*x+np.random.randn(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)

xb = np.c_[np.ones((<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)), x]
theta_linreg = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)
<span style="color: #8B008B; font-weight: bold">print</span>(theta_linreg)
theta = np.random.randn(<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>)

eta = <span style="color: #B452CD">0.1</span>
Niterations = <span style="color: #B452CD">1000</span>
m = <span style="color: #B452CD">100</span>

<span style="color: #8B008B; font-weight: bold">for</span> <span style="color: #658b00">iter</span> <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Niterations):
    gradients = <span style="color: #B452CD">2.0</span>/m*xb.T.dot(xb.dot(theta)-y)
    theta -= eta*gradients

<span style="color: #8B008B; font-weight: bold">print</span>(theta)
xnew = np.array([[<span style="color: #B452CD">0</span>],[<span style="color: #B452CD">2</span>]])
xbnew = np.c_[np.ones((<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>)), xnew]
ypredict = xbnew.dot(theta)
ypredict2 = xbnew.dot(theta_linreg)
plt.plot(xnew, ypredict, <span style="color: #CD5555">&quot;r-&quot;</span>)
plt.plot(xnew, ypredict2, <span style="color: #CD5555">&quot;b-&quot;</span>)
plt.plot(x, y ,<span style="color: #CD5555">&#39;ro&#39;</span>)
plt.axis([<span style="color: #B452CD">0</span>,<span style="color: #B452CD">2.0</span>,<span style="color: #B452CD">0</span>, <span style="color: #B452CD">15.0</span>])
plt.xlabel(<span style="color: #CD5555">r&#39;$x$&#39;</span>)
plt.ylabel(<span style="color: #CD5555">r&#39;$y$&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Random numbers &#39;</span>)
plt.show()
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22"># Importing various packages</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> SGDRegressor

x = <span style="color: #B452CD">2</span>*np.random.rand(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
y = <span style="color: #B452CD">4</span>+<span style="color: #B452CD">3</span>*x+np.random.randn(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)

xb = np.c_[np.ones((<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)), x]
theta_linreg = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)
<span style="color: #8B008B; font-weight: bold">print</span>(theta_linreg)
sgdreg = SGDRegressor(n_iter = <span style="color: #B452CD">50</span>, penalty=<span style="color: #658b00">None</span>, eta0=<span style="color: #B452CD">0.1</span>)
sgdreg.fit(x,y.ravel())
<span style="color: #8B008B; font-weight: bold">print</span>(sgdreg.intercept_, sgdreg.coef_)
</pre></div>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

    // Display navigation controls in the bottom right corner
    controls: true,

    // Display progress bar (below the horiz. slider)
    progress: true,

    // Display the page number of the current slide
    slideNumber: true,

    // Push each slide change to the browser history
    history: false,

    // Enable keyboard shortcuts for navigation
    keyboard: true,

    // Enable the slide overview mode
    overview: true,

    // Vertical centering of slides
    //center: true,
    center: false,

    // Enables touch navigation on devices with touch input
    touch: true,

    // Loop the presentation
    loop: false,

    // Change the presentation direction to be RTL
    rtl: false,

    // Turns fragments on and off globally
    fragments: true,

    // Flags if the presentation is running in an embedded mode,
    // i.e. contained within a limited portion of the screen
    embedded: false,

    // Number of milliseconds between automatically proceeding to the
    // next slide, disabled when set to 0, this value can be overwritten
    // by using a data-autoslide attribute on your slides
    autoSlide: 0,

    // Stop auto-sliding after user input
    autoSlideStoppable: true,

    // Enable slide navigation via mouse wheel
    mouseWheel: false,

    // Hides the address bar on mobile devices
    hideAddressBar: true,

    // Opens links in an iframe preview overlay
    previewLinks: false,

    // Transition style
    transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Transition speed
    transitionSpeed: 'default', // default/fast/slow

    // Transition style for full page slide backgrounds
    backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

    // Number of slides away from the current that are visible
    viewDistance: 3,

    // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

    // Parallax background size
    //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

    theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        //{ src: 'reveal.js/plugin/math/math.js', async: true }
    ]
});

Reveal.initialize({

    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1170,  // original: 960,
    height: 700,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
     end footer logo -->



</body>
</html>
