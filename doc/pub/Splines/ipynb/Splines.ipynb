{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods -->\n",
    "# Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Sep 21, 2018**\n",
    "\n",
    "Copyright 1999-2018, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Optimization, the central part of any Machine Learning algortithm\n",
    "\n",
    "Almost every problem in machine learning and data science starts with\n",
    "a dataset $X$, a model $g(\\beta)$, which is a function of the\n",
    "parameters $\\beta$ and a cost function $C(X, g(\\beta))$ that allows\n",
    "us to judge how well the model $g(\\beta)$ explains the observations\n",
    "$X$. The model is fit by finding the values of $\\beta$ that minimize\n",
    "the cost function. Ideally we would be able to solve for $\\beta$\n",
    "analytically, however this is not possible in general and we must use\n",
    "some approximative/numerical method to compute the minimum.\n",
    "\n",
    "## Steepest descent\n",
    "\n",
    "The method of steepest descent The basic idea of gradient descent is\n",
    "that a function $F(\\mathbf{x})$, \n",
    "$\\mathbf{x} \\equiv (x_1,\\cdots,x_n)$, decreases fastest if one goes from $\\bf {x}$ in the\n",
    "direction of the negative gradient $-\\nabla F(\\mathbf{x})$.\n",
    "\n",
    "It can be shown that if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma_k \\nabla F(\\mathbf{x}_k),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\gamma_k > 0$.\n",
    "\n",
    "For $\\gamma_k$ small enough, then $F(\\mathbf{x}_{k+1}) \\leq\n",
    "F(\\mathbf{x}_k)$. This means that for a sufficiently small $\\gamma_k$\n",
    "we are always moving towards smaller function values, i.e a minimum.\n",
    "\n",
    "<!-- !split  -->\n",
    "## More on Steepest descent\n",
    "\n",
    "The previous observation is the basis of the method of steepest\n",
    "descent, which is also referred to as just gradient descent (GD). One\n",
    "starts with an initial guess $\\mathbf{x}_0$ for a minimum of $F$ and\n",
    "computes new approximations according to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma_k \\nabla F(\\mathbf{x}_k), \\ \\ k \\geq 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\gamma_k$ is often referred to as the step length or\n",
    "the learning rate within the context of Machine Learning.\n",
    "\n",
    "<!-- !split  -->\n",
    "## The ideal\n",
    "\n",
    "Ideally the sequence $\\{\\mathbf{x}_k \\}_{k=0}$ converges to a global\n",
    "minimum of the function $F$. In general we do not know if we are in a\n",
    "global or local minimum. In the special case when $F$ is a convex\n",
    "function, all local minima are also global minima, so in this case\n",
    "gradient descent can converge to the global solution. The advantage of\n",
    "this scheme is that it is conceptually simple and straightforward to\n",
    "implement. However the method in this form has some severe\n",
    "limitations:\n",
    "\n",
    "In machine learing we are often faced with non-convex high dimensional\n",
    "cost functions with many local minima. Since GD is deterministic we\n",
    "will get stuck in a local minimum, if the method converges, unless we\n",
    "have a very good intial guess. This also implies that the scheme is\n",
    "sensitive to the chosen initial condition.\n",
    "\n",
    "Note that the gradient is a function of $\\mathbf{x} =\n",
    "(x_1,\\cdots,x_n)$ which makes it expensive to compute numerically.\n",
    "\n",
    "\n",
    "<!-- !split  -->\n",
    "## The sensitiveness of the gradient descent\n",
    "\n",
    "The gradient descent method \n",
    "is sensitive to the choice of learning rate $\\gamma_k$. This is due\n",
    "to the fact that we are only guaranteed that $F(\\mathbf{x}_{k+1}) \\leq\n",
    "F(\\mathbf{x}_k)$ for sufficiently small $\\gamma_k$. The problem is to\n",
    "determine an optimal learning rate. If the learning rate is chosen too\n",
    "small the method will take a long time to converge and if it is too\n",
    "large we can experience erratic behavior.\n",
    "\n",
    "Many of these shortcomings can be alleviated by introducing\n",
    "randomness. One such method is that of Stochastic Gradient Descent\n",
    "(SGD), see below.\n",
    "\n",
    "\n",
    "<!-- !split  -->\n",
    "## Convex functions\n",
    "\n",
    "Ideally we want our cost/loss function to be convex(concave).\n",
    "\n",
    "First we give the definition of a convex set: A set $C$ in\n",
    "$\\mathbb{R}^n$ is said to be convex if, for all $x$ and $y$ in $C$ and\n",
    "all $t \\in (0,1)$ , the point $(1 âˆ’ t)x + ty$ also belongs to\n",
    "C. Geometrically this means that every point on the line segment\n",
    "connecting $x$ and $y$ is in $C$ as discussed below.\n",
    "\n",
    "The convex subsets of $\\mathbb{R}$ are the intervals of\n",
    "$\\mathbb{R}$. Examples of convex sets of $\\mathbb{R}^2$ are the\n",
    "regular polygons (triangles, rectangles, pentagons, etc...).\n",
    "\n",
    "## Convex function\n",
    "\n",
    "**Convex function**: Let $X \\subset \\mathbb{R}^n$ be a convex set. Assume that the function $f: X \\rightarrow \\mathbb{R}$ is continuous, then $f$ is said to be convex if $$f(tx_1 + (1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2) $$ for all $x_1, x_2 \\in X$ and for all $t \\in [0,1]$. If $\\leq$ is replaced with a strict inequaltiy in the definition, we demand $x_1 \\neq x_2$ and $t\\in(0,1)$ then $f$ is said to be strictly convex. For a single variable function, convexity means that if you draw a straight line connecting $f(x_1)$ and $f(x_2)$, the value of the function on the interval $[x_1,x_2]$ is always below the line as illustrated below.\n",
    "\n",
    "## Conditions on convex functions\n",
    "\n",
    "In the following we state first and second-order conditions which\n",
    "ensures convexity of a function $f$. We write $D_f$ to denote the\n",
    "domain of $f$, i.e the subset of $R^n$ where $f$ is defined. For more\n",
    "details and proofs we refer to: [S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press](http://stanford.edu/boyd/cvxbook/, 2004).\n",
    "\n",
    "**First order condition.**\n",
    "\n",
    "Suppose $f$ is differentiable (i.e $\\nabla f(x)$ is well defined for\n",
    "all $x$ in the domain of $f$). Then $f$ is convex if and only if $D_f$\n",
    "is a convex set and $$f(y) \\geq f(x) + \\nabla f(x)^T (y-x) $$ holds\n",
    "for all $x,y \\in D_f$. This condition means that for a convex function\n",
    "the first order Taylor expansion (right hand side above) at any point\n",
    "a global under estimator of the function. To convince yourself you can\n",
    "make a drawing of $f(x) = x^2+1$ and draw the tangent line to $f(x)$ and\n",
    "note that it is always below the graph.\n",
    "\n",
    "\n",
    "\n",
    "**Second order condition.**\n",
    "\n",
    "Assume that $f$ is twice\n",
    "differentiable, i.e the Hessian matrix exists at each point in\n",
    "$D_f$. Then $f$ is convex if and only if $D_f$ is a convex set and its\n",
    "Hessian is positive semi-definite for all $x\\in D_f$. For a\n",
    "single-variable function this reduces to $f''(x) \\geq 0$. Geometrically this means that $f$ has nonnegative curvature\n",
    "everywhere.\n",
    "\n",
    "\n",
    "\n",
    "This condition is particularly useful since it gives us an procedure for determining if the function under consideration is convex, apart from using the definition.\n",
    "\n",
    "## More on convex functions\n",
    "\n",
    "The next result is of great importance to us and the reason why we are\n",
    "going on about convex functions. In machine learning we frequently\n",
    "have to minimize a loss/cost function in order to find the best\n",
    "parameters for the model we are considering. \n",
    "\n",
    "Ideally we want the\n",
    "global minimum (for high-dimensional models it is hard to know\n",
    "if we have local or global minimum). However, if the cost/loss function\n",
    "is convex the following result provides invaluable information:\n",
    "\n",
    "**Any minimum is global for convex functions.**\n",
    "\n",
    "Consider the problem of finding $x \\in \\mathbb{R}^n$ such that $f(x)$\n",
    "is minimal, where $f$ is convex and differentiable. Then, any point\n",
    "$x^*$ that satisfies $\\nabla f(x^*) = 0$ is a global minimum.\n",
    "\n",
    "\n",
    "\n",
    "This result means that if we know that the cost/loss function is convex and we are able to find a minimum, we are guaranteed that it is a global minimum.\n",
    "\n",
    "## Some simple problems\n",
    "\n",
    "1. Show that $f(x)=x^2$ is convex for $x \\in \\mathbb{R}$ using the definition of convexity. Hint: If you re-write the definition, $f$ is convex if the following holds for all $x,y \\in D_f$ and any $\\lambda \\in [0,1]$ $\\lambda f(x)+(1-\\lambda)f(y)-f(\\lambda x + (1-\\lambda) y ) \\geq 0$.\n",
    "\n",
    "2. Using the second order condition show that the following functions are convex on the specified domain.\n",
    "\n",
    " * $f(x) = e^x$ is convex for $x \\in \\mathbb{R}$.\n",
    "\n",
    " * $g(x) = -\\ln(x)$ is convex for $x \\in (0,\\infty)$.\n",
    "\n",
    "\n",
    "3. Let $f(x) = x^2$ and $g(x) = e^x$. Show that $f(g(x))$ and $g(f(x))$ is convex for $x \\in \\mathbb{R}$. Also show that if $f(x)$ is any convex function than $h(x) = e^{f(x)}$ is convex.\n",
    "\n",
    "4. A norm is any function that satisfy the following properties\n",
    "\n",
    " * $f(\\alpha x) = |\\alpha| f(x)$ for all $\\alpha \\in \\mathbb{R}$.\n",
    "\n",
    " * $f(x+y) \\leq f(x) + f(y)$\n",
    "\n",
    " * $f(x) \\leq 0$ for all $x \\in \\mathbb{R}^n$ with equality if and only if $x = 0$\n",
    "\n",
    "\n",
    "Using the definition of convexity, try to show that a function satisfying the properties above is convex (the third condition is not needed to show this).\n",
    "\n",
    "<!-- !split  -->\n",
    "## Revisiting our first homework\n",
    "\n",
    "We will use linear regression as a case study for the gradient descent\n",
    "methods. Linear regression is a great test case for the gradient\n",
    "descent methods discussed in the lectures since it has several\n",
    "desirable properties such as:\n",
    "\n",
    "1. An analytical solution (recall homework set 1).\n",
    "\n",
    "2. The gradient can be computed analytically.\n",
    "\n",
    "3. The cost function is convex which guarantees that gradient descent converges for small enough learning rates\n",
    "\n",
    "We revisit the example from homework set 1 where we had"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_i = 5x_i^2 + 0.1\\xi_i, \\ i=1,\\cdots,100\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $x_i \\in [0,1] $ chosen randomly with a uniform distribution. Additionally $\\xi_i$ represents stochastic noise chosen according to a normal distribution $\\cal {N}(0,1)$. \n",
    "The linear regression model is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_\\beta(x) = \\hat{y} = \\beta_0 + \\beta_1 x,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}_i = \\beta_0 + \\beta_1 x_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- !split  -->\n",
    "## Gradient descent example\n",
    "\n",
    "Let $\\mathbf{y} = (y_1,\\cdots,y_n)^T$, $\\mathbf{\\hat{y}} = (\\hat{y}_1,\\cdots,\\hat{y}_n)^T$ and $\\beta = (\\beta_0, \\beta_1)^T$\n",
    "\n",
    "It is convenient to write $\\mathbf{\\hat{y}} = X\\beta$ where $X \\in \\mathbb{R}^{100 \\times 2} $ is the design matrix given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X \\equiv \\begin{bmatrix}\n",
    "1 &amp; x_1  \\\\\n",
    "\\vdots &amp; \\vdots  \\\\\n",
    "1 &amp; x_{100} &amp;  \\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\beta) = ||X\\beta-\\mathbf{y}||^2 = ||X\\beta||^2 - 2 \\mathbf{y}^T X\\beta + ||\\mathbf{y}||^2 = \\sum_{i=1}^{100} (\\beta_0 + \\beta_1 x_i)^2 - 2 y_i (\\beta_0 + \\beta_1 x_i) + y_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we want to find $\\beta$ such that $C(\\beta)$ is minimized.\n",
    "\n",
    "## The derivative of the cost/loss function\n",
    "\n",
    "Computing $\\partial C(\\beta) / \\partial \\beta_0$ and $\\partial C(\\beta) / \\partial \\beta_1$ we can show  that the gradient can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\beta} C(\\beta) = (\\partial C(\\beta) / \\partial \\beta_0, \\partial C(\\beta) / \\partial \\beta_1)^T = 2\\begin{bmatrix} \\sum_{i=1}^{100} \\left(\\beta_0+\\beta_1x_i-y_i\\right) \\\\\n",
    "\\sum_{i=1}^{100}\\left( x_i (\\beta_0+\\beta_1x_i)-y_ix_i\\right) \\\\\n",
    "\\end{bmatrix} = 2X^T(X\\beta - \\mathbf{y}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $X$ is the design matrix defined above.\n",
    "\n",
    "## The Hessian matrix\n",
    "The Hessian matrix of $C(\\beta)$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{H} \\equiv \\begin{bmatrix}\n",
    "\\frac{\\partial^2 C(\\beta)}{\\partial \\beta_0^2} &amp; \\frac{\\partial^2 C(\\beta)}{\\partial \\beta_0 \\partial \\beta_1}  \\\\\n",
    "\\frac{\\partial^2 C(\\beta)}{\\partial \\beta_0 \\partial \\beta_1} &amp; \\frac{\\partial^2 C(\\beta)}{\\partial \\beta_1^2} &amp;  \\\\\n",
    "\\end{bmatrix} = 2X^T X.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result implies that $C(\\beta)$ is a convex function since the matrix $X^T X$ always is positive semi-definite.\n",
    "\n",
    "## Simple program\n",
    "\n",
    "We can now write a program that minimizes $C(\\beta)$ using the gradient descent method with a constant learning rate $\\gamma$ according to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{k+1} = \\beta_k - \\gamma \\nabla_\\beta C(\\beta_k), \\ k=0,1,\\cdots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the expression we computed for the gradient and let use a\n",
    "$\\beta_0$ be chosen randomly and let $\\gamma = 0.001$. Stop iterating\n",
    "when $||\\nabla_\\beta C(\\beta_k) || \\leq \\epsilon = 10^{-8}$. \n",
    "\n",
    "And finally we can compare our solution for $\\beta$ with the analytic result given by \n",
    "$\\beta= (X^TX)^{-1} X^T \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "The following setup is just a suggestion, feel free to write it the way you like.\n",
    "\"\"\"\n",
    "\n",
    "#Setup problem described in the exercise\n",
    "N  = 100 #Nr of datapoints\n",
    "M  = 2 #Nr of features\n",
    "x  = np.random.rand(N) #Uniformly generated x-values in [0,1]\n",
    "y  = 5*x**2 + 0.1*np.random.randn(N)\n",
    "X  = np.c_[np.ones(N),x] #Construct design matrix\n",
    "\n",
    "#Compute beta according to normal equations to compare with GD solution\n",
    "Xt_X_inv = np.linalg.inv(np.dot(X.T,X))\n",
    "Xt_y     = np.dot(X.transpose(),y)\n",
    "beta_NE = np.dot(Xt_X_inv,Xt_y)\n",
    "print(beta_NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Example\n",
    "\n",
    "Another simple example is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.11631855]\n",
      " [2.78555876]]\n",
      "[[4.11631855]\n",
      " [2.78555876]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4+3*x+np.random.randn(100,1)\n",
    "\n",
    "xb = np.c_[np.ones((100,1)), x]\n",
    "beta_linreg = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)\n",
    "print(beta_linreg)\n",
    "beta = np.random.randn(2,1)\n",
    "\n",
    "eta = 0.1\n",
    "Niterations = 1000\n",
    "m = 100\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = 2.0/m*xb.T.dot(xb.dot(beta)-y)\n",
    "    beta -= eta*gradients\n",
    "\n",
    "print(beta)\n",
    "xnew = np.array([[0],[2]])\n",
    "xbnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = xbnew.dot(beta)\n",
    "ypredict2 = xbnew.dot(beta_linreg)\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Gradient descent example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And a corresponding example using **scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4+3*x+np.random.randn(100,1)\n",
    "\n",
    "xb = np.c_[np.ones((100,1)), x]\n",
    "beta_linreg = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)\n",
    "print(beta_linreg)\n",
    "sgdreg = SGDRegressor(n_iter = 50, penalty=None, eta0=0.1)\n",
    "sgdreg.fit(x,y.ravel())\n",
    "print(sgdreg.intercept_, sgdreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- !split  -->\n",
    "## Gradient descent and Ridge\n",
    "\n",
    "We have also discussed Ridge regression where the loss function contains a regularized given by the $L_2$ norm of $\\beta$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C_{\\text{ridge}}(\\beta) = ||X\\beta -\\mathbf{y}||^2 + \\lambda ||\\beta||^2, \\ \\lambda \\geq 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize $C_{\\text{ridge}}(\\beta)$ using GD we only have adjust the gradient as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\beta C_{\\text{ridge}}(\\beta)  = 2\\begin{bmatrix} \\sum_{i=1}^{100} \\left(\\beta_0+\\beta_1x_i-y_i\\right) \\\\\n",
    "\\sum_{i=1}^{100}\\left( x_i (\\beta_0+\\beta_1x_i)-y_ix_i\\right) \\\\\n",
    "\\end{bmatrix} + 2\\lambda\\begin{bmatrix} \\beta_0 \\\\ \\beta_1\\end{bmatrix} = 2 (X^T(X\\beta - \\mathbf{y})+\\lambda \\beta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extend our program to minimize $C_{\\text{ridge}}(\\beta)$ using gradient descent and compare with the analytical solution given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{\\text{ridge}} = \\left(X^T X + \\lambda I_{2 \\times 2} \\right)^{-1} X^T \\mathbf{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $\\lambda = {0,1,10,50,100}$ ($\\lambda = 0$ corresponds to ordinary least squares). \n",
    "We can then compute $||\\beta_{\\text{ridge}}||$ for each $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "The following setup is just a suggestion, feel free to write it the way you like.\n",
    "\"\"\"\n",
    "\n",
    "#Setup problem described in the exercise\n",
    "N  = 100 #Nr of datapoints\n",
    "M  = 2   #Nr of features\n",
    "x  = np.random.rand(N)\n",
    "y  = 5*x**2 + 0.1*np.random.randn(N)\n",
    "\n",
    "\n",
    "#Compute analytic beta for Ridge regression \n",
    "X    = np.c_[np.ones(N),x]\n",
    "XT_X = np.dot(X.T,X)\n",
    "\n",
    "l  = 0.1 #Ridge parameter lambda\n",
    "Id = np.eye(XT_X.shape[0])\n",
    "\n",
    "Z = np.linalg.inv(XT_X+l*Id)\n",
    "beta_ridge = np.dot(Z,np.dot(X.T,y))\n",
    "\n",
    "print(beta_ridge)\n",
    "print(np.linalg.norm(beta_ridge)) #||beta||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent (SGD) and variants thereof address some of\n",
    "the shortcomings of the Gradient descent method discussed above.\n",
    "\n",
    "The underlying idea of SGD comes from the observation that the cost\n",
    "function, which we want to minimize, can almost always be written as a\n",
    "sum over $n$ data points $\\{\\mathbf{x}_i\\}_{i=1}^n$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\mathbf{\\beta}) = \\sum_{i=1}^n c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\beta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of gradients\n",
    "\n",
    "This in turn means that the gradient can be\n",
    "computed as a sum over $i$-gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\beta C(\\mathbf{\\beta}) = \\sum_i^n \\nabla_\\beta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\beta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochasticity/randomness is introduced by only taking the\n",
    "gradient on a subset of the data called minibatches.  If there are $n$\n",
    "data points and the size of each minibatch is $M$, there will be $n/M$\n",
    "minibatches. We denote these minibatches by $B_k$ where\n",
    "$k=1,\\cdots,n/M$.\n",
    "\n",
    "## SGD example\n",
    "As an example, suppose we have $10$ data points $(\\mathbf{x}_1,\\cdots, \\mathbf{x}_{10})$ \n",
    "and we choose to have $M=5$ minibathces,\n",
    "then each minibatch contains two data points. In particular we have\n",
    "$B_1 = (\\mathbf{x}_1,\\mathbf{x}_2), \\cdots, B_5 =\n",
    "(\\mathbf{x}_9,\\mathbf{x}_{10})$. Note that if you choose $M=1$ you\n",
    "have only a single batch with all data points and on the other extreme,\n",
    "you may choose $M=n$ resulting in a minibatch for each datapoint, i.e\n",
    "$B_k = \\mathbf{x}_k$.\n",
    "\n",
    "The idea is now to approximate the gradient by replacing the sum over\n",
    "all data points with a sum over the data points in one the minibatches\n",
    "picked at random in each gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\beta}\n",
    "C(\\mathbf{\\beta}) = \\sum_{i=1}^n \\nabla_\\beta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\beta}) \\rightarrow \\sum_{i \\in B_k}^n \\nabla_\\beta\n",
    "c_i(\\mathbf{x}_i, \\mathbf{\\beta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gradient step\n",
    "\n",
    "Thus a gradient descent step now looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{j+1} = \\beta_j - \\gamma_j \\sum_{i \\in B_k}^n \\nabla_\\beta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\beta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $k$ is picked at random with equal\n",
    "probability from $[1,n/M]$. An iteration over the number of\n",
    "minibathces (n/M) is commonly referred to as an epoch. Thus it is\n",
    "typical to choose a number of epochs and for each epoch iterate over\n",
    "the number of minibatches, as exemplified in the code below.\n",
    "\n",
    "## Simple example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "n = 100 #100 datapoints \n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "n_epochs = 10 #number of epochs\n",
    "\n",
    "j = 0\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    for i in range(m):\n",
    "        k = np.random.randint(m) #Pick the k-th minibatch at random\n",
    "        #Compute the gradient using the data in minibatch Bk\n",
    "        #Compute new suggestion for \n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the gradient only on a subset of the data has two important\n",
    "benefits. First, it introduces randomness which decreases the chance\n",
    "that our opmization scheme gets stuck in a local minima. Second, if\n",
    "the size of the minibatches are small relative to the number of\n",
    "datapoints ($M <  n$), the computation of the gradient is much\n",
    "cheaper since we sum over the datapoints in the $k-th$ minibatch and not\n",
    "all $n$ datapoints.\n",
    "\n",
    "## When do we stop?\n",
    "\n",
    "A natural question is when do we stop the search for a new minimum?\n",
    "One possibility is to compute the full gradient after a given number\n",
    "of epochs and check if the norm of the gradient is smaller than some\n",
    "threshold and stop if true. However, the condition that the gradient\n",
    "is zero is valid also for local minima, so this would only tell us\n",
    "that we are close to a local/global minimum. However, we could also\n",
    "evaluate the cost function at this point, store the result and\n",
    "continue the search. If the test kicks in at a later stage we can\n",
    "compare the values of the cost function and keep the $\\beta$ that\n",
    "gave the lowest value.\n",
    "\n",
    "## Slightly different approach\n",
    "\n",
    "Another approach is to let the step length $\\gamma_j$ depend on the\n",
    "number of epochs in such a way that it becomes very small after a\n",
    "reasonable time such that we do not move at all.\n",
    "\n",
    "As an example, let $e = 0,1,2,3,\\cdots$ denote the current epoch and let $t_0, t_1 > 0$ be two fixed numbers. Furthermore, let $t = e \\cdot m + i$ where $m$ is the number of minibatches and $i=0,\\cdots,m-1$. Then the function $$\\gamma_j(t; t_0, t_1) = \\frac{t_0}{t+t_1} $$ goes to zero as the number of epochs gets large. I.e. we start with a step length $\\gamma_j (0; t_0, t_1) = t_0/t_1$ which decays in *time* $t$.\n",
    "\n",
    "In this way we can fix the number of epochs, compute $\\beta$ and\n",
    "evaluate the cost function at the end. Repeating the computation will\n",
    "give a different result since the scheme is random by design. Then we\n",
    "pick the final $\\beta$ that gives the lowest value of the cost\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def step_length(t,t0,t1):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "n = 100 #100 datapoints \n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "n_epochs = 500 #number of epochs\n",
    "t0 = 1.0\n",
    "t1 = 10\n",
    "\n",
    "gamma_j = t0/t1\n",
    "j = 0\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    for i in range(m):\n",
    "        k = np.random.randint(m) #Pick the k-th minibatch at random\n",
    "        #Compute the gradient using the data in minibatch Bk\n",
    "        #Compute new suggestion for beta\n",
    "        t = epoch*m+i\n",
    "        gamma_j = step_length(t,t0,t1)\n",
    "        j += 1\n",
    "\n",
    "print(\"gamma_j after %d epochs: %g\" % (n_epochs,gamma_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient (CG) method\n",
    "The success of the CG method  for finding solutions of non-linear problems is based\n",
    "on the theory of conjugate gradients for linear systems of equations. It belongs\n",
    "to the class of iterative methods for solving problems from linear algebra of the type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A}\\hat{x} = \\hat{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the iterative process we end up with a problem like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}= \\hat{b}-\\hat{A}\\hat{x},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{r}$ is the so-called residual or error in the iterative process.\n",
    "\n",
    "When we have found the exact solution, $\\hat{r}=0$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "\n",
    "The residual is zero when we reach the minimum of the quadratic equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\hat{x})=\\frac{1}{2}\\hat{x}^T\\hat{A}\\hat{x} - \\hat{x}^T\\hat{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the constraint that the matrix $\\hat{A}$ is positive definite and symmetric.\n",
    "If we search for a minimum of the quantum mechanical  variance, then the matrix \n",
    "$\\hat{A}$, which is called the Hessian, is given by the second-derivative of the function we want to minimize.  This quantity is always positive definite.  In our case this corresponds normally to the second derivative of the energy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method, Newton's method first\n",
    "We seek the minimum of the energy or the variance as function of various variational parameters. \n",
    "In our case we have thus a function $f$ whose minimum we are seeking.\n",
    "In Newton's method we set $\\nabla f = 0$ and we can thus compute the next iteration point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}-\\hat{x}_i=\\hat{A}^{-1}\\nabla f(\\hat{x}_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting this equation from that of $\\hat{x}_{i+1}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_{i+1}-\\hat{x}_i=\\hat{A}^{-1}(\\nabla f(\\hat{x}_{i+1})-\\nabla f(\\hat{x}_i)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method\n",
    "In the CG method we define so-called conjugate directions and two vectors \n",
    "$\\hat{s}$ and $\\hat{t}$\n",
    "are said to be\n",
    "conjugate if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{s}^T\\hat{A}\\hat{t}= 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The philosophy of the CG method is to perform searches in various conjugate directions\n",
    "of our vectors $\\hat{x}_i$ obeying the above criterion, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_i^T\\hat{A}\\hat{x}_j= 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two vectors are conjugate if they are orthogonal with respect to \n",
    "this inner product. Being conjugate is a symmetric relation: if $\\hat{s}$ is conjugate to $\\hat{t}$, then $\\hat{t}$ is conjugate to $\\hat{s}$.\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "An example is given by the eigenvectors of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{v}_i^T\\hat{A}\\hat{v}_j= \\lambda\\hat{v}_i^T\\hat{v}_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is zero unless $i=j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "Assume now that we have a symmetric positive-definite matrix $\\hat{A}$ of size\n",
    "$n\\times n$. At each iteration $i+1$ we obtain the conjugate direction of a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_{i+1}=\\hat{x}_{i}+\\alpha_i\\hat{p}_{i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that $\\hat{p}_{i}$ is a sequence of $n$ mutually conjugate directions. \n",
    "Then the $\\hat{p}_{i}$  form a basis of $R^n$ and we can expand the solution \n",
    "$  \\hat{A}\\hat{x} = \\hat{b}$ in this basis, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}  = \\sum^{n}_{i=1} \\alpha_i \\hat{p}_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method\n",
    "The coefficients are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf{x} = \\sum^{n}_{i=1} \\alpha_i \\mathbf{A} \\mathbf{p}_i = \\mathbf{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying with $\\hat{p}_k^T$  from the left gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}_k^T \\hat{A}\\hat{x} = \\sum^{n}_{i=1} \\alpha_i\\hat{p}_k^T \\hat{A}\\hat{p}_i= \\hat{p}_k^T \\hat{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can define the coefficients $\\alpha_k$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha_k = \\frac{\\hat{p}_k^T \\hat{b}}{\\hat{p}_k^T \\hat{A} \\hat{p}_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method and iterations\n",
    "\n",
    "If we choose the conjugate vectors $\\hat{p}_k$ carefully, \n",
    "then we may not need all of them to obtain a good approximation to the solution \n",
    "$\\hat{x}$. \n",
    "We want to regard the conjugate gradient method as an iterative method. \n",
    "This will us to solve systems where $n$ is so large that the direct \n",
    "method would take too much time.\n",
    "\n",
    "We denote the initial guess for $\\hat{x}$ as $\\hat{x}_0$. \n",
    "We can assume without loss of generality that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_0=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or consider the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A}\\hat{z} = \\hat{b}-\\hat{A}\\hat{x}_0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "One can show that the solution $\\hat{x}$ is also the unique minimizer of the quadratic form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\hat{x}) = \\frac{1}{2}\\hat{x}^T\\hat{A}\\hat{x} - \\hat{x}^T \\hat{x} , \\quad \\hat{x}\\in\\mathbf{R}^n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests taking the first basis vector $\\hat{p}_1$ \n",
    "to be the gradient of $f$ at $\\hat{x}=\\hat{x}_0$, \n",
    "which equals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A}\\hat{x}_0-\\hat{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and \n",
    "$\\hat{x}_0=0$ it is equal $-\\hat{b}$.\n",
    "The other vectors in the basis will be conjugate to the gradient, \n",
    "hence the name conjugate gradient method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "Let  $\\hat{r}_k$ be the residual at the $k$-th step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}_k=\\hat{b}-\\hat{A}\\hat{x}_k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\hat{r}_k$ is the negative gradient of $f$ at \n",
    "$\\hat{x}=\\hat{x}_k$, \n",
    "so the gradient descent method would be to move in the direction $\\hat{r}_k$. \n",
    "Here, we insist that the directions $\\hat{p}_k$ are conjugate to each other, \n",
    "so we take the direction closest to the gradient $\\hat{r}_k$  \n",
    "under the conjugacy constraint. \n",
    "This gives the following expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}_{k+1}=\\hat{r}_k-\\frac{\\hat{p}_k^T \\hat{A}\\hat{r}_k}{\\hat{p}_k^T\\hat{A}\\hat{p}_k} \\hat{p}_k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method\n",
    "We can also  compute the residual iteratively as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}_{k+1}=\\hat{b}-\\hat{A}\\hat{x}_{k+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which equals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{b}-\\hat{A}(\\hat{x}_k+\\alpha_k\\hat{p}_k),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\hat{b}-\\hat{A}\\hat{x}_k)-\\alpha_k\\hat{A}\\hat{p}_k,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}_{k+1}=\\hat{r}_k-\\hat{A}\\hat{p}_{k},\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
