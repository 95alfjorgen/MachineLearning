{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Data Analysis and Machine Learning -->\n",
    "# Data Analysis and Machine Learning\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Jan 27, 2018**\n",
    "\n",
    "Copyright 1999-2018, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What is Machine Learning?\n",
    "\n",
    "Machine learning is the science of giving computers the ability to\n",
    "learn without being explicitly programmed.  The idea is that there\n",
    "exist generic algorithms which can be used to find patterns in a broad\n",
    "class of data sets without having to write code specifically for each\n",
    "problem. The algorithm will build its own logic based on the data.\n",
    "\n",
    "Machine learning is a subfield of computer science, and is closely\n",
    "related to computational statistics.  It evolved from the study of\n",
    "pattern recognition in artificial intelligence (AI) research, and has\n",
    "made contributions to AI tasks like computer vision, natural language\n",
    "processing and speech recognition. It has also, especially in later\n",
    "years, found applications in a wide variety of other areas, including\n",
    "bioinformatics, economy, physics, finance and marketing.\n",
    "\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "\n",
    "The approaches to machine learning are many, but are often split into two main categories. \n",
    "In *supervised learning* we know the answer to a problem,\n",
    "and let the computer deduce the logic behind it. On the other hand, *unsupervised learning*\n",
    "is a method for finding patterns and relationship in data sets without any prior knowledge of the system.\n",
    "Some authours also operate with a third category, namely *reinforcement learning*. This is a paradigm \n",
    "of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, \n",
    "solely from rewards and punishment.\n",
    "\n",
    "Another way to categorize machine learning tasks is to consider the desired output of a system.\n",
    "Some of the most common tasks are:\n",
    "\n",
    "  * Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.\n",
    "\n",
    "  * Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.\n",
    "\n",
    "  * Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.\n",
    "\n",
    "## Different algorithms\n",
    "In this course we will build our machine learning approach on a statistical foundation, with elements \n",
    "from data analysis, stochastic processes etc  before we proceed with the following machine learning algorithms\n",
    "\n",
    "1. Linear regression and its variants\n",
    "\n",
    "2. Decision tree algorithms, from simpler to more complex ones\n",
    "\n",
    "3. Nearest neighbors models\n",
    "\n",
    "4. Bayesian statistics \n",
    "\n",
    "5. Support vector machines and finally various variants of\n",
    "\n",
    "6. Artifical neural networks\n",
    "\n",
    "Before we proceed however, there are several practicalities with data analysis and software tools we would \n",
    "like to present. These tools will help us in our understanding of various machine learning algorithms. \n",
    "\n",
    "Our emphasis here is on understanding the mathematical aspects of different algorithms, however, where possible \n",
    "we will emphasize the importance of using available software. \n",
    "\n",
    "\n",
    "\n",
    "## Software and needed installations\n",
    "We will make intensive use of python as programming language and the myriad of available libraries. \n",
    "Furthermore, you will find IPython/Jupyter notebooks invaluable in your work. \n",
    "You can run **R** codes in the Jupyter/IPython notebooks, with the immediate benefit of visualizing your data.\n",
    "\n",
    "\n",
    "If you have Python installed (we recommend Python3) and you feel pretty familiar with installing different packages, \n",
    "we recommend that you install the following Python packages via **pip** as\n",
    "1. pip install numpy scipy matplotlib ipython scikit-learn mglearn sympy pandas pillow\n",
    "\n",
    "For Python3, replace **pip** with **pip3**.\n",
    "\n",
    "For OSX users we recommend also, after having installed Xcode, to install **brew**. Brew allows \n",
    "for a seamless installation of additional software via for example\n",
    "1. brew install python3\n",
    "\n",
    "For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution\n",
    "you can use **pip** as well and simply install Python as \n",
    "1. sudo apt-get install python3  (or python for pyhton2.7)\n",
    "\n",
    "etc etc. \n",
    "\n",
    "\n",
    "## Python installers\n",
    "If you don't want to perform these operations separately, we recommend two widely used distrubutions which set up \n",
    "all relevant dependencies for Python, namely\n",
    "1. [Anaconda](https://docs.anaconda.com/) Anaconda is an open source distribution of the Python and R programming languages for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. Package versions are managed by the package management system **conda**\n",
    "\n",
    "2. [Enthought canopy](https://www.enthought.com/product/canopy/)  is a Python distribution for scientific and analytic computing distribution and analysis environment, available for free and under a commercial license.\n",
    "\n",
    "## Installing R, C++, cython or Julia\n",
    "\n",
    "You will also find it convenient to utilize R. \n",
    "Jupyter/Ipython notebook allows you run **R** code interactively in your browser. The software library **R** is \n",
    "tuned to statistically analysis and allows for an easy usage of the tools we will discuss in these texts.\n",
    "\n",
    "To install **R** with Jupyter notebook [following the link here](https://mpacer.org/maths/r-kernel-for-ipython-notebook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Installing R, C++, cython or Julia\n",
    "\n",
    "\n",
    "For the C++ affecianodas, Jupyter/IPython notebook allows you also to install C++ and run codes written in this language \n",
    "interactively in the browser. Since we will emphasize writing many of the algorithms yourself, you can thus opt for\n",
    "either Python or C++ as programming languages. \n",
    "\n",
    "To add more entropy, **cython** can also be used when running your notebooks. It means that Python with the Jupyter/IPython notebook \n",
    "setup allows you to integrate widely popular softwares and tools for scientific computing. With its versatility, \n",
    "including symbolic operations, Python offers a unique computational environment. Your Jupyter/IPython notebook \n",
    "can easily be converted into a nicely rendered **PDF** file or a Latex file for further processing. For example, convert to latex as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jupyter nbconvert filename.ipynb --to latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the light mark-up language **doconce** you can convert a standard ascii text file into various HTML \n",
    "formats, ipython notebooks, latex files, pdf files etc. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introduction to Jupyter notebook and available tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "eye = np.eye(4)\n",
    "print(eye)\n",
    "sparse_mtx = sparse.csr_matrix(eye)\n",
    "print(sparse_mtx)\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.sin(x)\n",
    "plt.plot(x,y,marker='x')\n",
    "plt.show()\n",
    "data = {'Name': [\"John\", \"Anna\", \"Peter\", \"Linda\"], 'Location': [\"Nairobi\", \"Napoli\", \"London\", \"Buenos Aires\"], 'Age':[51, 21, 34, 45]}\n",
    "data_pandas = pd.DataFrame(data)\n",
    "display(data_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing data, more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import mglearn\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "x, y = mglearn.datasets.make_wave(n_samples=100)\n",
    "line = np.linspace(-3,3,1000,endpoint=False).reshape(-1,1)\n",
    "reg = DecisionTreeRegressor(min_samples_split=3).fit(x,y)\n",
    "plt.plot(line, reg.predict(line), label=\"decision tree\")\n",
    "regline = LinearRegression().fit(x,y)\n",
    "plt.plot(line, regline.predict(line), label= \"Linear Rgression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predator-Prey model from ecology\n",
    "\n",
    "\n",
    "The population dynamics of a simple predator-prey system is a\n",
    "classical example shown in many biology textbooks when ecological\n",
    "systems are discussed. The system contains all elements of the\n",
    "scientific method:\n",
    "\n",
    " * The set up of a specific hypothesis combined with\n",
    "\n",
    " * the experimental methods needed (one can study existing data or perform experiments)\n",
    "\n",
    " * analyzing and interpreting the data and performing further experiments if needed\n",
    "\n",
    " * trying to extract general behaviors and extract eventual laws or patterns\n",
    "\n",
    " * develop mathematical relations for the uncovered regularities/laws and test these by per forming new experiments\n",
    "\n",
    "## Case study from Hudson bay\n",
    "\n",
    "\n",
    "Lots of data about populations of hares and lynx collected from furs in Hudson Bay, Canada, are available. It is known that the populations oscillate. Why?\n",
    "Here we start by\n",
    "\n",
    "1. plotting the data\n",
    "\n",
    "2. derive a simple model for the population dynamics\n",
    "\n",
    "3. (fitting parameters in the model to the data)\n",
    "\n",
    "4. using the model predict the evolution other predator-pray systems\n",
    "\n",
    "## Hudson bay data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Most mammalian predators rely on a variety of prey, which complicates mathematical modeling; however, a few predators have become highly specialized and seek almost exclusively a single prey species. An example of this simplified predator-prey interaction is seen in Canadian northern forests, where the populations of the lynx and the snowshoe hare are intertwined in a life and death struggle.\n",
    "\n",
    "One reason that this particular system has been so extensively studied is that the Hudson Bay company kept careful records of all furs from the early 1800s into the 1900s. The records for the furs collected by the Hudson Bay company showed distinct oscillations (approximately 12 year periods), suggesting that these species caused almost periodic fluctuations of each other's populations. The table here shows data from 1900 to 1920.\n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">Year</th> <th align=\"center\">Hares (x1000)</th> <th align=\"center\">Lynx (x1000)</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"left\">   1900    </td> <td align=\"right\">   30.0             </td> <td align=\"right\">   4.0             </td> </tr>\n",
    "<tr><td align=\"left\">   1901    </td> <td align=\"right\">   47.2             </td> <td align=\"right\">   6.1             </td> </tr>\n",
    "<tr><td align=\"left\">   1902    </td> <td align=\"right\">   70.2             </td> <td align=\"right\">   9.8             </td> </tr>\n",
    "<tr><td align=\"left\">   1903    </td> <td align=\"right\">   77.4             </td> <td align=\"right\">   35.2            </td> </tr>\n",
    "<tr><td align=\"left\">   1904    </td> <td align=\"right\">   36.3             </td> <td align=\"right\">   59.4            </td> </tr>\n",
    "<tr><td align=\"left\">   1905    </td> <td align=\"right\">   20.6             </td> <td align=\"right\">   41.7            </td> </tr>\n",
    "<tr><td align=\"left\">   1906    </td> <td align=\"right\">   18.1             </td> <td align=\"right\">   19.0            </td> </tr>\n",
    "<tr><td align=\"left\">   1907    </td> <td align=\"right\">   21.4             </td> <td align=\"right\">   13.0            </td> </tr>\n",
    "<tr><td align=\"left\">   1908    </td> <td align=\"right\">   22.0             </td> <td align=\"right\">   8.3             </td> </tr>\n",
    "<tr><td align=\"left\">   1909    </td> <td align=\"right\">   25.4             </td> <td align=\"right\">   9.1             </td> </tr>\n",
    "<tr><td align=\"left\">   1910    </td> <td align=\"right\">   27.1             </td> <td align=\"right\">   7.4             </td> </tr>\n",
    "<tr><td align=\"left\">   1911    </td> <td align=\"right\">   40.3             </td> <td align=\"right\">   8.0             </td> </tr>\n",
    "<tr><td align=\"left\">   1912    </td> <td align=\"right\">   57               </td> <td align=\"right\">   12.3            </td> </tr>\n",
    "<tr><td align=\"left\">   1913    </td> <td align=\"right\">   76.6             </td> <td align=\"right\">   19.5            </td> </tr>\n",
    "<tr><td align=\"left\">   1914    </td> <td align=\"right\">   52.3             </td> <td align=\"right\">   45.7            </td> </tr>\n",
    "<tr><td align=\"left\">   1915    </td> <td align=\"right\">   19.5             </td> <td align=\"right\">   51.1            </td> </tr>\n",
    "<tr><td align=\"left\">   1916    </td> <td align=\"right\">   11.2             </td> <td align=\"right\">   29.7            </td> </tr>\n",
    "<tr><td align=\"left\">   1917    </td> <td align=\"right\">   7.6              </td> <td align=\"right\">   15.8            </td> </tr>\n",
    "<tr><td align=\"left\">   1918    </td> <td align=\"right\">   14.6             </td> <td align=\"right\">   9.7             </td> </tr>\n",
    "<tr><td align=\"left\">   1919    </td> <td align=\"right\">   16.2             </td> <td align=\"right\">   10.1            </td> </tr>\n",
    "<tr><td align=\"left\">   1920    </td> <td align=\"right\">   24.7             </td> <td align=\"right\">   8.6             </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "# Load in data file\n",
    "data = np.loadtxt('src/Hudson_Bay.csv', delimiter=',', skiprows=1)\n",
    "# Make arrays containing x-axis and hares and lynx populations\n",
    "year = data[:,0]\n",
    "hares = data[:,1]\n",
    "lynx = data[:,2]\n",
    "\n",
    "plt.plot(year, hares ,'b-+', year, lynx, 'r-o')\n",
    "plt.axis([1900,1920,0, 100.0])\n",
    "plt.xlabel(r'Year')\n",
    "plt.ylabel(r'Numbers of hares and lynx ')\n",
    "plt.legend(('Hares','Lynx'), loc='upper right')\n",
    "plt.title(r'Population of hares and lynx from 1900-1920 (x1000)}')\n",
    "plt.savefig('Hudson_Bay_data.pdf')\n",
    "plt.savefig('Hudson_Bay_data.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hares and lynx in Hudson bay from 1900 to 1920\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Hudson_Bay_data.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"fig/Hudson_Bay_data.png\" width=700>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Why now create a computer model for the hare and lynx populations?\n",
    "\n",
    "We see from the plot that there are indeed fluctuations.\n",
    "We would like to create a mathematical model that explains these\n",
    "population fluctuations. Ecologists have predicted that in a simple\n",
    "predator-prey system that a rise in prey population is followed (with\n",
    "a lag) by a rise in the predator population. When the predator\n",
    "population is sufficiently high, then the prey population begins\n",
    "dropping. After the prey population falls, then the predator\n",
    "population falls, which allows the prey population to recover and\n",
    "complete one cycle of this interaction. Thus, we see that\n",
    "qualitatively oscillations occur. Can a mathematical model predict\n",
    "this? What causes cycles to slow or speed up? What affects the\n",
    "amplitude of the oscillation or do you expect to see the oscillations\n",
    "damp to a stable equilibrium? The models tend to ignore factors like\n",
    "climate and other complicating factors. How significant are these?\n",
    "\n",
    " * We see oscillations in the data\n",
    "\n",
    " * What causes cycles to slow or speed up?\n",
    "\n",
    " * What affects the amplitude of the oscillation or do you expect to see the oscillations damp to a stable equilibrium?\n",
    "\n",
    " * With a model we can better *understand the data*\n",
    "\n",
    " * More important: we can understand the ecology dynamics of\n",
    "   predator-pray populations\n",
    "\n",
    "## The traditional (top-down) approach\n",
    "\n",
    "\n",
    "The classical way (in all books) is to present the Lotka-Volterra equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dH}{dt} &= H(a - b L)\\\\\n",
    "\\frac{dL}{dt} &= - L(d - c  H)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "\n",
    " * $H$ is the number of preys\n",
    "\n",
    " * $L$ the number of predators\n",
    "\n",
    " * $a$, $b$, $d$, $c$ are parameters\n",
    "\n",
    "Most books quickly establish the model and then use considerable space on\n",
    "discussing the qualitative properties of this *nonlinear system of\n",
    "ODEs* (which cannot be solved)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Basic  mathematics notation\n",
    "\n",
    " * Time points: $t_0,t_1,\\ldots,t_m$\n",
    "\n",
    " * Uniform distribution of time points: $t_n=n\\Delta t$\n",
    "\n",
    " * $H^n$: population of hares at time $t_n$\n",
    "\n",
    " * $L^n$: population of lynx at time $t_n$\n",
    "\n",
    " * We want to model the changes in populations, $\\Delta H=H^{n+1}-H^n$\n",
    "   and $\\Delta L=L^{n+1}-L^n$ during a general time interval $[t_{n+1},t_n]$\n",
    "   of length $\\Delta t=t_{n+1}-t_n$\n",
    "\n",
    "## Basic dynamics of the population of hares\n",
    "\n",
    "\n",
    "The population of hares evolves due to births and deaths exactly as a bacteria population:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta H = a \\Delta t H^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, hares have an additional loss in the population because\n",
    "they are eaten by lynx.\n",
    "All the hares and lynx can form\n",
    "$H\\cdot L$ pairs in total. When such pairs meet during a time\n",
    "interval $\\Delta t$, there is some\n",
    "small probablity that the lynx will eat the hare.\n",
    "So in fraction $b\\Delta t HL$, the lynx eat hares. This\n",
    "loss of hares must be accounted for. Subtracted in the equation for hares:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta H = a\\Delta t H^n - b \\Delta t H^nL^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic dynamics of the population of lynx\n",
    "\n",
    "\n",
    "We assume that the primary growth for the lynx population depends on sufficient food for raising lynx kittens, which implies an adequate source of nutrients from predation on hares. Thus, the growth of the lynx population does not only depend of how many lynx there are, but on how many hares they can eat.\n",
    "In a time interval $\\Delta t HL$ hares and lynx can meet, and in a\n",
    "fraction $b\\Delta t HL$ the lynx eats the hare. All of this does not\n",
    "contribute to the growth of lynx, again just a fraction of\n",
    "$b\\Delta t HL$ that we write as\n",
    "$d\\Delta t HL$. In addition, lynx die just as in the population\n",
    "dynamics with one isolated animal population, leading to a loss\n",
    "$-c\\Delta t L$.\n",
    "\n",
    "\n",
    "\n",
    "The accounting of lynx then looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta L = d\\Delta t H^nL^n - c\\Delta t L^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution equations\n",
    "\n",
    "\n",
    "By writing up the definition of $\\Delta H$ and $\\Delta L$, and putting\n",
    "all assumed known terms $H^n$ and $L^n$ on the right-hand side, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H^{n+1} = H^n + a\\Delta t H^n - b\\Delta t H^n L^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L^{n+1} = L^n + d\\Delta t H^nL^n - c\\Delta t L^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    " * These equations are ready to be implemented!\n",
    "\n",
    " * But to start, we need $H^0$ and $L^0$ \n",
    "   (which we can get from the data)\n",
    "\n",
    " * We also need values for $a$, $b$, $d$, $c$\n",
    "\n",
    "## Adapt the model to the Hudson Bay case\n",
    "\n",
    "\n",
    " * As always, models tend to be general - as here, applicable\n",
    "   to \"all\" predator-pray systems\n",
    "\n",
    " * The critical issue is whether the *interaction* between hares and lynx\n",
    "   is sufficiently well modeled by $\\hbox{const}HL$\n",
    "\n",
    " * The parameters $a$, $b$, $d$, and $c$ must be\n",
    "   estimated from data\n",
    "\n",
    " * Measure time in years\n",
    "\n",
    " * $t_0=1900$, $t_m=1920$\n",
    "\n",
    "## The program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def solver(m, H0, L0, dt, a, b, c, d, t0):\n",
    "    \"\"\"Solve the difference equations for H and L over m years\n",
    "    with time step dt (measured in years.\"\"\"\n",
    "\n",
    "    num_intervals = int(m/float(dt))\n",
    "    t = np.linspace(t0, t0 + m, num_intervals+1)\n",
    "    H = np.zeros(t.size)\n",
    "    L = np.zeros(t.size)\n",
    "\n",
    "    print('Init:', H0, L0, dt)\n",
    "    H[0] = H0\n",
    "    L[0] = L0\n",
    "\n",
    "    for n in range(0, len(t)-1):\n",
    "        H[n+1] = H[n] + a*dt*H[n] - b*dt*H[n]*L[n]\n",
    "        L[n+1] = L[n] + d*dt*H[n]*L[n] - c*dt*L[n]\n",
    "    return H, L, t\n",
    "\n",
    "# Load in data file\n",
    "data = np.loadtxt('src/Hudson_Bay.csv', delimiter=',', skiprows=1)\n",
    "# Make arrays containing x-axis and hares and lynx populations\n",
    "t_e = data[:,0]\n",
    "H_e = data[:,1]\n",
    "L_e = data[:,2]\n",
    "\n",
    "# Simulate using the model\n",
    "H, L, t = solver(m=20, H0=34.91, L0=3.857, dt=0.1,\n",
    "                 a=0.4807, b=0.02482, c=0.9272, d=0.02756,\n",
    "                 t0=1900)\n",
    "\n",
    "# Visualize simulations and data\n",
    "plt.plot(t_e, H_e, 'b-+', t_e, L_e, 'r-o', t, H, 'm--', t, L, 'k--')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Numbers of hares and lynx')\n",
    "plt.axis([1900, 1920, 0, 140])\n",
    "plt.title(r'Population of hares and lynx 1900-1920 (x1000)')\n",
    "plt.legend(('H_e', 'L_e', 'H', 'L'), loc='upper left')\n",
    "plt.savefig('Hudson_Bay_sim.pdf')\n",
    "plt.savefig('Hudson_Bay_sim.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The plot\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Hudson_Bay_sim.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"fig/Hudson_Bay_sim.png\" width=700>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "If we perform a least-square fitting, we can find optimal values for the parameters $a$, $b$, $d$, $c$. The optimal parameters are $a=0.4807$, $b=0.02482$, $d=0.9272$ and $c=0.02756$. These parameters result in a slightly modified initial conditions, namely $H(0) = 34.91$ and $L(0)=3.857$. With these parameters we are now ready to solve the equations and plot these data together with the experimental values.\n",
    "\n",
    "\n",
    "\n",
    "## Linear regression in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "data = np.loadtxt('src/Hudson_Bay.csv', delimiter=',', skiprows=1)\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "line = np.linspace(1900,1920,1000,endpoint=False).reshape(-1,1)\n",
    "reg = DecisionTreeRegressor(min_samples_split=3).fit(x.reshape(-1,1),y.reshape(-1,1))\n",
    "plt.plot(line, reg.predict(line), label=\"decision tree\")\n",
    "regline = LinearRegression().fit(x.reshape(-1,1),y.reshape(-1,1))\n",
    "plt.plot(line, regline.predict(line), label= \"Linear Regression\")\n",
    "plt.plot(x, y, label= \"Linear Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least squares in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        HudsonBay = read.csv(\"src/Hudson_Bay.csv\",header=T)\n",
    "        fix(HudsonBay)\n",
    "        dim(HudsonBay)\n",
    "        names(HudsonBay)\n",
    "        plot(HudsonBay$Year, HudsonBay$Hares..x1000.)\n",
    "        attach(HudsonBay)\n",
    "        plot(Year, Hares..x1000.)\n",
    "        plot(Year, Hares..x1000., col=\"red\", varwidth=T, xlab=\"Years\", ylab=\"Haresx 1000\")\n",
    "        summary(HudsonBay)\n",
    "        summary(Hares..x1000.)\n",
    "        library(MASS)\n",
    "        library(ISLR)\n",
    "        scatter.smooth(x=Year, y = Hares..x1000.)\n",
    "        linearMod = lm(Hares..x1000. ~ Year)\n",
    "        print(linearMod)\n",
    "        summary(linearMod)\n",
    "        plot(linearMod)\n",
    "        confint(linearMod)\n",
    "        predict(linearMod,data.frame(Year=c(1910,1914,1920)),interval=\"confidence\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Least squares in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        set.seed(1485)\n",
    "        len = 24\n",
    "        x = runif(len)\n",
    "        y = x^3+rnorm(len, 0,0.06)\n",
    "        ds = data.frame(x = x, y = y)\n",
    "        str(ds)\n",
    "        plot( y ~ x, main =\"Known cubic with noise\")\n",
    "        s  = seq(0,1,length =100)\n",
    "        lines(s, s^3, lty =2, col =\"green\")\n",
    "        m = nls(y ~ I(x^power), data = ds, start = list(power=1), trace = T)\n",
    "        class(m)\n",
    "        summary(m)\n",
    "        power = round(summary(m)$coefficients[1], 3)\n",
    "        power.se = round(summary(m)$coefficients[2], 3)\n",
    "        plot(y ~ x, main = \"Fitted power model\", sub = \"Blue: fit; green: known\")\n",
    "        s = seq(0, 1, length = 100)\n",
    "        lines(s, s^3, lty = 2, col = \"green\")\n",
    "        lines(s, predict(m, list(x = s)), lty = 1, col = \"blue\")\n",
    "        text(0, 0.5, paste(\"y =x^ (\", power, \" +/- \", power.se, \")\", sep = \"\"), pos = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Matrix and vector handling packages\n",
    "\n",
    "The Numerical Recipes codes have been rewritten in Fortran 90/95 and\n",
    "C/C++ by us.  The original source codes are taken from the widely used\n",
    "software package LAPACK, which follows two other popular packages\n",
    "developed in the 1970s, namely EISPACK and LINPACK.\n",
    "\n",
    "  * LINPACK: package for linear equations and least square problems.\n",
    "\n",
    "  * LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website <http://www.netlib.org> it is possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.\n",
    "\n",
    "  * BLAS (I, II and III): (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. Highly parallelized and efficient codes, all available for download from <http://www.netlib.org>.\n",
    "\n",
    "**Add python material on linear algebra and array handling, text on numpy etc**\n",
    "\n",
    "\n",
    "## Basic Matrix Features\n",
    "\n",
    " Matrix properties reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A} =\n",
    "      \\begin{bmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\\\\n",
    "                                 a_{21} & a_{22} & a_{23} & a_{24} \\\\\n",
    "                                   a_{31} & a_{32} & a_{33} & a_{34} \\\\\n",
    "                                  a_{41} & a_{42} & a_{43} & a_{44}\n",
    "             \\end{bmatrix}\\qquad\n",
    "\\mathbf{I} =\n",
    "      \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\\n",
    "                                 0 & 1 & 0 & 0 \\\\\n",
    "                                 0 & 0 & 1 & 0 \\\\\n",
    "                                 0 & 0 & 0 & 1\n",
    "             \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Matrix Features\n",
    "\n",
    "The inverse of a matrix is defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}^{-1} \\cdot \\mathbf{A} = I\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Matrix Features\n",
    "\n",
    " Matrix Properties Reminder\n",
    "\n",
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">              Relations               </th> <th align=\"center\">      Name     </th> <th align=\"center\">                            matrix elements                            </th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   $A = A^{T}$                               </td> <td align=\"center\">   symmetric          </td> <td align=\"center\">   $a_{ij} = a_{ji}$                                                          </td> </tr>\n",
    "<tr><td align=\"center\">   $A = \\left (A^{T} \\right )^{-1}$          </td> <td align=\"center\">   real orthogonal    </td> <td align=\"center\">   $\\sum_k a_{ik} a_{jk} = \\sum_k a_{ki} a_{kj} = \\delta_{ij}$                </td> </tr>\n",
    "<tr><td align=\"center\">   $A = A^{ * }$                             </td> <td align=\"center\">   real matrix        </td> <td align=\"center\">   $a_{ij} = a_{ij}^{ * }$                                                    </td> </tr>\n",
    "<tr><td align=\"center\">   $A = A^{\\dagger}$                         </td> <td align=\"center\">   hermitian          </td> <td align=\"center\">   $a_{ij} = a_{ji}^{ * }$                                                    </td> </tr>\n",
    "<tr><td align=\"center\">   $A = \\left (A^{\\dagger} \\right )^{-1}$    </td> <td align=\"center\">   unitary            </td> <td align=\"center\">   $\\sum_k a_{ik} a_{jk}^{ * } = \\sum_k a_{ki}^{ * } a_{kj} = \\delta_{ij}$    </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "## Some famous Matrices\n",
    "\n",
    "  * Diagonal if $a_{ij}=0$ for $i\\ne j$\n",
    "\n",
    "  * Upper triangular if $a_{ij}=0$ for $i > j$\n",
    "\n",
    "  * Lower triangular if $a_{ij}=0$ for $i < j$\n",
    "\n",
    "  * Upper Hessenberg if $a_{ij}=0$ for $i > j+1$\n",
    "\n",
    "  * Lower Hessenberg if $a_{ij}=0$ for $i < j+1$\n",
    "\n",
    "  * Tridiagonal if $a_{ij}=0$ for $|i -j| > 1$\n",
    "\n",
    "  * Lower banded with bandwidth $p$: $a_{ij}=0$ for $i > j+p$\n",
    "\n",
    "  * Upper banded with bandwidth $p$: $a_{ij}=0$ for $i < j+p$\n",
    "\n",
    "  * Banded, block upper triangular, block lower triangular....\n",
    "\n",
    "## Basic Matrix Features\n",
    "\n",
    " Some Equivalent Statements\n",
    "For an $N\\times N$ matrix  $\\mathbf{A}$ the following properties are all equivalent\n",
    "\n",
    "  * If the inverse of $\\mathbf{A}$ exists, $\\mathbf{A}$ is nonsingular.\n",
    "\n",
    "  * The equation $\\mathbf{Ax}=0$ implies $\\mathbf{x}=0$.\n",
    "\n",
    "  * The rows of $\\mathbf{A}$ form a basis of $R^N$.\n",
    "\n",
    "  * The columns of $\\mathbf{A}$ form a basis of $R^N$.\n",
    "\n",
    "  * $\\mathbf{A}$ is a product of elementary matrices.\n",
    "\n",
    "  * $0$ is not eigenvalue of $\\mathbf{A}$.\n",
    "\n",
    "## Matrix Handling in C/C++, Static and Dynamical allocation\n",
    "\n",
    " Static\n",
    "We have  an $N\\times N$ matrix A  with $N=100$\n",
    "In C/C++ this would be  defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           int N = 100;\n",
    "           double A[100][100];\n",
    "           //   initialize all elements to zero\n",
    "           for(i=0 ; i < N ; i++) {\n",
    "              for(j=0 ; j < N ; j++) {\n",
    "                 A[i][j] = 0.0;\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the way the matrix is organized, row-major order.\n",
    "\n",
    "\n",
    "\n",
    "## Matrix Handling in C/C++\n",
    "\n",
    " Row Major Order, Addition\n",
    "We have  $N\\times N$ matrices A, B and C and we wish to\n",
    "evaluate $A=B+C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}= \\mathbf{B}\\pm\\mathbf{C}  \\Longrightarrow a_{ij} = b_{ij}\\pm c_{ij},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In C/C++ this would be coded like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           for(i=0 ; i < N ; i++) {\n",
    "              for(j=0 ; j < N ; j++) {\n",
    "                 a[i][j] = b[i][j]+c[i][j]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Handling in C/C++\n",
    "\n",
    " Row Major Order, Multiplication\n",
    "We have  $N\\times N$ matrices A, B and C and we wish to\n",
    "evaluate $A=BC$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}=\\mathbf{BC}   \\Longrightarrow a_{ij} = \\sum_{k=1}^{n} b_{ik}c_{kj},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In C/C++ this would be coded like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           for(i=0 ; i < N ; i++) {\n",
    "              for(j=0 ; j < N ; j++) {\n",
    "                 for(k=0 ; k < N ; k++) {\n",
    "                    a[i][j]+=b[i][k]*c[k][j];\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic memory allocation in C/C++\n",
    "\n",
    "At least three possibilities in this course\n",
    "\n",
    "  * Do it yourself\n",
    "\n",
    "  * Use the functions provided in the library package lib.cpp\n",
    "\n",
    "  * Use Armadillo <http://arma.sourceforgenet> (a C++ linear algebra library, discussion both here and at lab). \n",
    "\n",
    "## Matrix Handling in C/C++, Dynamic Allocation\n",
    "\n",
    " Do it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        int N;\n",
    "        double **  A;\n",
    "        A = new double*[N]\n",
    "        for ( i = 0; i < N; i++)\n",
    "            A[i] = new double[N];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always free space when you don't need an array anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        for ( i = 0; i < N; i++)\n",
    "            delete[] A[i];\n",
    "        delete[] A;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, recommended!!\n",
    "\n",
    "  * Armadillo is a C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. The syntax is deliberately similar to Matlab.\n",
    "\n",
    "  * Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Various matrix decompositions are provided through optional integration with LAPACK, or one of its high performance drop-in replacements (such as the multi-threaded MKL or ACML libraries).\n",
    "\n",
    "  * A delayed evaluation approach is employed (at compile-time) to combine several operations into one and reduce (or eliminate) the need for temporaries. This is accomplished through recursive templates and template meta-programming.\n",
    "\n",
    "  * Useful for conversion of research code into production environments, or if C++ has been decided as the language of choice, due to speed and/or integration capabilities.\n",
    "\n",
    "  * The library is open-source software, and is distributed under a license that is useful in both open-source and commercial/proprietary contexts.\n",
    "\n",
    "## Armadillo, simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        #include <iostream>\n",
    "        #include <armadillo>\n",
    "        \n",
    "        using namespace std;\n",
    "        using namespace arma;\n",
    "        \n",
    "        int main(int argc, char** argv)\n",
    "          {\n",
    "          mat A = randu<mat>(5,5);\n",
    "          mat B = randu<mat>(5,5);\n",
    "        \n",
    "          cout << A*B << endl;\n",
    "        \n",
    "          return 0;\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, how to compile and install\n",
    "\n",
    "For people using Ubuntu, Debian, Linux Mint, simply go to the synaptic package manager and install\n",
    "armadillo from there.\n",
    "You may have to install Lapack as well.\n",
    "For Mac and Windows users, follow the instructions from the webpage\n",
    "<http://arma.sourceforge.net>.\n",
    "To compile, use for example (linux/ubuntu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        c++ -O2 -o program.x program.cpp  -larmadillo -llapack -lblas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the `-l` option indicates the library you wish to link to.\n",
    "\n",
    "For OS X users you may have to declare the paths to the include files and the libraries as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        c++ -O2 -o program.x program.cpp  -L/usr/local/lib -I/usr/local/include -larmadillo -llapack -lblas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        #include <iostream>\n",
    "        #include \"armadillo\"\n",
    "        using namespace arma;\n",
    "        using namespace std;\n",
    "        \n",
    "        int main(int argc, char** argv)\n",
    "          {\n",
    "          // directly specify the matrix size (elements are uninitialised)\n",
    "          mat A(2,3);\n",
    "          // .n_rows = number of rows    (read only)\n",
    "          // .n_cols = number of columns (read only)\n",
    "          cout << \"A.n_rows = \" << A.n_rows << endl;\n",
    "          cout << \"A.n_cols = \" << A.n_cols << endl;\n",
    "          // directly access an element (indexing starts at 0)\n",
    "          A(1,2) = 456.0;\n",
    "          A.print(\"A:\");\n",
    "          // scalars are treated as a 1x1 matrix,\n",
    "          // hence the code below will set A to have a size of 1x1\n",
    "          A = 5.0;\n",
    "          A.print(\"A:\");\n",
    "          // if you want a matrix with all elements set to a particular value\n",
    "          // the .fill() member function can be used\n",
    "          A.set_size(3,3);\n",
    "          A.fill(5.0);  A.print(\"A:\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          mat B;\n",
    "        \n",
    "          // endr indicates \"end of row\"\n",
    "          B << 0.555950 << 0.274690 << 0.540605 << 0.798938 << endr\n",
    "            << 0.108929 << 0.830123 << 0.891726 << 0.895283 << endr\n",
    "            << 0.948014 << 0.973234 << 0.216504 << 0.883152 << endr\n",
    "            << 0.023787 << 0.675382 << 0.231751 << 0.450332 << endr;\n",
    "        \n",
    "          // print to the cout stream\n",
    "          // with an optional string before the contents of the matrix\n",
    "          B.print(\"B:\");\n",
    "        \n",
    "          // the << operator can also be used to print the matrix\n",
    "          // to an arbitrary stream (cout in this case)\n",
    "          cout << \"B:\" << endl << B << endl;\n",
    "          // save to disk\n",
    "          B.save(\"B.txt\", raw_ascii);\n",
    "          // load from disk\n",
    "          mat C;\n",
    "          C.load(\"B.txt\");\n",
    "          C += 2.0 * B;\n",
    "          C.print(\"C:\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          // submatrix types:\n",
    "          //\n",
    "          // .submat(first_row, first_column, last_row, last_column)\n",
    "          // .row(row_number)\n",
    "          // .col(column_number)\n",
    "          // .cols(first_column, last_column)\n",
    "          // .rows(first_row, last_row)\n",
    "        \n",
    "          cout << \"C.submat(0,0,3,1) =\" << endl;\n",
    "          cout << C.submat(0,0,3,1) << endl;\n",
    "        \n",
    "          // generate the identity matrix\n",
    "          mat D = eye<mat>(4,4);\n",
    "        \n",
    "          D.submat(0,0,3,1) = C.cols(1,2);\n",
    "          D.print(\"D:\");\n",
    "        \n",
    "          // transpose\n",
    "          cout << \"trans(B) =\" << endl;\n",
    "          cout << trans(B) << endl;\n",
    "        \n",
    "          // maximum from each column (traverse along rows)\n",
    "          cout << \"max(B) =\" << endl;\n",
    "          cout << max(B) << endl;\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          // maximum from each row (traverse along columns)\n",
    "          cout << \"max(B,1) =\" << endl;\n",
    "          cout << max(B,1) << endl;\n",
    "          // maximum value in B\n",
    "          cout << \"max(max(B)) = \" << max(max(B)) << endl;\n",
    "          // sum of each column (traverse along rows)\n",
    "          cout << \"sum(B) =\" << endl;\n",
    "          cout << sum(B) << endl;\n",
    "          // sum of each row (traverse along columns)\n",
    "          cout << \"sum(B,1) =\" << endl;\n",
    "          cout << sum(B,1) << endl;\n",
    "          // sum of all elements\n",
    "          cout << \"sum(sum(B)) = \" << sum(sum(B)) << endl;\n",
    "          cout << \"accu(B)     = \" << accu(B) << endl;\n",
    "          // trace = sum along diagonal\n",
    "          cout << \"trace(B)    = \" << trace(B) << endl;\n",
    "          // random matrix -- values are uniformly distributed in the [0,1] interval\n",
    "          mat E = randu<mat>(4,4);\n",
    "          E.print(\"E:\");\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          // row vectors are treated like a matrix with one row\n",
    "          rowvec r;\n",
    "          r << 0.59499 << 0.88807 << 0.88532 << 0.19968;\n",
    "          r.print(\"r:\");\n",
    "        \n",
    "          // column vectors are treated like a matrix with one column\n",
    "          colvec q;\n",
    "          q << 0.81114 << 0.06256 << 0.95989 << 0.73628;\n",
    "          q.print(\"q:\");\n",
    "        \n",
    "          // dot or inner product\n",
    "          cout << \"as_scalar(r*q) = \" << as_scalar(r*q) << endl;\n",
    "        \n",
    "            // outer product\n",
    "          cout << \"q*r =\" << endl;\n",
    "          cout << q*r << endl;\n",
    "        \n",
    "        \n",
    "          // sum of three matrices (no temporary matrices are created)\n",
    "          mat F = B + C + D;\n",
    "          F.print(\"F:\");\n",
    "        \n",
    "            return 0;\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        #include <iostream>\n",
    "        #include \"armadillo\"\n",
    "        using namespace arma;\n",
    "        using namespace std;\n",
    "        \n",
    "        int main(int argc, char** argv)\n",
    "          {\n",
    "          cout << \"Armadillo version: \" << arma_version::as_string() << endl;\n",
    "        \n",
    "          mat A;\n",
    "        \n",
    "          A << 0.165300 << 0.454037 << 0.995795 << 0.124098 << 0.047084 << endr\n",
    "            << 0.688782 << 0.036549 << 0.552848 << 0.937664 << 0.866401 << endr\n",
    "            << 0.348740 << 0.479388 << 0.506228 << 0.145673 << 0.491547 << endr\n",
    "            << 0.148678 << 0.682258 << 0.571154 << 0.874724 << 0.444632 << endr\n",
    "            << 0.245726 << 0.595218 << 0.409327 << 0.367827 << 0.385736 << endr;\n",
    "        \n",
    "          A.print(\"A =\");\n",
    "        \n",
    "          // determinant\n",
    "          cout << \"det(A) = \" << det(A) << endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armadillo, simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          // inverse\n",
    "          cout << \"inv(A) = \" << endl << inv(A) << endl;\n",
    "          double k = 1.23;\n",
    "        \n",
    "          mat    B = randu<mat>(5,5);\n",
    "          mat    C = randu<mat>(5,5);\n",
    "        \n",
    "          rowvec r = randu<rowvec>(5);\n",
    "          colvec q = randu<colvec>(5);\n",
    "        \n",
    "        \n",
    "          // examples of some expressions\n",
    "          // for which optimised implementations exist\n",
    "          // optimised implementation of a trinary expression\n",
    "          // that results in a scalar\n",
    "          cout << \"as_scalar( r*inv(diagmat(B))*q ) = \";\n",
    "          cout << as_scalar( r*inv(diagmat(B))*q ) << endl;\n",
    "        \n",
    "          // example of an expression which is optimised\n",
    "          // as a call to the dgemm() function in BLAS:\n",
    "          cout << \"k*trans(B)*C = \" << endl << k*trans(B)*C;\n",
    "        \n",
    "            return 0;\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Elimination\n",
    "\n",
    "We start with the linear set of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf{x} = \\mathbf{w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume also that the matrix $\\mathbf{A}$ is non-singular and that the\n",
    "matrix elements along the diagonal satisfy $a_{ii} \\ne 0$. Simple $4\\times 4 $ example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "                           a_{11}& a_{12} &a_{13}& a_{14}\\\\\n",
    "                           a_{21}& a_{22} &a_{23}& a_{24}\\\\\n",
    "                           a_{31}& a_{32} &a_{33}& a_{34}\\\\\n",
    "                           a_{41}& a_{42} &a_{43}& a_{44}\\\\\n",
    "                      \\end{bmatrix} \\begin{bmatrix}\n",
    "                           x_1\\\\\n",
    "                           x_2\\\\\n",
    "                           x_3 \\\\\n",
    "                           x_4  \\\\\n",
    "                      \\end{bmatrix}\n",
    "  =\\begin{bmatrix}\n",
    "                           w_1\\\\\n",
    "                           w_2\\\\\n",
    "                           w_3 \\\\\n",
    "                           w_4\\\\\n",
    "                      \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Elimination\n",
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=w_1 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=w_2 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=w_3 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=w_4. \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Elimination\n",
    "\n",
    "The basic idea of Gaussian elimination is to use the first equation to eliminate the first unknown $x_1$\n",
    "from the remaining $n-1$ equations. Then we use the new second equation to eliminate the second unknown\n",
    "$x_2$ from the remaining $n-2$ equations. With $n-1$ such eliminations\n",
    "we obtain a so-called upper triangular set of equations of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=y_1 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "b_{22}x_2 + b_{23}x_3 + b_{24}x_4=y_2 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "b_{33}x_3 + b_{34}x_4=y_3 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:gaussbacksub\"></div>\n",
    "\n",
    "$$\n",
    "b_{44}x_4=y_4. \\nonumber\n",
    "\\label{eq:gaussbacksub} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this system of equations recursively starting from $x_n$ (in our case $x_4$) and proceed with\n",
    "what is called a backward substitution. \n",
    "\n",
    "\n",
    "## Gaussian Elimination\n",
    "This process can be expressed mathematically as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   x_m = \\frac{1}{b_{mm}}\\left(y_m-\\sum_{k=m+1}^nb_{mk}x_k\\right)\\quad m=n-1,n-2,\\dots,1.\n",
    "\\label{_auto1} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To arrive at such an upper triangular system of equations, we start by eliminating\n",
    "the unknown $x_1$ for $j=2,n$. We achieve this by multiplying the first equation by $a_{j1}/a_{11}$ and then subtract\n",
    "the result from the $j$th equation. We assume obviously that $a_{11}\\ne 0$ and that\n",
    "$\\mathbf{A}$ is not singular.\n",
    "\n",
    "\n",
    "## Gaussian Elimination\n",
    "\n",
    "Our actual $4\\times 4$ example reads after the first operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "                           a_{11}& a_{12} &a_{13}& a_{14}\\\\\n",
    "                           0& (a_{22}-\\frac{a_{21}a_{12}}{a_{11}}) &(a_{23}-\\frac{a_{21}a_{13}}{a_{11}}) & (a_{24}-\\frac{a_{21}a_{14}}{a_{11}})\\\\\n",
    "0& (a_{32}-\\frac{a_{31}a_{12}}{a_{11}})& (a_{33}-\\frac{a_{31}a_{13}}{a_{11}})& (a_{34}-\\frac{a_{31}a_{14}}{a_{11}})\\\\\n",
    "0&(a_{42}-\\frac{a_{41}a_{12}}{a_{11}}) &(a_{43}-\\frac{a_{41}a_{13}}{a_{11}}) & (a_{44}-\\frac{a_{41}a_{14}}{a_{11}}) \\\\\n",
    "                      \\end{bmatrix} \\begin{bmatrix}\n",
    "                           x_1\\\\\n",
    "                           x_2\\\\\n",
    "                           x_3 \\\\\n",
    "                           x_4  \\\\\n",
    "                      \\end{bmatrix} \n",
    "  =\\begin{bmatrix}\n",
    "                           y_1\\\\\n",
    "                           w_2^{(2)}\\\\\n",
    "                           w_3^{(2)} \\\\\n",
    "                           w_4^{(2)}\\\\\n",
    "                      \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=y_1 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(2)}_{22}x_2 + a^{(2)}_{23}x_3 + a^{(2)}_{24}x_4=w^{(2)}_2 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(2)}_{32}x_2 + a^{(2)}_{33}x_3 + a^{(2)}_{34}x_4=w^{(2)}_3 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(2)}_{42}x_2 + a^{(2)}_{43}x_3 + a^{(2)}_{44}x_4=w^{(2)}_4, \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\label{_auto2} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Elimination\n",
    "\n",
    "The new coefficients are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   b_{1k} = a_{1k}^{(1)} \\quad k=1,\\dots,n,\n",
    "\\label{_auto3} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each $a_{1k}^{(1)}$ is equal to the original $a_{1k}$ element. The other coefficients are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a_{jk}^{(2)} = a_{jk}^{(1)}-\\frac{a_{j1}^{(1)}a_{1k}^{(1)}}{a_{11}^{(1)}} \\quad j,k=2,\\dots,n,\n",
    "\\label{_auto4} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a new right-hand side given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_{1}=w_1^{(1)}, \\quad w_j^{(2)} =w_j^{(1)}-\\frac{a_{j1}^{(1)}w_1^{(1)}}{a_{11}^{(1)}} \\quad j=2,\\dots,n.\n",
    "\\label{_auto5} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also set $w_1^{(1)}=w_1$, the original vector element.\n",
    "We see that the system of unknowns $x_1,\\dots,x_n$ is transformed into an $(n-1)\\times (n-1)$ problem.\n",
    "\n",
    "\n",
    "## Gaussian Elimination\n",
    "\n",
    "This step is called forward substitution.\n",
    "Proceeding with these substitutions, we obtain the\n",
    "general expressions for the new coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   a_{jk}^{(m+1)} = a_{jk}^{(m)}-\\frac{a_{jm}^{(m)}a_{mk}^{(m)}}{a_{mm}^{(m)}} \\quad j,k=m+1,\\dots,n,\n",
    "\\label{_auto6} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $m=1,\\dots,n-1$ and a\n",
    "right-hand side given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   w_j^{(m+1)} =w_j^{(m)}-\\frac{a_{jm}^{(m)}w_m^{(m)}}{a_{mm}^{(m)}}\\quad j=m+1,\\dots,n.\n",
    "\\label{_auto7} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of $n-1$ elimations leads us to an equations which is solved by back substitution.\n",
    "If the arithmetics is exact and the matrix $\\mathbf{A}$ is not singular, then the computed answer will be exact.\n",
    "\n",
    "Even though the matrix elements along the diagonal are not zero,\n",
    "numerically small numbers may appear and subsequent divisions may lead to large numbers, which, if added\n",
    "to a small number may yield losses of precision. Suppose for example that our first division in $(a_{22}-a_{21}a_{12}/a_{11})$\n",
    "results in $-10^{-7}$ and that $a_{22}$ is one.\n",
    "one. We are then\n",
    "adding $10^7+1$. With single precision this results in $10^7$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Linear Algebra Methods\n",
    "\n",
    "  * Gaussian elimination, $O(2/3n^3)$ flops, general matrix\n",
    "\n",
    "  * LU decomposition, upper triangular and lower tridiagonal matrices, $O(2/3n^3)$ flops, general matrix. Get easily the inverse, determinant and can solve linear equations with back-substitution only, $O(n^2)$ flops\n",
    "\n",
    "  * Cholesky decomposition. Real symmetric or hermitian positive definite matrix, $O(1/3n^3)$ flops.\n",
    "\n",
    "  * Tridiagonal linear systems, important for differential equations. Normally positive definite and non-singular. $O(8n)$ flops for symmetric. Special case of banded matrices.\n",
    "\n",
    "  * Singular value decomposition\n",
    "\n",
    "  * the QR method will be discussed in chapter 7 in connection with eigenvalue systems. $O(4/3n^3)$ flops.\n",
    "\n",
    "## LU Decomposition\n",
    "\n",
    "The LU decomposition method means that we can rewrite\n",
    "this matrix as the product of two matrices $\\mathbf{L}$ and $\\mathbf{U}$\n",
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "                          a_{11} & a_{12} & a_{13} & a_{14} \\\\\n",
    "                          a_{21} & a_{22} & a_{23} & a_{24} \\\\\n",
    "                          a_{31} & a_{32} & a_{33} & a_{34} \\\\\n",
    "                          a_{41} & a_{42} & a_{43} & a_{44}\n",
    "                      \\end{bmatrix}\n",
    "                      = \\begin{bmatrix}\n",
    "                              1  & 0      & 0      & 0 \\\\\n",
    "                          l_{21} & 1      & 0      & 0 \\\\\n",
    "                          l_{31} & l_{32} & 1      & 0 \\\\\n",
    "                          l_{41} & l_{42} & l_{43} & 1\n",
    "                      \\end{bmatrix}\n",
    "                        \\begin{bmatrix}\n",
    "                          u_{11} & u_{12} & u_{13} & u_{14} \\\\\n",
    "                               0 & u_{22} & u_{23} & u_{24} \\\\\n",
    "                               0 & 0      & u_{33} & u_{34} \\\\\n",
    "                               0 & 0      &  0     & u_{44}\n",
    "             \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition\n",
    "\n",
    "LU decomposition forms the backbone of other algorithms in linear algebra, such as the\n",
    "solution of linear equations given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=w_1 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=w_2 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=w_3 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=w_4.  \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above set of equations is conveniently solved by using LU decomposition as an intermediate step.\n",
    "\n",
    "The matrix $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ has an LU factorization if the determinant\n",
    "is different from zero. If the LU factorization exists and $\\mathbf{A}$ is non-singular, then the LU factorization\n",
    "is unique and the determinant is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "det\\{\\mathbf{A}\\}=det\\{\\mathbf{LU}\\}= det\\{\\mathbf{L}\\}det\\{\\mathbf{U}\\}=u_{11}u_{22}\\dots u_{nn}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition, why?\n",
    "\n",
    "There are at least three main advantages with LU decomposition compared with standard Gaussian elimination:\n",
    "\n",
    "  * It is straightforward to compute the determinant of a matrix\n",
    "\n",
    "  * If we have to solve sets of linear equations with the same matrix but with different vectors $\\mathbf{y}$, the number of FLOPS is of the order $n^3$.\n",
    "\n",
    "  * The inverse is such an operation \n",
    "\n",
    "## LU Decomposition, linear equations\n",
    "\n",
    "With the LU decomposition it is rather\n",
    "simple to solve a system of linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=w_1 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=w_2 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=w_3 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=w_4. \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be written in matrix form as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Ax}=\\mathbf{w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{A}$ and $\\mathbf{w}$ are known and we have to solve for\n",
    "$\\mathbf{x}$. Using the LU dcomposition we write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A} \\mathbf{x} \\equiv \\mathbf{L} \\mathbf{U} \\mathbf{x} =\\mathbf{w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition, linear equations\n",
    "\n",
    "The previous equation can be calculated in two steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{L} \\mathbf{y} = \\mathbf{w};\\qquad \\mathbf{Ux}=\\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that this is correct we use to the LU decomposition\n",
    "to rewrite our system of linear equations as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{LUx}=\\mathbf{w},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and since the determinat of $\\mathbf{L}$ is equal to 1 (by construction\n",
    "since the diagonals of $\\mathbf{L}$ equal 1) we can use the inverse of\n",
    "$\\mathbf{L}$ to obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Ux}=\\mathbf{L^{-1}w}=\\mathbf{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which yields the intermediate step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{L^{-1}w}=\\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as soon as we have $\\mathbf{y}$ we can obtain $\\mathbf{x}$\n",
    "through $\\mathbf{Ux}=\\mathbf{y}$.\n",
    "\n",
    "\n",
    "## LU Decomposition, why?\n",
    "\n",
    "For our four-dimentional example this takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_1=w_1 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "l_{21}y_1 + y_2=w_2\\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "l_{31}y_1 + l_{32}y_2 + y_3 =w_3\\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "l_{41}y_1 + l_{42}y_2 + l_{43}y_3 + y_4=w_4. \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u_{11}x_1 +u_{12}x_2 +u_{13}x_3 + u_{14}x_4=y_1 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u_{22}x_2 + u_{23}x_3 + u_{24}x_4=y_2\\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u_{33}x_3 + u_{34}x_4=y_3\\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u_{44}x_4=y_4  \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows the basis for the algorithm\n",
    "needed to solve the set of $n$ linear equations.\n",
    "\n",
    "\n",
    "## LU Decomposition, linear equations\n",
    "\n",
    "The algorithm goes as follows\n",
    "\n",
    "  * Set up the matrix $\\bf A$ and the vector $\\bf w$ with their correct dimensions. This determines the dimensionality of the unknown vector $\\bf x$.\n",
    "\n",
    "  * Then LU decompose the matrix $\\bf A$ through a call to the function `ludcmp(double a, int n, int indx, double &d)`. This functions returns the LU decomposed matrix $\\bf A$, its determinant and the vector indx which keeps track of the number of interchanges of rows. If the determinant is zero, the solution is malconditioned.\n",
    "\n",
    "  * Thereafter you call the function  `lubksb(double a, int n, int indx, double w)` which uses the LU decomposed matrix $\\bf A$ and the vector $\\bf w$ and returns $\\bf x$ in the same place as $\\bf w$. Upon exit the original content in $\\bf w$ is destroyed. If you wish to keep this information, you should make a backup of it in your calling function.\n",
    "\n",
    "## LU Decomposition, the inverse of a matrix\n",
    "\n",
    "If the inverse exists then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the identity matrix. With an LU decomposed matrix we can rewrite the last equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{LU}\\mathbf{A}^{-1}=\\mathbf{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition, the inverse of a matrix\n",
    "\n",
    "If we assume that the first column (that is column 1) of the inverse matrix\n",
    "can be written as a vector with unknown entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}_1^{-1}= \\begin{bmatrix}\n",
    "                              a_{11}^{-1} \\\\\n",
    "                              a_{21}^{-1} \\\\\n",
    "                              \\dots \\\\\n",
    "                              a_{n1}^{-1} \\\\\n",
    "                    \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we have a linear set of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{LU}\\begin{bmatrix}\n",
    "                              a_{11}^{-1} \\\\\n",
    "                              a_{21}^{-1} \\\\\n",
    "                              \\dots \\\\\n",
    "                              a_{n1}^{-1} \\\\\n",
    "                    \\end{bmatrix} =\\begin{bmatrix}\n",
    "                               1 \\\\\n",
    "                              0 \\\\\n",
    "                              \\dots \\\\\n",
    "                              0 \\\\\n",
    "                    \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition, the inverse\n",
    "\n",
    "In a similar way we can compute the unknow entries of the second column,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{LU}\\begin{bmatrix}\n",
    "                              a_{12}^{-1} \\\\\n",
    "                              a_{22}^{-1} \\\\\n",
    "                              \\dots \\\\\n",
    "                              a_{n2}^{-1} \\\\\n",
    "                    \\end{bmatrix}=\\begin{bmatrix}\n",
    "                                0 \\\\\n",
    "                              1 \\\\\n",
    "                              \\dots \\\\\n",
    "                              0 \\\\\n",
    "                    \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and continue till we have solved all $n$ sets of linear equations.\n",
    "\n",
    "\n",
    "\n",
    "## [Using Armadillo to perform an LU decomposition](https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/CppQtCodesLectures/MatrixTest/main.cpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        #include <iostream>\n",
    "        #include \"armadillo\"\n",
    "        using namespace arma;\n",
    "        using namespace std;\n",
    "        \n",
    "        int main()\n",
    "          {\n",
    "           mat A = randu<mat>(5,5);\n",
    "           vec b = randu<vec>(5);\n",
    "        \n",
    "          A.print(\"A =\");\n",
    "          b.print(\"b=\");\n",
    "          // solve Ax = b\n",
    "          vec x = solve(A,b);\n",
    "          // print x\n",
    "          x.print(\"x=\");\n",
    "          // find LU decomp of A, if needed, P is the permutation matrix\n",
    "          mat L, U;\n",
    "          lu(L,U,A);\n",
    "          // print l\n",
    "          L.print(\" L= \");\n",
    "          // print U\n",
    "          U.print(\" U= \");\n",
    "          //Check that A = LU\n",
    "          (A-L*U).print(\"Test of LU decomposition\");\n",
    "            return 0;\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods, Chapter 6\n",
    "\n",
    " * Direct solvers such as Gauss elimination and  LU decomposition discussed in connection with project 1.\n",
    "\n",
    " * Iterative solvers such as Basic iterative solvers,  Jacobi,  Gauss-Seidel, Successive over-relaxation. These methods are easy to parallelize, as we will se later. Much used in solutions of partial differential equations.\n",
    "\n",
    " * Other iterative methods such as Krylov subspace methods with Generalized minimum residual (GMRES) and Conjugate gradient etc will not be discussed.\n",
    "\n",
    "## Iterative methods, Jacobi's method\n",
    "\n",
    "It is a simple method for solving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf{x}=\\mathbf{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{A}$ is a matrix and $\\mathbf{x}$ and $\\mathbf{b}$ are vectors. The vector $\\mathbf{x}$ is \n",
    "the unknown.\n",
    "\n",
    "It is an iterative scheme where we start with a guess for the unknown, and \n",
    "after $k+1$ iterations we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}^{(k+1)}= \\mathbf{D}^{-1}(\\mathbf{b}-(\\mathbf{L}+\\mathbf{U})\\mathbf{x}^{(k)}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\mathbf{A}=\\mathbf{D}+\\mathbf{U}+\\mathbf{L}$ and\n",
    "$\\mathbf{D}$ being a diagonal matrix, $\\mathbf{U}$ an upper triangular matrix and $\\mathbf{L}$ a  lower triangular\n",
    "matrix.\n",
    "\n",
    "If the matrix $\\mathbf{A}$ is positive definite or diagonally dominant, one can show that this method will always converge to the exact solution. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iterative methods, Jacobi's method\n",
    "\n",
    "We can demonstrate Jacobi's method by this $4\\times 4$ matrix problem. We assume a guess\n",
    "for the vector elements $x_i^{(0)}$, a guess which represents our first iteration. The new\n",
    "values are obtained by substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_1^{(1)} =(b_1-a_{12}x_2^{(0)} -a_{13}x_3^{(0)} - a_{14}x_4^{(0)})/a_{11} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_2^{(1)} =(b_2-a_{21}x_1^{(0)} - a_{23}x_3^{(0)} - a_{24}x_4^{(0)})/a_{22} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_3^{(1)} =(b_3- a_{31}x_1^{(0)} -a_{32}x_2^{(0)} -a_{34}x_4^{(0)})/a_{33} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_4^{(1)}=(b_4-a_{41}x_1^{(0)} -a_{42}x_2^{(0)} - a_{43}x_3^{(0)})/a_{44},  \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which after $k+1$ iterations reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_1^{(k+1)} =(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_2^{(k+1)} =(b_2-a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_3^{(k+1)} =(b_3- a_{31}x_1^{(k)} -a_{32}x_2^{(k)} -a_{34}x_4^{(k)})/a_{33} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_4^{(k+1)}=(b_4-a_{41}x_1^{(k)} -a_{42}x_2^{(k)} - a_{43}x_3^{(k)})/a_{44},  \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods, Jacobi's method\n",
    "\n",
    "We can generalize the above equations to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_i^{(k+1)}=(b_i-\\sum_{j=1, j\\ne i}^{n}a_{ij}x_j^{(k)})/a_{ii}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in an even more compact form as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}^{(k+1)}= \\mathbf{D}^{-1}(\\mathbf{b}-(\\mathbf{L}+\\mathbf{U})\\mathbf{x}^{(k)}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\mathbf{A}=\\mathbf{D}+\\mathbf{U}+\\mathbf{L}$ and\n",
    "$\\mathbf{D}$ being a diagonal matrix, $\\mathbf{U}$ an upper triangular matrix and $\\mathbf{L}$ a  lower triangular\n",
    "matrix.\n",
    "\n",
    "\n",
    "\n",
    "## Iterative methods, Gauss-Seidel's method\n",
    "\n",
    "Our $4\\times 4$ matrix problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_1^{(k+1)} =(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_2^{(k+1)} =(b_2-a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_3^{(k+1)} =(b_3- a_{31}x_1^{(k)} -a_{32}x_2^{(k)} -a_{34}x_4^{(k)})/a_{33} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_4^{(k+1)}=(b_4-a_{41}x_1^{(k)} -a_{42}x_2^{(k)} - a_{43}x_3^{(k)})/a_{44},  \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_1^{(k+1)} =(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_2^{(k+1)} =(b_2-a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_3^{(k+1)} =(b_3- a_{31}x_1^{(k+1)} -a_{32}x_2^{(k+1)} -a_{34}x_4^{(k)})/a_{33} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_4^{(k+1)}=(b_4-a_{41}x_1^{(k+1)} -a_{42}x_2^{(k+1)} - a_{43}x_3^{(k+1)})/a_{44},  \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which allows us to utilize the preceding solution (forward substitution). This improves normally the convergence\n",
    "behavior and leads to the Gauss-Seidel method!\n",
    "\n",
    "\n",
    "\n",
    "## Iterative methods, Gauss-Seidel's method\n",
    "\n",
    "We can generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_1^{(k+1)} =(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_2^{(k+1)} =(b_2-a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_3^{(k+1)} =(b_3- a_{31}x_1^{(k+1)} -a_{32}x_2^{(k+1)} -a_{34}x_4^{(k)})/a_{33} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_4^{(k+1)}=(b_4-a_{41}x_1^{(k+1)} -a_{42}x_2^{(k+1)} - a_{43}x_3^{(k+1)})/a_{44},  \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to the following form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{(k+1)}_i = \\frac{1}{a_{ii}} \\left(b_i - \\sum_{j > i}a_{ij}x^{(k)}_j - \\sum_{j < i}a_{ij}x^{(k+1)}_j \\right),\\quad i=1,2,\\ldots,n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure is generally continued until the changes made by an iteration are below some tolerance.\n",
    "\n",
    "The convergence properties of the Jacobi method and the \n",
    "Gauss-Seidel method are dependent on the matrix $\\mathbf{A}$. These methods converge when\n",
    "the matrix is symmetric positive-definite, or is strictly or irreducibly diagonally dominant.\n",
    "Both methods sometimes converge even if these conditions are not satisfied.\n",
    "\n",
    "\n",
    "\n",
    "## Iterative methods, Successive over-relaxation\n",
    "\n",
    "Given a square system of n linear equations with unknown $\\mathbf x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf x = \\mathbf b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\a_{n1} & a_{n2} & \\cdots & a_{nn} \\end{bmatrix}, \\qquad \\mathbf{x} = \\begin{bmatrix} x_{1} \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} , \\qquad \\mathbf{b} = \\begin{bmatrix} b_{1} \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods, Successive over-relaxation\n",
    "\n",
    "Then A can be decomposed into a diagonal component D, and strictly lower and upper triangular components L and U:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A} =\\mathbf{D} + \\mathbf{L} + \\mathbf{U},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D = \\begin{bmatrix} a_{11} & 0 & \\cdots & 0 \\\\ 0 & a_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\0 & 0 & \\cdots & a_{nn} \\end{bmatrix}, \\quad L = \\begin{bmatrix} 0 & 0 & \\cdots & 0 \\\\ a_{21} & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\a_{n1} & a_{n2} & \\cdots & 0 \\end{bmatrix}, \\quad U = \\begin{bmatrix} 0 & a_{12} & \\cdots & a_{1n} \\\\ 0 & 0 & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\0 & 0 & \\cdots & 0 \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system of linear equations may be rewritten as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(D+\\omega L) \\mathbf{x} = \\omega \\mathbf{b} - [\\omega U + (\\omega-1) D ] \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a constant $\\omega > 1$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iterative methods, Successive over-relaxation\n",
    "\n",
    "The method of successive over-relaxation is an iterative technique that solves the left hand side of this expression for $x$, using previous value for $x$ on the right hand side. Analytically, this may be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}^{(k+1)} = (D+\\omega L)^{-1} \\big(\\omega \\mathbf{b} - [\\omega U + (\\omega-1) D ] \\mathbf{x}^{(k)}\\big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by taking advantage of the triangular form of $(D+\\omega L)$, the elements of $x^{(k+1)}$ can be computed sequentially using forward substitution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{(k+1)}_i = (1-\\omega)x^{(k)}_i + \\frac{\\omega}{a_{ii}} \\left(b_i - \\sum_{j > i} a_{ij}x^{(k)}_j - \\sum_{j < i} a_{ij}x^{(k+1)}_j \\right),\\quad i=1,2,\\ldots,n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of relaxation factor is not necessarily easy, and depends upon the properties of the coefficient matrix. For symmetric, positive-definite matrices it can be proven that $0 < \\omega < 2$ will lead to convergence, but we are generally interested in faster convergence rather than just convergence.\n",
    "\n",
    "\n",
    "\n",
    "# Cubic Splines\n",
    "\n",
    "Cubic spline interpolation is among one of the most used \n",
    "methods for interpolating between data points where the arguments\n",
    "are organized as ascending series. In the library program we supply\n",
    "such a function, based on the so-called cubic spline method to be \n",
    "described below. \n",
    "\n",
    "A spline function consists of polynomial pieces defined on\n",
    "subintervals. The different subintervals are connected via\n",
    "various continuity relations.\n",
    "\n",
    "Assume we have at our disposal $n+1$ points $x_0, x_1, \\dots x_n$ \n",
    "arranged so that $x_0 < x_1 < x_2 < \\dots x_{n-1} < x_n$ (such points are called\n",
    "knots). A spline function $s$ of degree $k$ with $n+1$ knots is defined\n",
    "as follows\n",
    " * On every subinterval $[x_{i-1},x_i)$ *s* is a polynomial of degree $\\le k$.\n",
    "\n",
    " * $s$ has $k-1$ continuous derivatives in the whole interval $[x_0,x_n]$.\n",
    "\n",
    "## Splines\n",
    "\n",
    "As an example, consider a spline function of degree $k=1$ defined as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s(x)=\\begin{bmatrix} s_0(x)=a_0x+b_0 & x\\in [x_0, x_1) \\\\   \n",
    "                             s_1(x)=a_1x+b_1 & x\\in [x_1, x_2) \\\\   \n",
    "                             \\dots & \\dots \\\\\n",
    "                             s_{n-1}(x)=a_{n-1}x+b_{n-1} & x\\in \n",
    "                             [x_{n-1}, x_n] \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the polynomial consists of series of straight lines \n",
    "connected to each other at every endpoint. The number of continuous\n",
    "derivatives is then $k-1=0$, as expected when we deal with straight lines.\n",
    "Such a polynomial is quite easy to construct given\n",
    "$n+1$ points $x_0, x_1, \\dots x_n$ and their corresponding \n",
    "function values. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Splines\n",
    "\n",
    "The most commonly used spline function is the one with $k=3$, the so-called\n",
    "cubic spline function. \n",
    "Assume that we have in adddition to the $n+1$ knots a series of\n",
    "functions values $y_0=f(x_0), y_1=f(x_1), \\dots y_n=f(x_n)$.\n",
    "By definition, the polynomials $s_{i-1}$ and $s_i$ \n",
    "are thence supposed to interpolate the same point $i$, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s_{i-1}(x_i)= y_i = s_i(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $1 \\le i \\le n-1$. In total we have $n$ polynomials of the \n",
    "type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s_i(x)=a_{i0}+a_{i1}x+a_{i2}x^2+a_{i2}x^3,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yielding $4n$ coefficients to determine.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Splines\n",
    "\n",
    "Every subinterval provides in addition the $2n$ conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_i = s(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s(x_{i+1})= y_{i+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be fulfilled. If we also assume that $s'$ and $s''$ are continuous,\n",
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s'_{i-1}(x_i)= s'_i(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yields $n-1$ conditions. Similarly,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s''_{i-1}(x_i)= s''_i(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results in additional $n-1$ conditions. In total we have $4n$ coefficients\n",
    "and $4n-2$ equations to determine them, leaving us with $2$ degrees of \n",
    "freedom to be determined. \n",
    "\n",
    "\n",
    "\n",
    "## Splines\n",
    "\n",
    "Using the last equation we define two values for the second derivative, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s''_{i}(x_i)= f_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s''_{i}(x_{i+1})= f_{i+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and setting up a straight line between $f_i$ and $f_{i+1}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s_i''(x) = \\frac{f_i}{x_{i+1}-x_i}(x_{i+1}-x)+\n",
    "               \\frac{f_{i+1}}{x_{i+1}-x_i}(x-x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and integrating twice one obtains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s_i(x) = \\frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+\n",
    "               \\frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3\n",
    "             +c(x-x_i)+d(x_{i+1}-x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splines\n",
    "\n",
    "Using the conditions $s_i(x_i)=y_i$ and $s_i(x_{i+1})=y_{i+1}$ \n",
    "we can in turn determine the constants $c$ and $d$ resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s_i(x) =\\frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+\n",
    "               \\frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto8\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "            +(\\frac{y_{i+1}}{x_{i+1}-x_i}-\\frac{f_{i+1}(x_{i+1}-x_i)}{6})\n",
    "              (x-x_i)+\n",
    "             (\\frac{y_{i}}{x_{i+1}-x_i}-\\frac{f_{i}(x_{i+1}-x_i)}{6})\n",
    "             (x_{i+1}-x).\n",
    "\\label{_auto8} \\tag{9}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splines\n",
    "\n",
    "How to determine the values of the second\n",
    "derivatives $f_{i}$ and $f_{i+1}$? We use the continuity assumption \n",
    "of the first derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s'_{i-1}(x_i)= s'_i(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and set $x=x_i$. Defining $h_i=x_{i+1}-x_i$ we obtain finally\n",
    "the following expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{i-1}f_{i-1}+2(h_{i}+h_{i-1})f_i+h_if_{i+1}=\n",
    "   \\frac{6}{h_i}(y_{i+1}-y_i)-\\frac{6}{h_{i-1}}(y_{i}-y_{i-1}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and introducing the shorthands $u_i=2(h_{i}+h_{i-1})$, \n",
    "$v_i=\\frac{6}{h_i}(y_{i+1}-y_i)-\\frac{6}{h_{i-1}}(y_{i}-y_{i-1})$,\n",
    "we can reformulate the problem as a set of linear equations to be \n",
    "solved  through e.g., Gaussian elemination\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Splines\n",
    "\n",
    "Gaussian elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix} u_1 & h_1 &0 &\\dots & & & & \\\\\n",
    "                                 h_1 & u_2 & h_2 &0 &\\dots & & & \\\\\n",
    "                                  0   & h_2 & u_3 & h_3 &0 &\\dots & & \\\\\n",
    "                               \\dots& & \\dots &\\dots &\\dots &\\dots &\\dots & \\\\\n",
    "                                 &\\dots & & &0 &h_{n-3} &u_{n-2} &h_{n-2} \\\\\n",
    "                                 & && & &0 &h_{n-2} &u_{n-1} \\end{bmatrix}\n",
    "   \\begin{bmatrix} f_1 \\\\ \n",
    "                          f_2 \\\\\n",
    "                          f_3\\\\\n",
    "                          \\dots \\\\\n",
    "                          f_{n-2} \\\\ \n",
    "                          f_{n-1} \\end{bmatrix} =\n",
    "   \\begin{bmatrix} v_1 \\\\ \n",
    "                          v_2 \\\\\n",
    "                          v_3\\\\\n",
    "                          \\dots \\\\\n",
    "                          v_{n-2}\\\\\n",
    "                          v_{n-1} \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a set of tridiagonal equations and can be solved \n",
    "through only $O(n)$ operations.\n",
    "\n",
    "\n",
    "\n",
    "## Splines\n",
    "\n",
    "The functions supplied in the program library are *spline* and *splint*.\n",
    "In order to use cubic spline interpolation you need first to call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        spline(double x[], double y[], int n, double yp1,  double yp2, double y2[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as\n",
    "input $x[0,..,n - 1]$ and $y[0,..,n - 1]$ containing a tabulation\n",
    "$y_i = f(x_i)$ with $x_0 < x_1 < .. < x_{n - 1}$ \n",
    "together with the \n",
    "first derivatives  of $f(x)$ at $x_0$ and $x_{n-1}$, respectively. Then the\n",
    "function returns $y2[0,..,n-1]$ which contains the second derivatives of\n",
    "$f(x_i)$ at each point $x_i$. $n$ is the number of points.\n",
    "This function provides the cubic spline interpolation for all subintervals\n",
    "and is called only once.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Splines\n",
    "\n",
    "Thereafter, if you wish to make  various interpolations, you need to call the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        splint(double x[], double y[], double y2a[], int n, double x, double *y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which takes as input\n",
    "the tabulated values $x[0,..,n - 1]$ and $y[0,..,n - 1]$ and the output \n",
    "y2a[0,..,n - 1] from *spline*. It returns the value $y$ corresponding\n",
    "to the point $x$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient (CG) method\n",
    "\n",
    "The success of the CG method  for finding solutions of non-linear problems is based\n",
    "on the theory of conjugate gradients for linear systems of equations. It belongs\n",
    "to the class of iterative methods for solving problems from linear algebra of the type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A}\\hat{x} = \\hat{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the iterative process we end up with a problem like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}= \\hat{b}-\\hat{A}\\hat{x},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{r}$ is the so-called residual or error in the iterative process.\n",
    "\n",
    "When we have found the exact solution, $\\hat{r}=0$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "\n",
    "\n",
    "The residual is zero when we reach the minimum of the quadratic equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\hat{x})=\\frac{1}{2}\\hat{x}^T\\hat{A}\\hat{x} - \\hat{x}^T\\hat{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the constraint that the matrix $\\hat{A}$ is positive definite and symmetric.\n",
    "If we search for a minimum of the quantum mechanical  variance, then the matrix \n",
    "$\\hat{A}$, which is called the Hessian, is given by the second-derivative of the function we want to minimize.  This quantity is always positive definite.  In our case this corresponds normally to the second derivative of the energy.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method, Newton's method first\n",
    "\n",
    "We seek the minimum of the energy or the variance as function of various variational parameters. \n",
    "In our case we have thus a function $f$ whose minimum we are seeking.\n",
    "In Newton's method we set $\\nabla f = 0$ and we can thus compute the next iteration point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}-\\hat{x}_i=\\hat{A}^{-1}\\nabla f(\\hat{x}_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting this equation from that of $\\hat{x}_{i+1}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_{i+1}-\\hat{x}_i=\\hat{A}^{-1}(\\nabla f(\\hat{x}_{i+1})-\\nabla f(\\hat{x}_i)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example and demonstration\n",
    "\n",
    "The function $f$ can be either the energy or the variance.  If we choose the energy then we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\alpha}_{i+1}-\\hat{\\alpha}_i=\\hat{A}^{-1}(\\nabla E(\\hat{\\alpha}_{i+1})-\\nabla E(\\hat{\\alpha}_i)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simple harmonic oscillator model, the gradient and the Hessian $\\hat{A}$ are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d\\langle  E_L[\\alpha]\\rangle}{d\\alpha} = \\alpha-\\frac{1}{4\\alpha^3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a second derivative which is always positive (meaning that we find a minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A}= \\frac{d^2\\langle  E_L[\\alpha]\\rangle}{d\\alpha^2} = 1+\\frac{3}{4\\alpha^4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example and demonstration\n",
    "\n",
    "We get then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha_{i+1}=\\frac{4}{3}\\alpha_i-\\frac{\\alpha_i^4}{3\\alpha_{i+1}^3},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha_{i+1}^4-\\frac{4}{3}\\alpha_i\\alpha_{i+1}^4+\\frac{1}{3}\\alpha_i^4.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method\n",
    "\n",
    "In the CG method we define so-called conjugate directions and two vectors \n",
    "$\\hat{s}$ and $\\hat{t}$\n",
    "are said to be\n",
    "conjugate if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{s}^T\\hat{A}\\hat{t}= 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The philosophy of the CG method is to perform searches in various conjugate directions\n",
    "of our vectors $\\hat{x}_i$ obeying the above criterion, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_i^T\\hat{A}\\hat{x}_j= 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two vectors are conjugate if they are orthogonal with respect to \n",
    "this inner product. Being conjugate is a symmetric relation: if $\\hat{s}$ is conjugate to $\\hat{t}$, then $\\hat{t}$ is conjugate to $\\hat{s}$.\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "\n",
    "An example is given by the eigenvectors of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{v}_i^T\\hat{A}\\hat{v}_j= \\lambda\\hat{v}_i^T\\hat{v}_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is zero unless $i=j$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "\n",
    "Assume now that we have a symmetric positive-definite matrix $\\hat{A}$ of size\n",
    "$n\\times n$. At each iteration $i+1$ we obtain the conjugate direction of a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_{i+1}=\\hat{x}_{i}+\\alpha_i\\hat{p}_{i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that $\\hat{p}_{i}$ is a sequence of $n$ mutually conjugate directions. \n",
    "Then the $\\hat{p}_{i}$  form a basis of $R^n$ and we can expand the solution \n",
    "$  \\hat{A}\\hat{x} = \\hat{b}$ in this basis, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}  = \\sum^{n}_{i=1} \\alpha_i \\hat{p}_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method\n",
    "\n",
    "The coefficients are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf{x} = \\sum^{n}_{i=1} \\alpha_i \\mathbf{A} \\mathbf{p}_i = \\mathbf{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying with $\\hat{p}_k^T$  from the left gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}_k^T \\hat{A}\\hat{x} = \\sum^{n}_{i=1} \\alpha_i\\hat{p}_k^T \\hat{A}\\hat{p}_i= \\hat{p}_k^T \\hat{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can define the coefficients $\\alpha_k$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha_k = \\frac{\\hat{p}_k^T \\hat{b}}{\\hat{p}_k^T \\hat{A} \\hat{p}_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method and iterations\n",
    "\n",
    "\n",
    "If we choose the conjugate vectors $\\hat{p}_k$ carefully, \n",
    "then we may not need all of them to obtain a good approximation to the solution \n",
    "$\\hat{x}$. \n",
    "We want to regard the conjugate gradient method as an iterative method. \n",
    "This will us to solve systems where $n$ is so large that the direct \n",
    "method would take too much time.\n",
    "\n",
    "We denote the initial guess for $\\hat{x}$ as $\\hat{x}_0$. \n",
    "We can assume without loss of generality that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_0=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or consider the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A}\\hat{z} = \\hat{b}-\\hat{A}\\hat{x}_0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "\n",
    "One can show that the solution $\\hat{x}$ is also the unique minimizer of the quadratic form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\hat{x}) = \\frac{1}{2}\\hat{x}^T\\hat{A}\\hat{x} - \\hat{x}^T \\hat{x} , \\quad \\hat{x}\\in\\mathbf{R}^n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests taking the first basis vector $\\hat{p}_1$ \n",
    "to be the gradient of $f$ at $\\hat{x}=\\hat{x}_0$, \n",
    "which equals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A}\\hat{x}_0-\\hat{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and \n",
    "$\\hat{x}_0=0$ it is equal $-\\hat{b}$.\n",
    "The other vectors in the basis will be conjugate to the gradient, \n",
    "hence the name conjugate gradient method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjugate gradient method\n",
    "\n",
    "Let  $\\hat{r}_k$ be the residual at the $k$-th step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}_k=\\hat{b}-\\hat{A}\\hat{x}_k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\hat{r}_k$ is the negative gradient of $f$ at \n",
    "$\\hat{x}=\\hat{x}_k$, \n",
    "so the gradient descent method would be to move in the direction $\\hat{r}_k$. \n",
    "Here, we insist that the directions $\\hat{p}_k$ are conjugate to each other, \n",
    "so we take the direction closest to the gradient $\\hat{r}_k$  \n",
    "under the conjugacy constraint. \n",
    "This gives the following expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}_{k+1}=\\hat{r}_k-\\frac{\\hat{p}_k^T \\hat{A}\\hat{r}_k}{\\hat{p}_k^T\\hat{A}\\hat{p}_k} \\hat{p}_k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method\n",
    "\n",
    "We can also  compute the residual iteratively as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}_{k+1}=\\hat{b}-\\hat{A}\\hat{x}_{k+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which equals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{b}-\\hat{A}(\\hat{x}_k+\\alpha_k\\hat{p}_k),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\hat{b}-\\hat{A}\\hat{x}_k)-\\alpha_k\\hat{A}\\hat{p}_k,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}_{k+1}=\\hat{r}_k-\\hat{A}\\hat{p}_{k},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of probability theory\n",
    "\n",
    "## Domains and probabilities\n",
    "\n",
    "Consider the following simple example, namely the tossing of a dice, resulting in  the following possible values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\{2,3,4,5,6,7,8,9,10,11,12\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are called the *domain*. \n",
    "To this domain we have the corresponding *probabilities*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\{1/36,2/36/3/36,4/36,5/36,6/36,5/36,4/36,3/36,2/36,1/36\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tossing a dice\n",
    "\n",
    "The numbers in the domain are the outcomes of the physical process tossing the dice.\n",
    "We cannot tell beforehand whether the outcome is 3 or 5 or any other number in this domain.\n",
    "This defines the randomness of the outcome, or unexpectedness or any other synonimous word which\n",
    "encompasses the uncertitude of the final outcome. \n",
    "\n",
    "The only thing we can tell beforehand\n",
    "is that say the outcome 2 has a certain probability.  \n",
    "If our favorite hobby is to  spend an hour every evening throwing dice and \n",
    "registering the sequence of outcomes, we will note that the numbers in the above domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\{2,3,4,5,6,7,8,9,10,11,12\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "appear in a random order. After 11 throws the results may look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\{10,8,6,3,6,9,11,8,12,4,5\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic variables\n",
    "\n",
    "\n",
    "**Random variables are characterized by a domain which contains all possible values that the random value may take. This domain has a corresponding PDF**.\n",
    "\n",
    "\n",
    "\n",
    "## Stochastic variables and the main concepts, the discrete case\n",
    "\n",
    "There are two main concepts associated with a stochastic variable. The\n",
    "*domain* is the set $\\mathbb D = \\{x\\}$ of all accessible values\n",
    "the variable can assume, so that $X \\in \\mathbb D$. An example of a\n",
    "discrete domain is the set of six different numbers that we may get by\n",
    "throwing of a dice, $x\\in\\{1,\\,2,\\,3,\\,4,\\,5,\\,6\\}$.\n",
    "\n",
    "The *probability distribution function (PDF)* is a function\n",
    "$p(x)$ on the domain which, in the discrete case, gives us the\n",
    "probability or relative frequency with which these values of $X$\n",
    "occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\mathrm{Prob}(X=x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic variables and the main concepts, the continuous case\n",
    "\n",
    "In the continuous case, the PDF does not directly depict the\n",
    "actual probability. Instead we define the probability for the\n",
    "stochastic variable to assume any value on an infinitesimal interval\n",
    "around $x$ to be $p(x)dx$. The continuous function $p(x)$ then gives us\n",
    "the *density* of the probability rather than the probability\n",
    "itself. The probability for a stochastic variable to assume any value\n",
    "on a non-infinitesimal interval $[a,\\,b]$ is then just the integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{Prob}(a\\leq X\\leq b) = \\int_a^b p(x)dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitatively speaking, a stochastic variable represents the values of\n",
    "numbers chosen as if by chance from some specified PDF so that the\n",
    "selection of a large set of these numbers reproduces this PDF.\n",
    "\n",
    "\n",
    "\n",
    "## The cumulative probability\n",
    "\n",
    "Of interest to us is the *cumulative probability\n",
    "distribution function* (**CDF**), $P(x)$, which is just the probability\n",
    "for a stochastic variable $X$ to assume any value less than $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(x)=\\mathrm{Prob(}X\\leq x\\mathrm{)} =\n",
    "\\int_{-\\infty}^x p(x^{\\prime})dx^{\\prime}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation between a CDF and its corresponding PDF is then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\frac{d}{dx}P(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of PDFs\n",
    "\n",
    "\n",
    "There are two properties that all PDFs must satisfy. The first one is\n",
    "positivity (assuming that the PDF is normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "0 \\leq p(x) \\leq 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, it would be nonsensical for any of the values of the domain\n",
    "to occur with a probability greater than $1$ or less than $0$. Also,\n",
    "the PDF must be normalized. That is, all the probabilities must add up\n",
    "to unity.  The probability of \"anything\" to happen is always unity. For\n",
    "both discrete and continuous PDFs, this condition is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{x_i\\in\\mathbb D} p(x_i) & =  1,\\\\\n",
    "\\int_{x\\in\\mathbb D} p(x)\\,dx & =  1.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important distributions, the uniform distribution\n",
    "\n",
    "The first one\n",
    "is the most basic PDF; namely the uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:unifromPDF\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(x) = \\frac{1}{b-a}\\theta(x-a)\\theta(b-x),\n",
    "\\label{eq:unifromPDF} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\theta(x)=0 & x<0 \\\\\n",
    "\\theta(x)=\\frac{1}{b-a} & \\in [a,b].\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution with $b=1$ and $a=0$ is used to generate random numbers. \n",
    "\n",
    "\n",
    "\n",
    "## Gaussian distribution\n",
    "\n",
    "The second one is the Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp{(-\\frac{(x-\\mu)^2}{2\\sigma^2})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with mean value $\\mu$ and standard deviation $\\sigma$. If $\\mu=0$ and $\\sigma=1$, it is normally called the **standard normal distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp{(-\\frac{x^2}{2})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following simple Python code plots the above distribution for different values of $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEhCAYAAAB7mQezAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQRAEIQgRBkEQBCEIEQZBEAQhiP8PJl0WhHyBYiMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10876d668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import acos, exp, sqrt\n",
    "from  matplotlib import pyplot as plt\n",
    "from matplotlib import rc, rcParams\n",
    "import matplotlib.units as units\n",
    "import matplotlib.ticker as ticker\n",
    "rc('text',usetex=True)\n",
    "rc('font',**{'family':'serif','serif':['Gaussian distribution']})\n",
    "font = {'family' : 'serif',\n",
    "        'color'  : 'darkred',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16,\n",
    "        }\n",
    "pi = acos(-1.0)\n",
    "mu0 = 0.0\n",
    "sigma0 = 1.0\n",
    "mu1= 1.0\n",
    "sigma1 = 2.0\n",
    "mu2 = 2.0\n",
    "sigma2 = 4.0\n",
    "\n",
    "x = np.linspace(-20.0, 20.0)\n",
    "v0 = np.exp(-(x*x-2*x*mu0+mu0*mu0)/(2*sigma0*sigma0))/sqrt(2*pi*sigma0*sigma0)\n",
    "v1 = np.exp(-(x*x-2*x*mu1+mu1*mu1)/(2*sigma1*sigma1))/sqrt(2*pi*sigma1*sigma1)\n",
    "v2 = np.exp(-(x*x-2*x*mu2+mu2*mu2)/(2*sigma2*sigma2))/sqrt(2*pi*sigma2*sigma2)\n",
    "plt.plot(x, v0, 'b-', x, v1, 'r-', x, v2, 'g-')\n",
    "plt.title(r'{\\bf Gaussian distributions}', fontsize=20)\n",
    "plt.text(-19, 0.3, r'Parameters: $\\mu = 0$, $\\sigma = 1$', fontdict=font)\n",
    "plt.text(-19, 0.18, r'Parameters: $\\mu = 1$, $\\sigma = 2$', fontdict=font)\n",
    "plt.text(-19, 0.08, r'Parameters: $\\mu = 2$, $\\sigma = 4$', fontdict=font)\n",
    "plt.xlabel(r'$x$',fontsize=20)\n",
    "plt.ylabel(r'$p(x)$ [MeV]',fontsize=20)\n",
    "\n",
    "# Tweak spacing to prevent clipping of ylabel                                                                       \n",
    "plt.subplots_adjust(left=0.15)\n",
    "plt.savefig('gaussian.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential distribution\n",
    "\n",
    "Another important distribution in science is the exponential distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\alpha\\exp{-(\\alpha x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation values\n",
    "\n",
    "Let $h(x)$ be an arbitrary continuous function on the domain of the stochastic\n",
    "variable $X$ whose PDF is $p(x)$. We define the *expectation value*\n",
    "of $h$ with respect to $p$ as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:expectation_value_of_h_wrt_p\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\langle h \\rangle_X \\equiv \\int\\! h(x)p(x)\\,dx\n",
    "\\label{eq:expectation_value_of_h_wrt_p} \\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever the PDF is known implicitly, like in this case, we will drop\n",
    "the index $X$ for clarity.  \n",
    "A particularly useful class of special expectation values are the\n",
    "*moments*. The $n$-th moment of the PDF $p$ is defined as\n",
    "follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x^n \\rangle \\equiv \\int\\! x^n p(x)\\,dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic variables and the main concepts, mean values\n",
    "\n",
    "The zero-th moment $\\langle 1\\rangle$ is just the normalization condition of\n",
    "$p$. The first moment, $\\langle x\\rangle$, is called the *mean* of $p$\n",
    "and often denoted by the letter $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x\\rangle  = \\mu \\equiv \\int x p(x)dx,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a continuous distribution and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x\\rangle  = \\mu \\equiv \\frac{1}{N}\\sum_{i=1}^N x_i p(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a discrete distribution. \n",
    "Qualitatively it represents the centroid or the average value of the\n",
    "PDF and is therefore simply called the expectation value of $p(x)$.\n",
    "\n",
    "\n",
    "\n",
    "## Stochastic variables and the main concepts, central moments, the variance\n",
    "\n",
    "\n",
    "A special version of the moments is the set of *central moments*, the n-th central moment defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle (x-\\langle x\\rangle )^n\\rangle  \\equiv \\int\\! (x-\\langle x\\rangle)^n p(x)\\,dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero-th and first central moments are both trivial, equal $1$ and\n",
    "$0$, respectively. But the second central moment, known as the\n",
    "*variance* of $p$, is of particular interest. For the stochastic\n",
    "variable $X$, the variance is denoted as $\\sigma^2_X$ or $\\mathrm{Var}(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma^2_X &=\\mathrm{Var}(X) =  \\langle (x-\\langle x\\rangle)^2\\rangle  =\n",
    "\\int (x-\\langle x\\rangle)^2 p(x)dx\\\\\n",
    "& =  \\int\\left(x^2 - 2 x \\langle x\\rangle^{2} +\\langle x\\rangle^2\\right)p(x)dx\\\\\n",
    "& =  \\langle x^2\\rangle\\rangle - 2 \\langle x\\rangle\\langle x\\rangle + \\langle x\\rangle^2\\\\\n",
    "& =  \\langle x^2 \\rangle - \\langle x\\rangle^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square root of the variance, $\\sigma =\\sqrt{\\langle (x-\\langle x\\rangle)^2\\rangle}$ is called the \n",
    "**standard deviation** of $p$. It is the RMS (root-mean-square)\n",
    "value of the deviation of the PDF from its mean value, interpreted\n",
    "qualitatively as the \"spread\" of $p$ around its mean.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Probability Distribution Functions\n",
    "\n",
    "\n",
    "The following table collects properties of probability distribution functions.\n",
    "In our notation we reserve the label $p(x)$ for the probability of a certain event,\n",
    "while $P(x)$ is the cumulative probability. \n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">             </th> <th align=\"center\">               Discrete PDF               </th> <th align=\"center\">          Continuous PDF          </th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"left\">   Domain           </td> <td align=\"center\">   $\\left\\{x_1, x_2, x_3, \\dots, x_N\\right\\}$    </td> <td align=\"center\">   $[a,b]$                               </td> </tr>\n",
    "<tr><td align=\"left\">   Probability      </td> <td align=\"center\">   $p(x_i)$                                      </td> <td align=\"center\">   $p(x)dx$                              </td> </tr>\n",
    "<tr><td align=\"left\">   Cumulative       </td> <td align=\"center\">   $P_i=\\sum_{l=1}^ip(x_l)$                      </td> <td align=\"center\">   $P(x)=\\int_a^xp(t)dt$                 </td> </tr>\n",
    "<tr><td align=\"left\">   Positivity       </td> <td align=\"center\">   $ 0\\le p(x_i)\\le 1$                           </td> <td align=\"center\">   $ p(x) \\ge 0$                         </td> </tr>\n",
    "<tr><td align=\"left\">   Positivity       </td> <td align=\"center\">   $ 0\\le P_i\\le 1$                              </td> <td align=\"center\">   $ 0\\le P(x)\\le 1$                     </td> </tr>\n",
    "<tr><td align=\"left\">   Monotonic        </td> <td align=\"center\">   $P_i\\ge P_j$ if $x_i\\ge x_j$                  </td> <td align=\"center\">   $P(x_i)\\ge P(x_j)$ if $x_i\\ge x_j$    </td> </tr>\n",
    "<tr><td align=\"left\">   Normalization    </td> <td align=\"center\">   $P_N=1$                                       </td> <td align=\"center\">   $P(b)=1$                              </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Probability Distribution Functions\n",
    "\n",
    "With a PDF we can compute expectation values of selected quantities such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x^k\\rangle=\\frac{1}{N}\\sum_{i=1}^{N}x_i^kp(x_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we have a discrete PDF or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x^k\\rangle=\\int_a^b x^kp(x)dx,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the case of a continuous PDF. We have already defined the mean value $\\mu$\n",
    "and the variance $\\sigma^2$. \n",
    "\n",
    "\n",
    "\n",
    "## The three famous Probability Distribution Functions\n",
    "\n",
    "\n",
    "There are at least three PDFs which one may encounter. These are the\n",
    "\n",
    "**Uniform distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x)=\\frac{1}{b-a}\\Theta(x-a)\\Theta(b-x),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yielding probabilities different from zero in the interval $[a,b]$.\n",
    "\n",
    "**The exponential distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x)=\\alpha \\exp{(-\\alpha x)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yielding probabilities different from zero in the interval $[0,\\infty)$ and with mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu = \\int_0^{\\infty}xp(x)dx=\\int_0^{\\infty}x\\alpha \\exp{(-\\alpha x)}dx=\\frac{1}{\\alpha},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2=\\int_0^{\\infty}x^2p(x)dx-\\mu^2 = \\frac{1}{\\alpha^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distribution Functions, the normal distribution\n",
    "\n",
    "Finally, we have the so-called univariate normal  distribution, or just the **normal distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x)=\\frac{1}{b\\sqrt{2\\pi}}\\exp{\\left(-\\frac{(x-a)^2}{2b^2}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with probabilities different from zero in the interval $(-\\infty,\\infty)$.\n",
    "The integral $\\int_{-\\infty}^{\\infty}\\exp{\\left(-(x^2\\right)}dx$ appears in many calculations, its value\n",
    "is $\\sqrt{\\pi}$,  a result we will need when we compute the mean value and the variance.\n",
    "The mean value is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu = \\int_0^{\\infty}xp(x)dx=\\frac{1}{b\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x \\exp{\\left(-\\frac{(x-a)^2}{2b^2}\\right)}dx,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which becomes with a suitable change of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu =\\frac{1}{b\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}b\\sqrt{2}(a+b\\sqrt{2}y)\\exp{-y^2}dy=a.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distribution Functions, the normal distribution\n",
    "\n",
    "Similarly, the variance becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2 = \\frac{1}{b\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}(x-\\mu)^2 \\exp{\\left(-\\frac{(x-a)^2}{2b^2}\\right)}dx,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and inserting the mean value and performing a variable change we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2 = \\frac{1}{b\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}b\\sqrt{2}(b\\sqrt{2}y)^2\\exp{\\left(-y^2\\right)}dy=\n",
    "\\frac{2b^2}{\\sqrt{\\pi}}\\int_{-\\infty}^{\\infty}y^2\\exp{\\left(-y^2\\right)}dy,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and performing a final integration by parts we obtain the well-known result $\\sigma^2=b^2$.\n",
    "It is useful to introduce the standard normal distribution as well, defined by $\\mu=a=0$, viz. a distribution\n",
    "centered around zero and with a variance $\\sigma^2=1$, leading to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto9\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   p(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\frac{x^2}{2}\\right)}.\n",
    "\\label{_auto9} \\tag{12}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distribution Functions, the cumulative distribution\n",
    "\n",
    "\n",
    "The exponential and uniform distributions have simple cumulative functions,\n",
    "whereas the normal distribution does not, being proportional to the so-called\n",
    "error function $erf(x)$, given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x\\exp{\\left(-\\frac{t^2}{2}\\right)}dt,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is difficult to evaluate in a quick way. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Probability Distribution Functions, other important distribution\n",
    "\n",
    "\n",
    "Some other PDFs which one encounters often in the natural sciences are the binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\left(\\begin{array}{c} n \\\\ x\\end{array}\\right)y^x(1-y)^{n-x} \\hspace{0.5cm}x=0,1,\\dots,n,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y$ is the probability for a specific event, such as the tossing of a coin or moving left or right\n",
    "in case of a random walker. Note that $x$ is a discrete stochastic variable. \n",
    "\n",
    "The sequence of binomial trials is characterized by the following definitions\n",
    "\n",
    "  * Every experiment is thought to consist of $N$ independent trials.\n",
    "\n",
    "  * In every independent trial one registers if a specific situation happens or not, such as the  jump to the left or right of a random walker.\n",
    "\n",
    "  * The probability for every outcome in a single trial has the same value, for example the outcome of tossing (either heads or tails) a coin is always $1/2$.\n",
    "\n",
    "## Probability Distribution Functions, the binomial distribution\n",
    "\n",
    "\n",
    "In order to compute the mean and variance we need to recall Newton's binomial\n",
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(a+b)^m=\\sum_{n=0}^m \\left(\\begin{array}{c} m \\\\ n\\end{array}\\right)a^nb^{m-n},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be used to show that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{x=0}^n\\left(\\begin{array}{c} n \\\\ x\\end{array}\\right)y^x(1-y)^{n-x} = (y+1-y)^n = 1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the PDF is normalized to one. \n",
    "The mean value is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu = \\sum_{x=0}^n x\\left(\\begin{array}{c} n \\\\ x\\end{array}\\right)y^x(1-y)^{n-x} =\n",
    "\\sum_{x=0}^n x\\frac{n!}{x!(n-x)!}y^x(1-y)^{n-x},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu = \n",
    "\\sum_{x=0}^n x\\frac{(n-1)!}{(x-1)!(n-1-(x-1))!}y^{x-1}(1-y)^{n-1-(x-1)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we rewrite as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu=ny\\sum_{\\nu=0}^n\\left(\\begin{array}{c} n-1 \\\\ \\nu\\end{array}\\right)y^{\\nu}(1-y)^{n-1-\\nu} =ny(y+1-y)^{n-1}=ny.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance is slightly trickier to get. It reads $\\sigma^2=ny(1-y)$. \n",
    "\n",
    "\n",
    "## Probability Distribution Functions, Poisson's  distribution\n",
    "\n",
    "\n",
    "Another important distribution with discrete stochastic variables $x$ is  \n",
    "the Poisson model, which resembles the exponential distribution and reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\frac{\\lambda^x}{x!} e^{-\\lambda} \\hspace{0.5cm}x=0,1,\\dots,;\\lambda > 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case both the mean value and the variance are easier to calculate,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu = \\sum_{x=0}^{\\infty} x \\frac{\\lambda^x}{x!} e^{-\\lambda} = \\lambda e^{-\\lambda}\\sum_{x=1}^{\\infty}\n",
    "\\frac{\\lambda^{x-1}}{(x-1)!}=\\lambda,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the variance is $\\sigma^2=\\lambda$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Probability Distribution Functions, Poisson's  distribution\n",
    "\n",
    "An example of applications of the Poisson distribution could be the counting\n",
    "of the number of $\\alpha$-particles emitted from a radioactive source in a given time interval.\n",
    "In the limit of $n\\rightarrow \\infty$ and for small probabilities $y$, the binomial distribution\n",
    "approaches the Poisson distribution. Setting $\\lambda = ny$, with $y$ the probability for an event in\n",
    "the binomial distribution we can show that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lim_{n\\rightarrow \\infty}\\left(\\begin{array}{c} n \\\\ x\\end{array}\\right)y^x(1-y)^{n-x} e^{-\\lambda}=\\sum_{x=1}^{\\infty}\\frac{\\lambda^x}{x!} e^{-\\lambda}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet the  covariance!\n",
    "\n",
    "An important quantity in a statistical analysis is the so-called covariance. \n",
    "\n",
    "Consider the set $\\{X_i\\}$ of $n$\n",
    "stochastic variables (not necessarily uncorrelated) with the\n",
    "multivariate PDF $P(x_1,\\dots,x_n)$. The *covariance* of two\n",
    "of the stochastic variables, $X_i$ and $X_j$, is defined as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto10\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{Cov}(X_i,\\,X_j)  = \\langle (x_i-\\langle x_i\\rangle)(x_j-\\langle x_j\\rangle)\\rangle \n",
    "\\label{_auto10} \\tag{13}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:def_covariance\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "=\\int\\cdots\\int (x_i-\\langle x_i\\rangle)(x_j-\\langle x_j\\rangle)P(x_1,\\dots,x_n)\\,dx_1\\dots dx_n,\n",
    "\\label{eq:def_covariance} \\tag{14}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x_i\\rangle =\n",
    "\\int\\cdots\\int x_i P(x_1,\\dots,x_n)\\,dx_1\\dots dx_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet the  covariance in matrix disguise\n",
    "\n",
    "If we consider the above covariance as a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C_{ij} =\\mathrm{Cov}(X_i,\\,X_j),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the diagonal elements are just the familiar\n",
    "variances, $C_{ii} = \\mathrm{Cov}(X_i,\\,X_i) = \\mathrm{Var}(X_i)$. It turns out that\n",
    "all the off-diagonal elements are zero if the stochastic variables are\n",
    "uncorrelated. \n",
    "\n",
    "\n",
    "\n",
    "## Meet the  covariance, uncorrelated events\n",
    "\n",
    "\n",
    "This is easy to show, keeping in mind the linearity of\n",
    "the expectation value. Consider the stochastic variables $X_i$ and\n",
    "$X_j$, ($i\\neq j$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Cov}(X_i,\\,X_j) &= \\langle (x_i-\\langle x_i\\rangle)(x_j-\\langle x_j\\rangle)\\rangle\\\\\n",
    "&=\\langle x_i x_j - x_i\\langle x_j\\rangle - \\langle x_i\\rangle x_j + \\langle x_i\\rangle\\langle x_j\\rangle\\rangle\\\\\n",
    "&=\\langle x_i x_j\\rangle - \\langle x_i\\langle x_j\\rangle\\rangle - \\langle \\langle x_i\\rangle x_j \\rangle +\n",
    "\\langle \\langle x_i\\rangle\\langle x_j\\rangle\\rangle\\\\\n",
    "&=\\langle x_i x_j\\rangle - \\langle x_i\\rangle\\langle x_j\\rangle - \\langle x_i\\rangle\\langle x_j\\rangle +\n",
    "\\langle x_i\\rangle\\langle x_j\\rangle\\\\\n",
    "&=\\langle x_i x_j\\rangle - \\langle x_i\\rangle\\langle x_j\\rangle\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X_i$ and $X_j$ are independent, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x_i x_j\\rangle =\n",
    "\\langle x_i\\rangle\\langle x_j\\rangle=\\mathrm{Cov}(X_i, X_j) = 0\\ \\ (i\\neq j).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical experiments and the covariance\n",
    "\n",
    "\n",
    "Now that we have constructed an idealized mathematical framework, let\n",
    "us try to apply it to empirical observations. Examples of relevant\n",
    "physical phenomena may be spontaneous decays of nuclei, or a purely\n",
    "mathematical set of numbers produced by some deterministic\n",
    "mechanism. It is the latter we will deal with, using so-called pseudo-random\n",
    "number generators.  In general our observations will contain only a limited set of\n",
    "observables. We remind the reader that\n",
    "a *stochastic process* is a process that produces sequentially a\n",
    "chain of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\{x_1, x_2,\\dots\\,x_k,\\dots\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical experiments and the covariance\n",
    "\n",
    "We will call these\n",
    "values our *measurements* and the entire set as our measured\n",
    "*sample*.  The action of measuring all the elements of a sample\n",
    "we will call a stochastic *experiment* (since, operationally,\n",
    "they are often associated with results of empirical observation of\n",
    "some physical or mathematical phenomena; precisely an experiment). We\n",
    "assume that these values are distributed according to some \n",
    "PDF $p_X^{\\phantom X}(x)$, where $X$ is just the formal symbol for the\n",
    "stochastic variable whose PDF is $p_X^{\\phantom X}(x)$. Instead of\n",
    "trying to determine the full distribution $p$ we are often only\n",
    "interested in finding the few lowest moments, like the mean\n",
    "$\\mu_X^{\\phantom X}$ and the variance $\\sigma_X^{\\phantom X}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Numerical experiments and the covariance, actual situations\n",
    "\n",
    "In practical situations however, a sample is always of finite size. Let that\n",
    "size be $n$. The expectation value of a sample $\\alpha$, the **sample mean**, is then defined as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x_{\\alpha} \\rangle \\equiv \\frac{1}{n}\\sum_{k=1}^n x_{\\alpha,k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *sample variance* is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{Var}(x) \\equiv \\frac{1}{n}\\sum_{k=1}^n (x_{\\alpha,k} - \\langle x_{\\alpha} \\rangle)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with its square root being the *standard deviation of the sample*. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Numerical experiments and the covariance, our observables\n",
    "\n",
    "You can think of the above observables as a set of quantities which define\n",
    "a given experiment. This experiment is then repeated several times, say $m$ times.\n",
    "The total average is then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:exptmean\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\langle X_m \\rangle= \\frac{1}{m}\\sum_{\\alpha=1}^mx_{\\alpha}=\\frac{1}{mn}\\sum_{\\alpha, k} x_{\\alpha,k},\n",
    "\\label{eq:exptmean} \\tag{15}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the last sums end at $m$ and $n$.\n",
    "The total variance is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2_m= \\frac{1}{mn^2}\\sum_{\\alpha=1}^m(\\langle x_{\\alpha} \\rangle-\\langle X_m \\rangle)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we rewrite as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:exptvariance\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma^2_m=\\frac{1}{m}\\sum_{\\alpha=1}^m\\sum_{kl=1}^n (x_{\\alpha,k}-\\langle X_m \\rangle)(x_{\\alpha,l}-\\langle X_m \\rangle).\n",
    "\\label{eq:exptvariance} \\tag{16}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical experiments and the covariance, the sample variance\n",
    "\n",
    "\n",
    "We define also the sample variance $\\sigma^2$ of all $mn$ individual experiments as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:sampleexptvariance\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma^2=\\frac{1}{mn}\\sum_{\\alpha=1}^m\\sum_{k=1}^n (x_{\\alpha,k}-\\langle X_m \\rangle)^2.\n",
    "\\label{eq:sampleexptvariance} \\tag{17}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These quantities, being known experimental values or the results from our calculations, \n",
    "may differ, in some cases\n",
    "significantly,  from the similarly named\n",
    "exact values for the mean value $\\mu_X$, the variance $\\mathrm{Var}(X)$\n",
    "and the covariance $\\mathrm{Cov}(X,Y)$. \n",
    "\n",
    "\n",
    "\n",
    "## Numerical experiments and the covariance, central limit theorem\n",
    "\n",
    "\n",
    "The central limit theorem states that the PDF $\\tilde{p}(z)$ of\n",
    "the average of $m$ random values corresponding to a PDF $p(x)$ \n",
    "is a normal distribution whose mean is the \n",
    "mean value of the PDF $p(x)$ and whose variance is the variance\n",
    "of the PDF $p(x)$ divided by $m$, the number of values used to compute $z$.\n",
    "\n",
    "The central limit theorem leads then to the well-known expression for the\n",
    "standard deviation, given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma_m=\n",
    "\\frac{\\sigma}{\\sqrt{m}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases the above estimate for the standard deviation, in particular if correlations are strong, may be too simplistic.  We need therefore a more precise defintion of the error and the variance in our results.\n",
    "\n",
    "\n",
    "\n",
    "## Definition of Correlation Functions and Standard Deviation\n",
    "\n",
    "Our estimate of the true average $\\mu_{X}$ is the sample mean $\\langle X_m \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu_{X}^{\\phantom X} \\approx X_m=\\frac{1}{mn}\\sum_{\\alpha=1}^m\\sum_{k=1}^n x_{\\alpha,k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use Eq. ([eq:exptvariance](#eq:exptvariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2_m=\\frac{1}{mn^2}\\sum_{\\alpha=1}^m\\sum_{kl=1}^n (x_{\\alpha,k}-\\langle X_m \\rangle)(x_{\\alpha,l}-\\langle X_m \\rangle),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and rewrite it as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2_m=\\frac{\\sigma^2}{n}+\\frac{2}{mn^2}\\sum_{\\alpha=1}^m\\sum_{k<l}^n (x_{\\alpha,k}-\\langle X_m \\rangle)(x_{\\alpha,l}-\\langle X_m \\rangle),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the first term is the sample variance of all $mn$ experiments divided by $n$\n",
    "and the last term is nothing but the covariance which arises when $k\\ne l$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Definition of Correlation Functions and Standard Deviation\n",
    "\n",
    "Our estimate of the true average $\\mu_{X}$ is the sample mean $\\langle X_m \\rangle$\n",
    "\n",
    "If the \n",
    "observables are uncorrelated, then the covariance is zero and we obtain a total variance\n",
    "which agrees with the central limit theorem. Correlations may often be present in our data set, resulting in a non-zero covariance.  The first term is normally called the uncorrelated \n",
    "contribution.\n",
    "Computationally the uncorrelated first term is much easier to treat\n",
    "efficiently than the second.\n",
    "We just accumulate separately the values $x^2$ and $x$ for every\n",
    "measurement $x$ we receive. The correlation term, though, has to be\n",
    "calculated at the end of the experiment since we need all the\n",
    "measurements to calculate the cross terms. Therefore, all measurements\n",
    "have to be stored throughout the experiment.\n",
    "\n",
    "\n",
    "\n",
    "## Definition of Correlation Functions and Standard Deviation\n",
    "\n",
    "\n",
    "Let us analyze the problem by splitting up the correlation term into\n",
    "partial sums of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_d = \\frac{1}{nm}\\sum_{\\alpha=1}^m\\sum_{k=1}^{n-d}(x_{\\alpha,k}-\\langle X_m \\rangle)(x_{\\alpha,k+d}-\\langle X_m \\rangle),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation term of the total variance can now be rewritten in terms of\n",
    "$f_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{2}{mn^2}\\sum_{\\alpha=1}^m\\sum_{k<l}^n (x_{\\alpha,k}-\\langle X_m \\rangle)(x_{\\alpha,l}-\\langle X_m \\rangle)=\n",
    "\\frac{2}{n}\\sum_{d=1}^{n-1} f_d\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Correlation Functions and Standard Deviation\n",
    "\n",
    "The value of $f_d$ reflects the correlation between measurements\n",
    "separated by the distance $d$ in the samples.  Notice that for\n",
    "$d=0$, $f$ is just the sample variance, $\\sigma^2$. If we divide $f_d$\n",
    "by $\\sigma^2$, we arrive at the so called **autocorrelation function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:autocorrelformal\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\kappa_d = \\frac{f_d}{\\sigma^2}\n",
    "\\label{eq:autocorrelformal} \\tag{18}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives us a useful measure of the correlation pair correlation\n",
    "starting always at $1$ for $d=0$.\n",
    "\n",
    "\n",
    "\n",
    "## Definition of Correlation Functions and Standard Deviation, sample variance\n",
    "\n",
    "\n",
    "The sample variance of the $mn$ experiments can now be\n",
    "written in terms of the autocorrelation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:error_estimate_corr_time\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_m^2=\\frac{\\sigma^2}{n}+\\frac{2}{n}\\cdot\\sigma^2\\sum_{d=1}^{n-1}\n",
    "\\frac{f_d}{\\sigma^2}=\\left(1+2\\sum_{d=1}^{n-1}\\kappa_d\\right)\\frac{1}{n}\\sigma^2=\\frac{\\tau}{n}\\cdot\\sigma^2\n",
    "\\label{eq:error_estimate_corr_time} \\tag{19}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we see that $\\sigma_m$ can be expressed in terms of the\n",
    "uncorrelated sample variance times a correction factor $\\tau$ which\n",
    "accounts for the correlation between measurements. We call this\n",
    "correction factor the *autocorrelation time*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:autocorrelation_time\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tau = 1+2\\sum_{d=1}^{n-1}\\kappa_d\n",
    "\\label{eq:autocorrelation_time} \\tag{20}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- It is closely related to the area under the graph of the -->\n",
    "<!-- autocorrelation function. -->\n",
    "For a correlation free experiment, $\\tau$\n",
    "equals 1. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Definition of Correlation Functions and Standard Deviation\n",
    "\n",
    "From the point of view of\n",
    "Eq. ([eq:error_estimate_corr_time](#eq:error_estimate_corr_time)) we can interpret a sequential\n",
    "correlation as an effective reduction of the number of measurements by\n",
    "a factor $\\tau$. The effective number of measurements becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "n_\\mathrm{eff} = \\frac{n}{\\tau}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To neglect the autocorrelation time $\\tau$ will always cause our\n",
    "simple uncorrelated estimate of $\\sigma_m^2\\approx \\sigma^2/n$ to\n",
    "be less than the true sample error. The estimate of the error will be\n",
    "too \"good\". On the other hand, the calculation of the full\n",
    "autocorrelation time poses an efficiency problem if the set of\n",
    "measurements is very large.  The solution to this problem is given by \n",
    "more practically oriented methods like the blocking technique.\n",
    "<!-- add ref here to flybjerg -->\n",
    "\n",
    "\n",
    "\n",
    "# Random Numbers\n",
    "\n",
    "\n",
    "Uniform deviates are just random numbers that lie within a specified range\n",
    "(typically 0 to 1), with any one number in the range just as likely as any other. They\n",
    "are, in other words, what you probably think random numbers are. However,\n",
    "we want to distinguish uniform deviates from other sorts of random numbers, for\n",
    "example numbers drawn from a normal (Gaussian) distribution of specified mean\n",
    "and standard deviation. These other sorts of deviates are almost always generated by\n",
    "performing appropriate operations on one or more uniform deviates, as we will see\n",
    "in subsequent sections. So, a reliable source of random uniform deviates, the subject\n",
    "of this section, is an essential building block for any sort of stochastic modeling\n",
    "or Monte Carlo computer work.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Random Numbers, better name: pseudo random numbers\n",
    "\n",
    "\n",
    "A disclaimer is however appropriate. It should be fairly obvious that \n",
    "something as deterministic as a computer cannot generate purely random numbers.\n",
    "\n",
    "Numbers generated by any of the standard algorithms are in reality pseudo random\n",
    "numbers, hopefully abiding to the following criteria:\n",
    "\n",
    "  * they produce a uniform distribution in the interval [0,1].\n",
    "\n",
    "  * correlations between random numbers are negligible\n",
    "\n",
    "  * the period before the same sequence of random numbers is repeated   is as large as possible and finally\n",
    "\n",
    "  * the algorithm should be fast.\n",
    "\n",
    "# Random number generator RNG\n",
    "\n",
    " The most common random number generators are based on so-called\n",
    "Linear congruential relations of the type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_i=(aN_{i-1}+c) \\mathrm{MOD} (M),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which yield a number in the interval [0,1] through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_i=N_i/M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number \n",
    "$M$ is called the period and it should be as large as possible \n",
    " and \n",
    "$N_0$ is the starting value, or seed. The function $\\mathrm{MOD}$ means the remainder,\n",
    "that is if we were to evaluate $(13)\\mathrm{MOD}(9)$, the outcome is the remainder\n",
    "of the division $13/9$, namely $4$.\n",
    "\n",
    "\n",
    "\n",
    "# Random number generator RNG and periodic outputs\n",
    "\n",
    "\n",
    "The problem with such generators is that their outputs are periodic;\n",
    "they \n",
    "will start to repeat themselves with a period that is at most $M$. If however\n",
    "the parameters $a$ and $c$ are badly chosen, the period may be even shorter.\n",
    "\n",
    "Consider the following example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_i=(6N_{i-1}+7) \\mathrm{MOD} (5),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a seed $N_0=2$. This generator produces the sequence\n",
    "$4,1,3,0,2,4,1,3,0,2,...\\dots$, i.e., a sequence with period $5$.\n",
    "However, increasing $M$ may not guarantee a larger period as the following\n",
    "example shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_i=(27N_{i-1}+11) \\mathrm{MOD} (54),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which still, with $N_0=2$, results in $11,38,11,38,11,38,\\dots$, a period of\n",
    "just $2$.\n",
    "\n",
    "\n",
    "\n",
    "# Random number generator RNG and its period\n",
    "\n",
    "Typical periods for the random generators provided in the program library \n",
    "are of the order of $\\sim 10^9$ or larger. Other random number generators which have\n",
    "become increasingly popular are so-called shift-register generators.\n",
    "In these generators each successive number depends on many preceding\n",
    "values (rather than the last values as in the linear congruential\n",
    "generator).\n",
    "For example, you could make a shift register generator whose $l$th \n",
    "number is the sum of the $l-i$th and $l-j$th values with modulo $M$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_l=(aN_{l-i}+cN_{l-j})\\mathrm{MOD}(M).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random number generator RNG, other examples\n",
    "\n",
    "Such a generator again produces a sequence of pseudorandom numbers\n",
    "but this time with a period much larger than $M$.\n",
    "It is also possible to construct more elaborate algorithms by including\n",
    "more than two past terms in the sum of each iteration.\n",
    "One example is the generator of [Marsaglia and Zaman](http://dl.acm.org/citation.cfm?id=187154)\n",
    "which consists of two congruential relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:mz1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   N_l=(N_{l-3}-N_{l-1})\\mathrm{MOD}(2^{31}-69),\n",
    "\\label{eq:mz1} \\tag{21}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "followed by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:mz2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   N_l=(69069N_{l-1}+1013904243)\\mathrm{MOD}(2^{32}),\n",
    "\\label{eq:mz2} \\tag{22}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which according to the authors has a period larger than $2^{94}$.\n",
    "\n",
    "\n",
    "\n",
    "# Random number generator RNG, other examples\n",
    "\n",
    "Instead of  using modular addition, we could use the bitwise\n",
    "exclusive-OR ($\\oplus$) operation so that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_l=(N_{l-i})\\oplus (N_{l-j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the bitwise action of $\\oplus$ means that if $N_{l-i}=N_{l-j}$ the result is\n",
    "$0$ whereas if $N_{l-i}\\ne N_{l-j}$ the result is\n",
    "$1$. As an example, consider the case where  $N_{l-i}=6$ and $N_{l-j}=11$. The first\n",
    "one has a bit representation (using 4 bits only) which reads $0110$ whereas the \n",
    "second number is $1011$. Employing the $\\oplus$ operator yields \n",
    "$1101$, or $2^3+2^2+2^0=13$.\n",
    "\n",
    "In Fortran90, the bitwise $\\oplus$ operation is coded through the intrinsic\n",
    "function $\\mathrm{IEOR}(m,n)$ where $m$ and $n$ are the input numbers, while in $C$\n",
    "it is given by $m\\wedge n$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Random number generator RNG, RAN0\n",
    "\n",
    "\n",
    "We show here how the linear congruential algorithm can be implemented, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_i=(aN_{i-1}) \\mathrm{MOD} (M).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since $a$ and $N_{i-1}$ are integers and their multiplication \n",
    "could become greater than the standard 32 bit integer, there is a trick via \n",
    "Schrage's algorithm which approximates the multiplication\n",
    "of large integers through the factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "M=aq+r,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q=[M/a],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "r = M\\hspace{0.1cm}\\mathrm{MOD} \\hspace{0.1cm}a.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the brackets denote integer division. In the code below the numbers \n",
    "$q$ and $r$ are chosen so that $r < q$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Random number generator RNG, RAN0\n",
    "\n",
    "\n",
    "To see how this works we note first that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rntrick1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(aN_{i-1}) \\mathrm{MOD} (M)= (aN_{i-1}-[N_{i-1}/q]M)\\mathrm{MOD} (M),\n",
    "\\label{eq:rntrick1} \\tag{23}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we can add or subtract any integer multiple of $M$ from $aN_{i-1}$.\n",
    "The last term $[N_{i-1}/q]M\\mathrm{MOD}(M)$ is zero since the integer division \n",
    "$[N_{i-1}/q]$ just yields a constant which is multiplied with $M$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Random number generator RNG, RAN0\n",
    "\n",
    "We can now rewrite Eq. ([eq:rntrick1](#eq:rntrick1)) as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rntrick2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(aN_{i-1}) \\mathrm{MOD} (M)= (aN_{i-1}-[N_{i-1}/q](aq+r))\\mathrm{MOD} (M),\n",
    "\\label{eq:rntrick2} \\tag{24}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which results\n",
    "in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rntrick3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(aN_{i-1}) \\mathrm{MOD} (M)= \\left(a(N_{i-1}-[N_{i-1}/q]q)-[N_{i-1}/q]r)\\right)\\mathrm{MOD} (M),\n",
    "\\label{eq:rntrick3} \\tag{25}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yielding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rntrick4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(aN_{i-1}) \\mathrm{MOD} (M)= \\left(a(N_{i-1}\\mathrm{MOD} (q)) -[N_{i-1}/q]r)\\right)\\mathrm{MOD} (M).\n",
    "\\label{eq:rntrick4} \\tag{26}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random number generator RNG, RAN0\n",
    "\n",
    "The term $[N_{i-1}/q]r$ is always smaller or equal $N_{i-1}(r/q)$ and with $r < q$ we obtain always a \n",
    "number smaller than $N_{i-1}$, which is smaller than $M$. \n",
    "And since the number $N_{i-1}\\mathrm{MOD} (q)$ is between zero and $q-1$ then\n",
    "$a(N_{i-1}\\mathrm{MOD} (q))< aq$. Combined with our definition of $q=[M/a]$ ensures that \n",
    "this term is also smaller than $M$ meaning that both terms fit into a\n",
    "32-bit signed integer. None of these two terms can be negative, but their difference could.\n",
    "The algorithm below adds $M$ if their difference is negative.\n",
    "Note that the program uses the bitwise $\\oplus$ operator to generate\n",
    "the starting point for each generation of a random number. The period\n",
    "of $ran0$ is $\\sim 2.1\\times 10^{9}$. A special feature of this\n",
    "algorithm is that is should never be called with the initial seed \n",
    "set to $0$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Random number generator RNG, RAN0 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            /*\n",
    "             ** The function\n",
    "             **           ran0()\n",
    "             ** is an \"Minimal\" random number generator of Park and Miller\n",
    "             ** Set or reset the input value\n",
    "             ** idum to any integer value (except the unlikely value MASK)\n",
    "             ** to initialize the sequence; idum must not be altered between\n",
    "             ** calls for sucessive deviates in a sequence.\n",
    "             ** The function returns a uniform deviate between 0.0 and 1.0.\n",
    "             */\n",
    "        double ran0(long &idum)\n",
    "        {\n",
    "           const int a = 16807, m = 2147483647, q = 127773;\n",
    "           const int r = 2836, MASK = 123459876;\n",
    "           const double am = 1./m;\n",
    "           long     k;\n",
    "           double   ans;\n",
    "           idum ^= MASK;\n",
    "           k = (*idum)/q;\n",
    "           idum = a*(idum - k*q) - r*k;\n",
    "           // add m if negative difference\n",
    "           if(idum < 0) idum += m;\n",
    "           ans=am*(idum);\n",
    "           idum ^= MASK;\n",
    "           return ans;\n",
    "        } // End: function ran0() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Selected Random Number Generators\n",
    "\n",
    "\n",
    "As mentioned previously, the underlying PDF for the generation of\n",
    "random numbers is the uniform distribution, meaning that the \n",
    "probability for finding a number $x$ in the interval [0,1] is $p(x)=1$.\n",
    "\n",
    "A random number generator should produce numbers which are uniformly distributed\n",
    "in this interval. The table  shows the distribution of $N=10000$ random\n",
    "numbers generated by the functions in the program library.\n",
    "We note in this table that the number of points in the various\n",
    "intervals $0.0-0.1$, $0.1-0.2$ etc are fairly close to $1000$, with some minor\n",
    "deviations. \n",
    "\n",
    "Two additional measures are the standard deviation $\\sigma$ and the mean\n",
    "$\\mu=\\langle x\\rangle$.\n",
    "\n",
    "\n",
    "\n",
    "## Properties of Selected Random Number Generators\n",
    "\n",
    "For the uniform distribution, the mean value $\\mu$ is then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu=\\langle x\\rangle=\\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the standard deviation is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma=\\sqrt{\\langle x^2\\rangle-\\mu^2}=\\frac{1}{\\sqrt{12}}=0.2886.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Selected Random Number Generators\n",
    "\n",
    "The various random number generators produce results which agree rather well with\n",
    "these limiting values. \n",
    "\n",
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">$x$-bin </th> <th align=\"center\"> ran0 </th> <th align=\"center\"> ran1 </th> <th align=\"center\"> ran2 </th> <th align=\"center\"> ran3 </th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   0.0-0.1     </td> <td align=\"right\">   1013      </td> <td align=\"right\">   991       </td> <td align=\"right\">   938       </td> <td align=\"right\">   1047      </td> </tr>\n",
    "<tr><td align=\"center\">   0.1-0.2     </td> <td align=\"right\">   1002      </td> <td align=\"right\">   1009      </td> <td align=\"right\">   1040      </td> <td align=\"right\">   1030      </td> </tr>\n",
    "<tr><td align=\"center\">   0.2-0.3     </td> <td align=\"right\">   989       </td> <td align=\"right\">   999       </td> <td align=\"right\">   1030      </td> <td align=\"right\">   993       </td> </tr>\n",
    "<tr><td align=\"center\">   0.3-0.4     </td> <td align=\"right\">   939       </td> <td align=\"right\">   960       </td> <td align=\"right\">   1023      </td> <td align=\"right\">   937       </td> </tr>\n",
    "<tr><td align=\"center\">   0.4-0.5     </td> <td align=\"right\">   1038      </td> <td align=\"right\">   1001      </td> <td align=\"right\">   1002      </td> <td align=\"right\">   992       </td> </tr>\n",
    "<tr><td align=\"center\">   0.5-0.6     </td> <td align=\"right\">   1037      </td> <td align=\"right\">   1047      </td> <td align=\"right\">   1009      </td> <td align=\"right\">   1009      </td> </tr>\n",
    "<tr><td align=\"center\">   0.6-0.7     </td> <td align=\"right\">   1005      </td> <td align=\"right\">   989       </td> <td align=\"right\">   1003      </td> <td align=\"right\">   989       </td> </tr>\n",
    "<tr><td align=\"center\">   0.7-0.8     </td> <td align=\"right\">   986       </td> <td align=\"right\">   962       </td> <td align=\"right\">   985       </td> <td align=\"right\">   954       </td> </tr>\n",
    "<tr><td align=\"center\">   0.8-0.9     </td> <td align=\"right\">   1000      </td> <td align=\"right\">   1027      </td> <td align=\"right\">   1009      </td> <td align=\"right\">   1023      </td> </tr>\n",
    "<tr><td align=\"center\">   0.9-1.0     </td> <td align=\"right\">   991       </td> <td align=\"right\">   1015      </td> <td align=\"right\">   961       </td> <td align=\"right\">   1026      </td> </tr>\n",
    "<tr><td align=\"center\">   $\\mu$       </td> <td align=\"right\">   0.4997    </td> <td align=\"right\">   0.5018    </td> <td align=\"right\">   0.4992    </td> <td align=\"right\">   0.4990    </td> </tr>\n",
    "<tr><td align=\"center\">   $\\sigma$    </td> <td align=\"right\">   0.2882    </td> <td align=\"right\">   0.2892    </td> <td align=\"right\">   0.2861    </td> <td align=\"right\">   0.2915    </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "## Simple demonstration of RNGs using python\n",
    "\n",
    "The following simple Python code plots the distribution of the produced random numbers using the linear congruential RNG employed by Python. The trend displayed in the previous table is seen rather clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# initialize the rng with a seed\n",
    "random.seed() \n",
    "counts = 10000\n",
    "values = np.zeros(counts)   \n",
    "for i in range (1, counts, 1):\n",
    "    values[i] = random.random()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(values, 10, facecolor='green')\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('Number of counts')\n",
    "plt.title(r'Test of uniform distribution')\n",
    "plt.axis([0, 1, 0, 1100])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Selected Random Number Generators\n",
    "\n",
    "Since our random numbers, which are typically generated via a linear congruential algorithm,\n",
    "are never fully independent, we can then define \n",
    "an important test which measures the degree of correlation, namely the  so-called  \n",
    "auto-correlation function defined previously, see again Eq. ([eq:autocorrelformal](#eq:autocorrelformal)).\n",
    "We rewrite it here as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C_k=\\frac{f_d}\n",
    "             {\\sigma^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $C_0=1$. Recall that \n",
    "$\\sigma^2=\\langle x_i^2\\rangle-\\langle x_i\\rangle^2$ and that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_d = \\frac{1}{nm}\\sum_{\\alpha=1}^m\\sum_{k=1}^{n-d}(x_{\\alpha,k}-\\langle X_m \\rangle)(x_{\\alpha,k+d}-\\langle X_m \\rangle),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-vanishing of $C_k$ for $k\\ne 0$ means that the random\n",
    "numbers are not independent. The independence of the random numbers is crucial \n",
    "in the evaluation of other expectation values. If they are not independent, our\n",
    "assumption for approximating $\\sigma_N$ is no longer valid.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Correlation function and which random number generators should I use\n",
    "\n",
    "The program here computes the correlation function for one of the standard functions included with the c++ compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        //  This function computes the autocorrelation function for \n",
    "        //  the standard c++ random number generator\n",
    "        \n",
    "        #include <fstream>\n",
    "        #include <iomanip>\n",
    "        #include <iostream>\n",
    "        #include <cmath>\n",
    "        using namespace std;\n",
    "        // output file as global variable\n",
    "        ofstream ofile;  \n",
    "        \n",
    "        //     Main function begins here     \n",
    "        int main(int argc, char* argv[])\n",
    "        {\n",
    "             int n;\n",
    "             char *outfilename;\n",
    "        \n",
    "             cin >> n;\n",
    "             double MCint = 0.;      double MCintsqr2=0.;\n",
    "             double invers_period = 1./RAND_MAX; // initialise the random number generator\n",
    "             srand(time(NULL));  // This produces the so-called seed in MC jargon\n",
    "             // Compute the variance and the mean value of the uniform distribution\n",
    "             // Compute also the specific values x for each cycle in order to be able to\n",
    "             // the covariance and the correlation function  \n",
    "             // Read in output file, abort if there are too few command-line arguments\n",
    "             if( argc <= 2 ){\n",
    "               cout << \"Bad Usage: \" << argv[0] << \n",
    "        \t \" read also output file and number of cycles on same line\" << endl;\n",
    "               exit(1);\n",
    "             }\n",
    "             else{\n",
    "               outfilename=argv[1];\n",
    "             }\n",
    "             ofile.open(outfilename); \n",
    "             // Get  the number of Monte-Carlo samples\n",
    "             n = atoi(argv[2]);\n",
    "             double *X;  \n",
    "             X = new double[n];\n",
    "             for (int i = 0;  i < n; i++){\n",
    "                   double x = double(rand())*invers_period; \n",
    "                   X[i] = x;\n",
    "                   MCint += x;\n",
    "                   MCintsqr2 += x*x;\n",
    "             }\n",
    "             double Mean = MCint/((double) n );\n",
    "             MCintsqr2 = MCintsqr2/((double) n );\n",
    "             double STDev = sqrt(MCintsqr2-Mean*Mean);\n",
    "             double Variance = MCintsqr2-Mean*Mean;\n",
    "        //   Write mean value and standard deviation \n",
    "             cout << \" Standard deviation= \" << STDev << \" Integral = \" << Mean << endl;\n",
    "        \n",
    "             // Now we compute the autocorrelation function\n",
    "             double *autocor;  autocor = new double[n];\n",
    "             for (int j = 0; j < n; j++){\n",
    "               double sum = 0.0;\n",
    "               for (int k = 0; k < (n-j); k++){\n",
    "        \t sum  += (X[k]-Mean)*(X[k+j]-Mean); \n",
    "               }\n",
    "               autocor[j] = sum/Variance/((double) n );\n",
    "               ofile << setiosflags(ios::showpoint | ios::uppercase);\n",
    "               ofile << setw(15) << setprecision(8) << j;\n",
    "               ofile << setw(15) << setprecision(8) << autocor[j] << endl;\n",
    "             }\n",
    "             ofile.close();  // close output file\n",
    "             return 0;\n",
    "        }  // end of main program \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation function and which random number generators should I use\n",
    "\n",
    "The following Python code plots the results for the correlation function from the above program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from  matplotlib import pyplot as plt\n",
    "# Load in data file\n",
    "data = np.loadtxt(\"datafiles/autocor.dat\")\n",
    "# Make arrays containing x-axis and binding energies as function of A\n",
    "x = data[:,0]\n",
    "corr = data[:,1]\n",
    "plt.plot(x, corr ,'ro')\n",
    "plt.axis([0,1000,-0.2, 1.1])\n",
    "plt.xlabel(r'$d$')\n",
    "plt.ylabel(r'$C_d$')\n",
    "plt.title(r'autocorrelation function for RNG')\n",
    "plt.savefig('autocorr.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which RNG should I use?\n",
    "\n",
    "* In the library files lib.cpp and lib.h we have included four popular RNGs taken from the widely used textbook [Numerical Recipes](http://numerical.recipes/). These are called ran0, ran1, ran2 and ran3.\n",
    "\n",
    "* C++ has a class called **random**. The [random class](http://www.cplusplus.com/reference/random/) contains a large selection of RNGs and is highly recommended. Some of these RNGs have very large periods making it thereby very safe to use these RNGs in case one is performing large calculations. In particular, the [Mersenne twister random number engine](http://www.cplusplus.com/reference/random/mersenne_twister_engine/) has a period of $2^{19937}$. \n",
    "\n",
    "## How to use the Mersenne generator\n",
    "\n",
    "The following part of a c++ code (from project 4) sets up the uniform distribution for $x\\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        /*\n",
    "        \n",
    "        //  You need this \n",
    "        #include <random>\n",
    "        \n",
    "        // Initialize the seed and call the Mersienne algo\n",
    "        std::random_device rd;\n",
    "        std::mt19937_64 gen(rd());\n",
    "        // Set up the uniform distribution for x \\in [[0, 1]\n",
    "        std::uniform_real_distribution<double> RandomNumberGenerator(0.0,1.0);\n",
    "        \n",
    "        // Now use the RNG\n",
    "        int ix = (int) (RandomNumberGenerator(gen)*NSpins);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why blocking?\n",
    " Statistical analysis\n",
    "    * Monte Carlo simulations can be treated as *computer experiments*\n",
    "\n",
    "    * The results can be analysed with the same statistical tools as we would use analysing experimental data.\n",
    "\n",
    "    * As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.\n",
    "\n",
    "A very good article which explains blocking is H. Flyvbjerg and H. G. Petersen, *Error estimates on averages of correlated data*,  [Journal of Chemical Physics 91, 461-466 (1989)](http://scitation.aip.org/content/aip/journal/jcp/91/1/10.1063/1.457480).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Why blocking?\n",
    " Statistical analysis\n",
    "    * As in other experiments, Monte Carlo experiments have two classes of errors:\n",
    "\n",
    "      * Statistical errors\n",
    "\n",
    "      * Systematical errors\n",
    "\n",
    "\n",
    "    * Statistical errors can be estimated using standard tools from statistics\n",
    "\n",
    "    * Systematical errors are method specific and must be treated differently from case to case. (In VMC a common source is the step length or time step in importance sampling)\n",
    "\n",
    "## Code to demonstrate the calculation of the autocorrelation function\n",
    "The following code computes the autocorrelation function, the covariance and the standard deviation\n",
    "for standard RNG. \n",
    "The [following  file](https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/Programs/LecturePrograms/programs/Blocking/autocorrelation.cpp) gives the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        //  This function computes the autocorrelation function for \n",
    "        //  the Mersenne random number generator with a uniform distribution\n",
    "        #include <iostream>\n",
    "        #include <fstream>\n",
    "        #include <iomanip>\n",
    "        #include <cstdlib>\n",
    "        #include <random>\n",
    "        #include <armadillo>\n",
    "        #include <string>\n",
    "        #include <cmath>\n",
    "        using namespace  std;\n",
    "        using namespace arma;\n",
    "        // output file\n",
    "        ofstream ofile;\n",
    "        \n",
    "        //     Main function begins here     \n",
    "        int main(int argc, char* argv[])\n",
    "        {\n",
    "          int MonteCarloCycles;\n",
    "          string filename;\n",
    "          if (argc > 1) {\n",
    "            filename=argv[1];\n",
    "            MonteCarloCycles = atoi(argv[2]);\n",
    "            string fileout = filename;\n",
    "            string argument = to_string(MonteCarloCycles);\n",
    "            fileout.append(argument);\n",
    "            ofile.open(fileout);\n",
    "          }\n",
    "        \n",
    "          // Compute the variance and the mean value of the uniform distribution\n",
    "          // Compute also the specific values x for each cycle in order to be able to\n",
    "          // compute the covariance and the correlation function  \n",
    "        \n",
    "          vec X  = zeros<vec>(MonteCarloCycles);\n",
    "          double MCint = 0.;      double MCintsqr2=0.;\n",
    "          std::random_device rd;\n",
    "          std::mt19937_64 gen(rd());\n",
    "          // Set up the uniform distribution for x \\in [[0, 1]\n",
    "          std::uniform_real_distribution<double> RandomNumberGenerator(0.0,1.0);\n",
    "          for (int i = 0;  i < MonteCarloCycles; i++){\n",
    "            double x =   RandomNumberGenerator(gen); \n",
    "            X(i) = x;\n",
    "            MCint += x;\n",
    "            MCintsqr2 += x*x;\n",
    "          }\n",
    "          double Mean = MCint/((double) MonteCarloCycles );\n",
    "          MCintsqr2 = MCintsqr2/((double) MonteCarloCycles );\n",
    "          double STDev = sqrt(MCintsqr2-Mean*Mean);\n",
    "          double Variance = MCintsqr2-Mean*Mean;\n",
    "          //   Write mean value and variance\n",
    "          cout << \" Sample variance= \" << Variance  << \" Mean value = \" << Mean << endl;\n",
    "          // Now we compute the autocorrelation function\n",
    "          vec autocorrelation = zeros<vec>(MonteCarloCycles);\n",
    "          for (int j = 0; j < MonteCarloCycles; j++){\n",
    "            double sum = 0.0;\n",
    "            for (int k = 0; k < (MonteCarloCycles-j); k++){\n",
    "              sum  += (X(k)-Mean)*(X(k+j)-Mean); \n",
    "            }\n",
    "            autocorrelation(j) = sum/Variance/((double) MonteCarloCycles );\n",
    "            ofile << setiosflags(ios::showpoint | ios::uppercase);\n",
    "            ofile << setw(15) << setprecision(8) << j;\n",
    "            ofile << setw(15) << setprecision(8) << autocorrelation(j) << endl;\n",
    "          }\n",
    "          // Now compute the exact covariance using the autocorrelation function\n",
    "          double Covariance = 0.0;\n",
    "          for (int j = 0; j < MonteCarloCycles; j++){\n",
    "            Covariance  += autocorrelation(j);\n",
    "          }\n",
    "          Covariance *=  2.0/((double) MonteCarloCycles);\n",
    "          // Compute now the total variance, including the covariance, and obtain the standard deviation\n",
    "          double TotalVariance = (Variance/((double) MonteCarloCycles ))+Covariance;\n",
    "          cout << \"Covariance =\" << Covariance << \"Totalvariance= \" << TotalVariance << \"Sample Variance/n= \" << (Variance/((double) MonteCarloCycles )) << endl;\n",
    "          cout << \" STD from sample variance= \" << sqrt(Variance/((double) MonteCarloCycles )) << \" STD with covariance = \" << sqrt(TotalVariance) << endl;\n",
    "        \n",
    "          ofile.close();  // close output file\n",
    "          return 0;\n",
    "        }  // end of main program \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is blocking?\n",
    " Blocking\n",
    "    * Say that we have a set of samples from a Monte Carlo experiment\n",
    "\n",
    "    * Assuming (wrongly) that our samples are uncorrelated our best estimate of the standard deviation of the mean $\\langle \\mathbf{M}\\rangle$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma=\\sqrt{\\frac{1}{n}\\left(\\langle \\mathbf{M}^2\\rangle-\\langle \\mathbf{M}\\rangle^2\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the samples are correlated we can rewrite our results to show  that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma=\\sqrt{\\frac{1+2\\tau/\\Delta t}{n}\\left(\\langle \\mathbf{M}^2\\rangle-\\langle \\mathbf{M}\\rangle^2\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\tau$ is the correlation time (the time between a sample and the next uncorrelated sample) and $\\Delta t$ is time between each sample\n",
    "\n",
    "\n",
    "\n",
    "## What is blocking?\n",
    " Blocking \n",
    "    * If $\\Delta t\\gg\\tau$ our first estimate of $\\sigma$ still holds\n",
    "\n",
    "    * Much more common that $\\Delta t<\\tau$\n",
    "\n",
    "    * In the method of data blocking we divide the sequence of samples into blocks\n",
    "\n",
    "    * We then take the mean $\\langle \\mathbf{M}_i\\rangle$ of block $i=1\\ldots n_{blocks}$ to calculate the total mean and variance\n",
    "\n",
    "    * The size of each block must be so large that sample $j$ of block $i$ is not correlated with sample $j$ of block $i+1$\n",
    "\n",
    "    * The correlation time $\\tau$ would be a good choice\n",
    "\n",
    "## What is blocking?\n",
    " Blocking\n",
    "    * Problem: We don't know $\\tau$ or it is too expensive to compute\n",
    "\n",
    "    * Solution: Make a plot of std. dev. as a function of blocksize\n",
    "\n",
    "    * The estimate of std. dev. of correlated data is too low $\\to$ the error will increase with increasing block size until the blocks are uncorrelated, where we reach a plateau\n",
    "\n",
    "    * When the std. dev. stops increasing the blocks are uncorrelated\n",
    "\n",
    "## Implementation\n",
    "\n",
    "    * Do a Monte Carlo simulation, storing all samples to file\n",
    "\n",
    "    * Do the statistical analysis on this file, independently of your Monte Carlo program\n",
    "\n",
    "    * Read the file into an array\n",
    "\n",
    "    * Loop over various block sizes\n",
    "\n",
    "    * For each block size $n_b$, loop over the array in steps of $n_b$ taking the mean of elements $i n_b,\\ldots,(i+1) n_b$\n",
    "\n",
    "    * Take the mean and variance of the resulting array\n",
    "\n",
    "    * Write the results for each block size to file for later\n",
    "      analysis\n",
    "\n",
    "## Actual implementation with code, main function\n",
    "When the file gets large, it can be useful to write your data in binary mode instead of ascii characters.\n",
    "The [following python file](https://github.com/CompPhysics/MachineLearning/blob/master/doc/Programs/Sampling/analysis.py)   reads data from file with the output from every Monte Carlo cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Blocking\n",
    "    @timeFunction\n",
    "    def blocking(self, blockSizeMax = 500):\n",
    "        blockSizeMin = 1\n",
    "\n",
    "        self.blockSizes = []\n",
    "        self.meanVec = []\n",
    "        self.varVec = []\n",
    "\n",
    "        for i in range(blockSizeMin, blockSizeMax):\n",
    "            if(len(self.data) % i != 0):\n",
    "                pass#continue\n",
    "            blockSize = i\n",
    "            meanTempVec = []\n",
    "            varTempVec = []\n",
    "            startPoint = 0\n",
    "            endPoint = blockSize\n",
    "\n",
    "            while endPoint <= len(self.data):\n",
    "                meanTempVec.append(np.average(self.data[startPoint:endPoint]))\n",
    "                startPoint = endPoint\n",
    "                endPoint += blockSize\n",
    "            mean, var = np.average(meanTempVec), np.var(meanTempVec)/len(meanTempVec)\n",
    "            self.meanVec.append(mean)\n",
    "            self.varVec.append(var)\n",
    "            self.blockSizes.append(blockSize)\n",
    "\n",
    "        self.blockingAvg = np.average(self.meanVec[-200:])\n",
    "        self.blockingVar = (np.average(self.varVec[-200:]))\n",
    "        self.blockingStd = np.sqrt(self.blockingVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bootstrap method\n",
    "\n",
    "The Bootstrap  resampling method is also very popular. It is very simple:\n",
    "\n",
    "1. Start with your sample of measurements and compute the sample variance and the mean values\n",
    "\n",
    "2. Then start again but pick in a random way the numbers in the sample and recalculate the mean and the sample variance.\n",
    "\n",
    "3. Repeat this $K$ times.\n",
    "\n",
    "It can be shown, see the article by [Efron](https://projecteuclid.org/download/pdf_1/euclid.aos/1176344552)\n",
    "that it produces the correct standard deviation.\n",
    "\n",
    "This method is very useful for small ensembles of data points.  \n",
    "\n",
    "\n",
    "## Bootstrapping\n",
    "Given a set of $N$ data, assume that we are interested in some \n",
    "observable $\\theta$ which may be estimated from that set. This observable can also be for example the result of a fit based on all $N$ raw data. \n",
    "Let us call the value of the observable obtained from the original \n",
    "data set $\\hat{\\theta}$. One recreates from the sample repeatedly \n",
    "other samples by choosing randomly $N$ data out of the original set. \n",
    "This costs essentially nothing, since we just recycle the original data set for the building of new sets. \n",
    "\n",
    "\n",
    "## Bootstrapping, recipe\n",
    "Let us assume we have done this $K$ times and thus have $K$ sets of $N$ \n",
    "data values each. \n",
    "Of course some values will enter more than once in the new sets. For each of these sets one computes the observable $\\theta$ resulting in values $\\theta_k$ with $k = 1,...,K$. Then one determines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{\\theta} = \\frac{1}{K} \\sum_{k=1}^K \\theta_k,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "sigma^2_{\\tilde{\\theta}} = \\frac{1}{K} \\sum_{k=1}^K \\left(\\theta_k-\\tilde{\\theta}\\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are estimators for $\\angle\\theta\\rangle$ and its variance. They are not unbiased and therefore \n",
    "$\\tilde{\\theta}\\neq\\hat{\\theta}$  for finite K. \n",
    "\n",
    "The difference is called bias and gives an idea on how far away the result may be from \n",
    "the true $\\angle\\theta\\rangle$. As final result for the observable one quotes $\\angle\\theta\\rangle = \\tilde{\\theta} \\pm \\sigma_{\\tilde{\\theta}}$ .\n",
    "\n",
    "\n",
    "\n",
    "## Bootstrapping, [code](https://github.com/CompPhysics/MachineLearning/blob/master/doc/Programs/Sampling/analysis.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        # Bootstrap\n",
    "            @timeFunction\n",
    "            def bootstrap(self, nBoots = 1000):\n",
    "                bootVec = np.zeros(nBoots)\n",
    "                for k in range(0,nBoots):\n",
    "                    bootVec[k] = np.average(np.random.choice(self.data, len(self.data)))\n",
    "                self.bootAvg = np.average(bootVec)\n",
    "                self.bootVar = np.var(bootVec)\n",
    "                self.bootStd = np.std(bootVec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife, [code](https://github.com/CompPhysics/MachineLearning/blob/master/doc/Programs/Sampling/analysis.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        # Jackknife\n",
    "            @timeFunction\n",
    "            def jackknife(self):\n",
    "                jackknVec = np.zeros(len(self.data))\n",
    "                for k in range(0,len(self.data)):\n",
    "                    jackknVec[k] = np.average(np.delete(self.data, k))\n",
    "                self.jackknAvg = self.avg - (len(self.data) - 1) * (np.average(jackknVec) - self.avg)\n",
    "                self.jackknVar = float(len(self.data) - 1) * np.var(jackknVec)\n",
    "                self.jackknStd = np.sqrt(self.jackknVar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression analysis, overarching aims\n",
    "\n",
    "\n",
    "Regression modeling deals with the description of  the sampling distribution of a given random variable $y$ varies as function of another variable or a set of such variables $\\hat{x} =[x_0, x_1,\\dots, x_p]^T$. \n",
    "The first variable is called the **dependent**, the **outcome** or the **response** variable while the set of variables $\\hat{x}$ is called the independent variable, or the predictor variable or the explanatory variable. \n",
    "\n",
    "A regression model aims at finding a likelihood function $p(y\\vert \\hat{x})$, that is the conditional distribution for $y$ with a given $\\hat{x}$. The estimation of  $p(y\\vert \\hat{x})$ is made using a data set with \n",
    "* $n$ cases $i = 0, 1, 2, \\dots, n-1$ \n",
    "\n",
    "* Response (dependent or outcome) variable $y_i$ with $i = 0, 1, 2, \\dots, n-1$ \n",
    "\n",
    "* $p$ Explanatory (independent or predictor) variables $\\hat{x}_i=[x_{i0}, x_{i1}, \\dots, x_{ip}]$ with $i = 0, 1, 2, \\dots, n-1$   \n",
    "\n",
    " The goal of the regression analysis is to extract/exploit relationship between $y_i$ and $\\hat{x}_i$ in or to infer causal dependencies, approximations to the likelihood functions, functional relationships and to make predictions .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## General linear models\n",
    "\n",
    "Before we proceed let us study a case from linear algebra where we aim at fitting a set of data $\\hat{y}=[y_0,y_1,\\dots,y_{n-1}]$. We could think of these data as a result of an experiment or a complicated numerical experiment. These data are functions of a series of variables $\\hat{x}=[x_0,x_1,\\dots,x_{n-1}]$, that is $y_i = y(x_i)$ with $i=0,1,2,\\dots,n-1$. The variables $x_i$ could represent physical quantities like time, temperature, position etc. We assume that $y(x)$ is a smooth function. \n",
    "\n",
    "Since obtaining these data points may not be trivial, we want to use these data to fit a function which can allow us to make predictions for values of $y$ which are not in the present set. The perhaps simplest approach is to assume we can parametrize our function in terms of a polynomial of degree $n-1$ with $n$ points, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=y(x) \\rightarrow y(x_i)=\\tilde{y}_i+\\epsilon_i=\\sum_{j=0}^{n-1} \\beta_i x_i^j+\\epsilon_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\epsilon_i$ is the error in our approximation. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Rewriting the fitting procedure as a linear algebra problem\n",
    "\n",
    "For every set of values $y_i,x_i$ we have thus the corresponding set of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "y_0&=\\beta_0+\\beta_1x_0^1+\\beta_2x_0^2+\\dots+\\beta_{n-1}x_0^{n-1}+\\epsilon_0\\\\\n",
    "y_1&=\\beta_0+\\beta_1x_1^1+\\beta_2x_1^2+\\dots+\\beta_{n-1}x_1^{n-1}+\\epsilon_1\\\\\n",
    "y_2&=\\beta_0+\\beta_1x_2^1+\\beta_2x_2^2+\\dots+\\beta_{n-1}x_2^{n-1}+\\epsilon_2\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{n-1}&=\\beta_0+\\beta_1x_{n-1}^1+\\beta_2x_{n-1}^2+\\dots+\\beta_1x_{n-1}^{n-1}+\\epsilon_{n-1}.\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting the fitting procedure as a linear algebra problem, follows\n",
    "\n",
    "Defining the vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "2\n",
    "9\n",
    " \n",
    "<\n",
    "<\n",
    "<\n",
    "!\n",
    "!\n",
    "M\n",
    "A\n",
    "T\n",
    "H\n",
    "_\n",
    "B\n",
    "L\n",
    "O\n",
    "C\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "3\n",
    "0\n",
    " \n",
    "<\n",
    "<\n",
    "<\n",
    "!\n",
    "!\n",
    "M\n",
    "A\n",
    "T\n",
    "H\n",
    "_\n",
    "B\n",
    "L\n",
    "O\n",
    "C\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\epsilon} = [\\epsilon_0,\\epsilon_1, \\epsilon_2,\\dots, \\epsilon_{n-1}]^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{X}=\n",
    "\\begin{bmatrix} \n",
    "1& x_{0}^1 &x_{0}^2& \\dots & \\dots &x_{0}^{n-1}\\\\\n",
    "1& x_{1}^1 &x_{1}^2& \\dots & \\dots &x_{1}^{n-1}\\\\\n",
    "1& x_{2}^1 &x_{2}^2& \\dots & \\dots &x_{2}^{n-1}\\\\                      \n",
    "\\dots& \\dots &\\dots& \\dots & \\dots &\\dots\\\\\n",
    "1& x_{n-1}^1 &x_{n-1}^2& \\dots & \\dots &x_{n-1}^{n-1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can rewrite our equations as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \\hat{X}\\hat{\\beta}+\\hat{\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing the fitting procedure as a linear algebra problem\n",
    "\n",
    "We are obviously not limited to the above polynomial. We could replace the various powers of $x$ with elements of Fourier series, that is, instead of $x_i^j$ we could have $\\cos{(j x_i)}$ or $\\sin{(j x_i)}$, or time series or other orthogonal functions.\n",
    "For every set of values $y_i,x_i$ we can then generalize the equations to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "y_0&=\\beta_0x_{00}+\\beta_1x_{01}+\\beta_2x_{02}+\\dots+\\beta_{n-1}x_{0n-1}+\\epsilon_0\\\\\n",
    "y_1&=\\beta_0x_{10}+\\beta_1x_{11}+\\beta_2x_{12}+\\dots+\\beta_{n-1}x_{1n-1}+\\epsilon_1\\\\\n",
    "y_2&=\\beta_0x_{20}+\\beta_1x_{21}+\\beta_2x_{22}+\\dots+\\beta_{n-1}x_{2n-1}+\\epsilon_2\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{i}&=\\beta_0x_{i0}+\\beta_1x_{i1}+\\beta_2x_{i2}+\\dots+\\beta_{n-1}x_{in-1}+\\epsilon_i\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{n-1}&=\\beta_0x_{n-1,0}+\\beta_1x_{n-1,2}+\\beta_2x_{n-1,2}+\\dots+\\beta_1x_{n-1,n-1}+\\epsilon_{n-1}.\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing the fitting procedure as a linear algebra problem\n",
    "\n",
    "We redefine in turn the matrix $\\hat{X}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{X}=\n",
    "\\begin{bmatrix} \n",
    "x_{00}& x_{01} &x_{02}& \\dots & \\dots &x_{0,n-1}\\\\\n",
    "x_{10}& x_{11} &x_{12}& \\dots & \\dots &x_{1,n-1}\\\\\n",
    "x_{20}& x_{21} &x_{22}& \\dots & \\dots &x_{2,n-1}\\\\                      \n",
    "\\dots& \\dots &\\dots& \\dots & \\dots &\\dots\\\\\n",
    "x_{n-1,0}& x_{n-1,1} &x_{n-1,2}& \\dots & \\dots &x_{n-1,n-1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and without loss of generality we rewrite again  our equations as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \\hat{X}\\hat{\\beta}+\\hat{\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left-hand side of this equation forms know. Our error vector $\\hat{\\epsilon}$ and the parameter vector $\\hat{\\beta}$ are our unknow quantities. How can we obtain the optimal set of $\\beta_i$ values? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Optimizing our parameters\n",
    "\n",
    "We have defined the matrix $\\hat{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "y_0&=\\beta_0x_{00}+\\beta_1x_{01}+\\beta_2x_{02}+\\dots+\\beta_{n-1}x_{0n-1}+\\epsilon_0\\\\\n",
    "y_1&=\\beta_0x_{10}+\\beta_1x_{11}+\\beta_2x_{12}+\\dots+\\beta_{n-1}x_{1n-1}+\\epsilon_1\\\\\n",
    "y_2&=\\beta_0x_{20}+\\beta_1x_{21}+\\beta_2x_{22}+\\dots+\\beta_{n-1}x_{2n-1}+\\epsilon_1\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{i}&=\\beta_0x_{i0}+\\beta_1x_{i1}+\\beta_2x_{i2}+\\dots+\\beta_{n-1}x_{in-1}+\\epsilon_1\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{n-1}&=\\beta_0x_{n-1,0}+\\beta_1x_{n-1,2}+\\beta_2x_{n-1,2}+\\dots+\\beta_1x_{n-1,n-1}+\\epsilon_{n-1}.\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing our parameters, more details\n",
    "\n",
    "We well use this matrix to define the approximation $\\hat{\\tilde{y}}$ via the unknown quantity $\\hat{\\beta}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\tilde{y}}= \\hat{X}\\hat{\\beta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in order to find the optimal parameters $\\beta_i$ instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values $y_i$ (which represent hopefully the exact values) and the parametrized values $\\tilde{y}_i$, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(\\hat{\\beta})=\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\left(\\hat{y}-\\hat{\\tilde{y}}\\right)^T\\left(\\hat{y}-\\hat{\\tilde{y}}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or using the matrix $\\hat{X}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(\\hat{\\beta})=\\left(\\hat{y}-\\hat{X}\\hat{\\beta}\\right)^T\\left(\\hat{y}-\\hat{X}\\hat{\\beta}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations and optimizing our parameters\n",
    "\n",
    "The function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(\\hat{\\beta})=\\left(\\hat{y}-\\hat{X}\\hat{\\beta}\\right)^T\\left(\\hat{y}-\\hat{X}\\hat{\\beta}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be linked to the variance of the quantity $y_i$ if we interpret the latter as the mean value of for example a numerical  experiment. When linking below with the maximum likelihood approach below, we will indeed interpret $y_i$ as a mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_{i}=\\langle y_i \\rangle = \\beta_0x_{i,0}+\\beta_1x_{i,1}+\\beta_2x_{i,2}+\\dots+\\beta_{n-1}x_{i,n-1}+\\epsilon_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\langle y_i \\rangle$ is the mean value. Keep in mind also that till now  we have treated $y_i$ as the exact value. Normally, the response (dependent or outcome) variable $y_i$ the outcome of a numerical experiment or another type of experiment and is thus only an approximation to the true value. It is then always accompanied by an error estimate, often limited to a statistical error estimate given by the standard deviation discussed earlier. In the discussion here we will treat $y_i$ as our exact value for the response variable.\n",
    "\n",
    "In order to find the parameters $\\beta_i$ we will then minimize the spread of $Q(\\hat{\\beta})$ by requiring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial Q(\\hat{\\beta})}{\\partial \\beta_j} = \\frac{\\partial }{\\partial \\beta_j}\\left[ \\sum_{i=0}^{n-1}\\left(y_i-\\beta_0x_{i,0}-\\beta_1x_{i,1}-\\beta_2x_{i,2}-\\dots-\\beta_{n-1}x_{i,n-1}\\right)^2\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which results in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial Q(\\hat{\\beta})}{\\partial \\beta_j} = -2\\left[ \\sum_{i=0}^{n-1}x_{ij}\\left(y_i-\\beta_0x_{i,0}-\\beta_1x_{i,1}-\\beta_2x_{i,2}-\\dots-\\beta_{n-1}x_{i,n-1}\\right)\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in a matrix-vector form as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial Q(\\hat{\\beta})}{\\partial \\hat{\\beta}} = 0 = \\hat{X}^T\\left( \\hat{y}-\\hat{X}\\hat{\\beta}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations and optimizing our parameters\n",
    "\n",
    "We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial Q(\\hat{\\beta})}{\\partial \\hat{\\beta}} = 0 = \\hat{X}^T\\left( \\hat{y}-\\hat{X}\\hat{\\beta}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{X}^T\\hat{y} = \\hat{X}^T\\hat{X}\\hat{\\beta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if the matrix $\\hat{X}^T\\hat{X}$ is invertible we have the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\beta} =\\left(\\hat{X}^T\\hat{X}\\right)^{-1}\\hat{X}^T\\hat{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations and optimizing our parameters\n",
    "\n",
    "The residuals $\\hat{\\epsilon}$ are in turn given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\epsilon} = \\hat{y}-\\hat{\\tilde{y}} = \\hat{y}-\\hat{X}\\hat{\\beta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{X}^T\\left( \\hat{y}-\\hat{X}\\hat{\\beta}\\right)= 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{X}^T\\hat{\\epsilon}=\\hat{X}^T\\left( \\hat{y}-\\hat{X}\\hat{\\beta}\\right)= 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning that the solution for $\\hat{\\beta}$ is the one which minimizes the residuals.  Later we will link this with the maximum likelihood approach.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The $\\chi^2$ function\n",
    "\n",
    "\n",
    "Normally, the response (dependent or outcome) variable $y_i$ the outcome of a numerical experiment or another type of experiment and is thus only an approximation to the true value. It is then always accompanied by an error estimate, often limited to a statistical error estimate given by the standard deviation discussed earlier. In the discussion here we will treat $y_i$ as our exact value for the response variable.\n",
    "\n",
    "Introducing the standard deviation $\\sigma_i$ for each measurement $y_i$, we define now the $\\chi^2$ function as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\chi^2(\\hat{\\beta})=\\sum_{i=0}^{n-1}\\frac{\\left(y_i-\\tilde{y}_i\\right)^2}{\\sigma_i^2}=\\left(\\hat{y}-\\hat{\\tilde{y}}\\right)^T\\frac{1}{\\hat{\\Sigma^2}}\\left(\\hat{y}-\\hat{\\tilde{y}}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the matrix $\\hat{\\Sigma}$ is a diagonal matrix with $\\sigma_i$ as matrix elements. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The $\\chi^2$ function\n",
    "\n",
    "\n",
    "In order to find the parameters $\\beta_i$ we will then minimize the spread of $\\chi^2(\\hat{\\beta})$ by requiring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\hat{\\beta})}{\\partial \\beta_j} = \\frac{\\partial }{\\partial \\beta_j}\\left[ \\sum_{i=0}^{n-1}\\left(\\frac{y_i-\\beta_0x_{i,0}-\\beta_1x_{i,1}-\\beta_2x_{i,2}-\\dots-\\beta_{n-1}x_{i,n-1}}{\\sigma_i}\\right)^2\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which results in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\hat{\\beta})}{\\partial \\beta_j} = -2\\left[ \\sum_{i=0}^{n-1}\\frac{x_{ij}}{\\sigma_i}\\left(\\frac{y_i-\\beta_0x_{i,0}-\\beta_1x_{i,1}-\\beta_2x_{i,2}-\\dots-\\beta_{n-1}x_{i,n-1}}{\\sigma_i}\\right)\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in a matrix-vector form as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\hat{\\beta})}{\\partial \\hat{\\beta}} = 0 = \\hat{A}^T\\left( \\hat{b}-\\hat{A}\\hat{\\beta}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have defined the matrix $\\hat{A} =\\hat{X}/\\hat{\\Sigma}$ with matrix elements $a_{ij} = x_{ij}/\\sigma_i$ and the vector $\\hat{b}$ with elements $b_i = y_i/\\sigma_i$.   \n",
    "\n",
    "\n",
    "\n",
    "## The $\\chi^2$ function\n",
    "\n",
    "\n",
    "We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\hat{\\beta})}{\\partial \\hat{\\beta}} = 0 = \\hat{A}^T\\left( \\hat{b}-\\hat{A}\\hat{\\beta}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A}^T\\hat{b} = \\hat{A}^T\\hat{A}\\hat{\\beta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if the matrix $\\hat{A}^T\\hat{A}$ is invertible we have the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\beta} =\\left(\\hat{A}^T\\hat{A}\\right)^{-1}\\hat{A}^T\\hat{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\chi^2$ function\n",
    "\n",
    "\n",
    "If we then introduce the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{H} =  \\hat{A}^T\\hat{A},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have then the following expression for the parameters $\\beta_j$ (the matrix elements of $\\hat{H}$ are $h_{ij}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_j = \\sum_{k=0}^{p-1}h_{jk}\\sum_{i=0}^{n-1}\\frac{y_i}{\\sigma_i}\\frac{x_{ik}}{\\sigma_i} = \\sum_{k=0}^{p-1}h_{jk}\\sum_{i=0}^{n-1}b_ia_{ik}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We state without proof the expression for the uncertainty  in the parameters $\\beta_j$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2(\\beta_j) = \\sum_{i=0}^{n-1}\\sigma_i^2\\left( \\frac{\\partial \\beta_j}{\\partial y_i}\\right)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2(\\beta_j) = \\left(\\sum_{k=0}^{p-1}h_{jk}\\sum_{i=0}^{n-1}a_{ik}\\right)\\left(\\sum_{l=0}^{p-1}h_{jl}\\sum_{m=0}^{n-1}a_{ml}\\right) = h_{jj}!\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\chi^2$ function\n",
    "\n",
    "The first step here is to approximate the function $y$ with a first-order polynomial, that is we write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=y(x) \\rightarrow y(x_i) \\approx \\beta_0+\\beta_1 x_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By computing the derivatives of $\\chi^2$ with respect to $\\beta_0$ and $\\beta_1$ show that these are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\hat{\\beta})}{\\partial \\beta_0} = -2\\left[ \\sum_{i=0}^{1}\\left(\\frac{y_i-\\beta_0-\\beta_1x_{i}}{\\sigma_i^2}\\right)\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\hat{\\beta})}{\\partial \\beta_0} = -2\\left[ \\sum_{i=0}^{1}x_i\\left(\\frac{y_i-\\beta_0-\\beta_1x_{i}}{\\sigma_i^2}\\right)\\right]=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\chi^2$ function\n",
    "\n",
    "\n",
    "We define then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma =  \\sum_{i=0}^{1}\\frac{1}{\\sigma_i^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "6\n",
    "7\n",
    " \n",
    "<\n",
    "<\n",
    "<\n",
    "!\n",
    "!\n",
    "M\n",
    "A\n",
    "T\n",
    "H\n",
    "_\n",
    "B\n",
    "L\n",
    "O\n",
    "C\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "6\n",
    "8\n",
    " \n",
    "<\n",
    "<\n",
    "<\n",
    "!\n",
    "!\n",
    "M\n",
    "A\n",
    "T\n",
    "H\n",
    "_\n",
    "B\n",
    "L\n",
    "O\n",
    "C\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "6\n",
    "9\n",
    " \n",
    "<\n",
    "<\n",
    "<\n",
    "!\n",
    "!\n",
    "M\n",
    "A\n",
    "T\n",
    "H\n",
    "_\n",
    "B\n",
    "L\n",
    "O\n",
    "C\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma_{xy} = \\sum_{i=0}^{1}\\frac{y_ix_{i}}{\\sigma_i^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and show that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "7\n",
    "1\n",
    " \n",
    "<\n",
    "<\n",
    "<\n",
    "!\n",
    "!\n",
    "M\n",
    "A\n",
    "T\n",
    "H\n",
    "_\n",
    "B\n",
    "L\n",
    "O\n",
    "C\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_1 = \\frac{\\gamma_{xy}\\gamma-\\gamma_x\\gamma_y}{\\gamma\\gamma_{xx}-\\gamma_x^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSM suffers often from both being underdetermined and overdetermined in the unknown coefficients $\\beta_i$.  A better approach is to use the Singular Value Decomposition (SVD) method discussed below. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The singular value decompostion\n",
    "\n",
    "How can we use the singular value decomposition to find the parameters $\\beta_j$? More details will come. We first note that a general $m\\times n$ matrix $\\hat{A}$ can be written in terms of a diagonal matrix $\\hat{\\Sigma}$ of dimensionality $n\\times n$ and two orthognal matrices $\\hat{U}$ and $\\hat{V}$, where the first has dimensionality $m \\times n$ and the last dimensionality $n\\times n$. We have then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{A} = \\hat{U}\\hat{\\Sigma}\\hat{V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Artificial neurons\n",
    "The field of artificial neural networks has a long history of development, and is closely connected with \n",
    "the advancement of computer science and computers in general. A model of artificial neurons \n",
    "was first developed by McCulloch and Pitts in 1943  to study signal processing in the brain and \n",
    "has later been refined by others. The general idea is to mimic neural networks in the human brain, which\n",
    "is composed of billions of neurons that communicate with each other by sending electrical signals. \n",
    "Each neuron accumulates its incoming signals, \n",
    "which must exceed an activation threshold to yield an output. If the threshold is not overcome, the neuron\n",
    "remains inactive, i.e. has zero output.  \n",
    "\n",
    "This behaviour has inspired a simple mathematical model for an artificial neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"artificialNeuron\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y = f\\left(\\sum_{i=1}^n w_ix_i\\right) = f(u)\n",
    "\\label{artificialNeuron} \\tag{27}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the output $y$ of the neuron is the value of its activation function, which have as input\n",
    "a weighted sum of signals $x_i, \\dots ,x_n$ received by $n$ other neurons.\n",
    "\n",
    "\n",
    "## Neural network types\n",
    "\n",
    "An artificial neural network (NN), is a computational model that consists of layers of connected neurons, or *nodes*. \n",
    "It is supposed to mimic a biological nervous system by letting each neuron interact with other neurons\n",
    "by sending signals in the form of mathematical functions between layers. \n",
    "A wide variety of different NNs have\n",
    "been developed, but most of them consist of an input layer, an output layer and eventual layers in-between, called\n",
    "*hidden layers*. All layers can contain an arbitrary number of nodes, and each connection between two nodes\n",
    "is associated with a weight variable. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Feed-forward neural networks\n",
    "The feed-forward neural network (FFNN) was the first and simplest type of NN devised. In this network, \n",
    "the information moves in only one direction: forward through the layers.\n",
    "\n",
    "Nodes are represented by circles, while the arrows display the connections between the nodes, including the \n",
    "direction of information flow. Additionally, each arrow corresponds to a weight variable, not displayed here. \n",
    "We observe that each node in a layer is connected to *all* nodes in the subsequent layer, \n",
    "making this a so-called *fully-connected* FFNN. \n",
    "\n",
    "\n",
    "\n",
    "A different variant of FFNNs are *convolutional neural networks* (CNNs), which have a connectivity pattern\n",
    "inspired by the animal visual cortex. Individual neurons in the visual cortex only respond to stimuli from\n",
    "small sub-regions of the visual field, called a receptive field. This makes the neurons well-suited to exploit the strong\n",
    "spatially local correlation present in natural images. The response of each neuron can be approximated mathematically \n",
    "as a convolution operation. \n",
    "\n",
    "CNNs emulate the behaviour of neurons in the visual cortex by enforcing a *local* connectivity pattern\n",
    "between nodes of adjacent layers: Each node\n",
    "in a convolutional layer is connected only to a subset of the nodes in the previous layer, \n",
    "in contrast to the fully-connected FFNN.\n",
    "Often, CNNs \n",
    "consist of several convolutional layers that learn local features of the input, with a fully-connected layer at the end, \n",
    "which gathers all the local data and produces the outputs. They have wide applications in image and video recognition\n",
    "\n",
    "\n",
    "## Recurrent neural networks\n",
    "\n",
    "So far we have only mentioned NNs where information flows in one direction: forward. *Recurrent neural networks* on\n",
    "the other hand, have connections between nodes that form directed *cycles*. This creates a form of \n",
    "internal memory which are able to capture information on what has been calculated before; the output is dependent \n",
    "on the previous computations. Recurrent NNs make use of sequential information by performing the same task for \n",
    "every element in a sequence, where each element depends on previous elements. An example of such information is \n",
    "sentences, making recurrent NNs especially well-suited for handwriting and speech recognition.\n",
    "\n",
    "\n",
    "## Other types of networks\n",
    "\n",
    "There are many other kinds of NNs that have been developed. One type that is specifically designed for interpolation\n",
    "in multidimensional space is the radial basis function (RBF) network. RBFs are typically made up of three layers: \n",
    "an input layer, a hidden layer with non-linear radial symmetric activation functions and a linear output layer (''linear'' here\n",
    "means that each node in the output layer has a linear activation function). The layers are normally fully-connected and \n",
    "there are no cycles, thus RBFs can be viewed as a type of fully-connected FFNN. They are however usually treated as\n",
    "a separate type of NN due the unusual activation functions.\n",
    "\n",
    "\n",
    "Other types of NNs could also be mentioned, but are outside the scope of this work. We will now move on to a detailed description\n",
    "of how a fully-connected FFNN works, and how it can be used to interpolate data sets. \n",
    "\n",
    "\n",
    "## Multilayer perceptrons\n",
    "\n",
    "One use often so-called  fully-connected feed-forward neural networks with three\n",
    "or more layers (an input layer, one or more hidden layers and an output layer)\n",
    "consisting of neurons that have non-linear activation functions.\n",
    "\n",
    "Such networks are often called *multilayer perceptrons* (MLPs)\n",
    "\n",
    "\n",
    "## Why multilayer perceptrons?\n",
    "\n",
    "According to the *Universal approximation theorem*, a feed-forward neural network with just a single hidden layer containing \n",
    "a finite number of neurons can approximate a continuous multidimensional function to arbitrary accuracy, \n",
    "assuming the activation function for the hidden layer is a **non-constant, bounded and monotonically-increasing continuous function**.\n",
    "Note that the requirements on the activation function only applies to the hidden layer, the output nodes are always\n",
    "assumed to be linear, so as to not restrict the range of output values. \n",
    "\n",
    "We note that this theorem is only applicable to a NN with *one* hidden layer. \n",
    "Therefore, we can easily construct an NN \n",
    "that employs activation functions which do not satisfy the above requirements, as long as we have at least one layer\n",
    "with activation functions that *do*. Furthermore, although the universal approximation theorem\n",
    "lays the theoretical foundation for regression with neural networks, it does not say anything about how things work in practice: \n",
    "A neural network can still be able to approximate a given function reasonably well without having the flexibility to fit *all other*\n",
    "functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Mathematical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"artificialNeuron2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y = f\\left(\\sum_{i=1}^n w_ix_i + b_i\\right) = f(u)\n",
    "\\label{artificialNeuron2} \\tag{28}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an FFNN of such neurons, the *inputs* $x_i$\n",
    "are the *outputs* of the neurons in the preceding layer. Furthermore, an MLP is fully-connected, \n",
    "which means that each neuron receives a weighted sum of the outputs of *all* neurons in the previous layer. \n",
    "\n",
    "\n",
    "## Mathematical model\n",
    "\n",
    "First, for each node $i$ in the first hidden layer, we calculate a weighted sum $u_i^1$ of the input coordinates $x_j$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto11\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " u_i^1 = \\sum_{j=1}^2 w_{ij}^1 x_j  + b_i^1 \n",
    "\\label{_auto11} \\tag{29}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is the argument to the activation function $f_1$ of each neuron $i$,\n",
    "producing the output $y_i^1$ of all neurons in layer 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"outputLayer1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_i^1 = f_1(u_i^1) = f_1\\left(\\sum_{j=1}^2 w_{ij}^1 x_j  + b_i^1\\right)\n",
    "\\label{outputLayer1} \\tag{30}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we assume that all nodes in the same layer have identical activation functions, hence the notation $f_l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"generalLayer\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_i^l = f_l(u_i^l) = f_l\\left(\\sum_{j=1}^{N_{l-1}} w_{ij}^l y_j^{l-1} + b_i^l\\right)\n",
    "\\label{generalLayer} \\tag{31}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $N_l$ is the number of nodes in layer $l$. When the output of all the nodes in the first hidden layer are computed,\n",
    "the values of the subsequent layer can be calculated and so forth until the output is obtained. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Mathematical model\n",
    "\n",
    "The output of neuron $i$ in layer 2 is thus,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto12\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_i^2 = f_2\\left(\\sum_{j=1}^3 w_{ij}^2 y_j^1 + b_i^2\\right) \n",
    "\\label{_auto12} \\tag{32}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"outputLayer2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    " = f_2\\left[\\sum_{j=1}^3 w_{ij}^2f_1\\left(\\sum_{k=1}^2 w_{jk}^1 x_k + b_j^1\\right) + b_i^2\\right]\n",
    "\\label{outputLayer2} \\tag{33}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have substituted $y_m^1$ with. Finally, the NN output yields,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto13\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_1^3 = f_3\\left(\\sum_{j=1}^3 w_{1m}^3 y_j^2 + b_1^3\\right) \n",
    "\\label{_auto13} \\tag{34}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto14\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    " = f_3\\left[\\sum_{j=1}^3 w_{1j}^3 f_2\\left(\\sum_{k=1}^3 w_{jk}^2 f_1\\left(\\sum_{m=1}^2 w_{km}^1 x_m + b_k^1\\right) + b_j^2\\right)\n",
    "  + b_1^3\\right]\n",
    "\\label{_auto14} \\tag{35}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical model\n",
    "\n",
    "We can generalize this expression to an MLP with $l$ hidden layers. The complete functional form\n",
    "is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"completeNN\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y^{l+1}_1\\! = \\!f_{l+1}\\!\\left[\\!\\sum_{j=1}^{N_l}\\! w_{1j}^3 f_l\\!\\left(\\!\\sum_{k=1}^{N_{l-1}}\\! w_{jk}^2 f_{l-1}\\!\\left(\\!\n",
    " \\dots \\!f_1\\!\\left(\\!\\sum_{n=1}^{N_0} \\!w_{mn}^1 x_n\\! + \\!b_m^1\\!\\right)\n",
    " \\!\\dots \\!\\right) \\!+ \\!b_k^2\\!\\right)\n",
    " \\!+ \\!b_1^3\\!\\right] \n",
    "\\label{completeNN} \\tag{36}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which illustrates a basic property of MLPs: The only independent variables are the input values $x_n$. \n",
    "\n",
    "\n",
    "## Mathematical model\n",
    "\n",
    "This confirms that an MLP,\n",
    "despite its quite convoluted mathematical form, is nothing more than an analytic function, specifically a \n",
    "mapping of real-valued vectors $\\vec{x} \\in \\mathbb{R}^n \\rightarrow \\vec{y} \\in \\mathbb{R}^m$. \n",
    "In our example, $n=2$ and $m=1$. Consequentially, \n",
    "the number of input and output values of the function we want to fit must be equal to the number of inputs and outputs of our MLP.  \n",
    "\n",
    "Furthermore, the flexibility and universality of a MLP can be illustrated by realizing that \n",
    "the expression  is essentially a nested sum of scaled activation functions of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto15\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " h(x) = c_1 f(c_2 x + c_3) + c_4\n",
    "\\label{_auto15} \\tag{37}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the parameters $c_i$ are weights and biases. By adjusting these parameters, the activation functions\n",
    "can be shifted up and down or left and right, change slope or be rescaled \n",
    "which is the key to the flexibility of a neural network. \n",
    "\n",
    "\n",
    "### Matrix-vector notation\n",
    "\n",
    "We can introduce a more convenient notation for the activations in a NN. \n",
    "\n",
    "Additionally, we can represent the biases and activations\n",
    "as layer-wise column vectors $\\vec{b}_l$ and $\\vec{y}_l$, so that the $i$-th element of each vector \n",
    "is the bias $b_i^l$ and activation $y_i^l$ of node $i$ in layer $l$ respectively. \n",
    "\n",
    "We have that $\\mathrm{W}_l$ is a $N_{l-1} \\times N_l$ matrix, while $\\vec{b}_l$ and $\\vec{y}_l$ are $N_l \\times 1$ column vectors. \n",
    "With this notation, the sum in  becomes a matrix-vector multiplication, and we can write\n",
    "the equation for the activations of hidden layer 2 in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto16\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\vec{y}_2 = f_2(\\mathrm{W}_2 \\vec{y}_{1} + \\vec{b}_{2}) = \n",
    " f_2\\left(\\left[\\begin{array}{ccc}\n",
    "    w^2_{11} &w^2_{12} &w^2_{13} \\\\\n",
    "    w^2_{21} &w^2_{22} &w^2_{23} \\\\\n",
    "    w^2_{31} &w^2_{32} &w^2_{33} \\\\\n",
    "    \\end{array} \\right] \\cdot\n",
    "    \\left[\\begin{array}{c}\n",
    "           y^1_1 \\\\\n",
    "           y^1_2 \\\\\n",
    "           y^1_3 \\\\\n",
    "          \\end{array}\\right] + \n",
    "    \\left[\\begin{array}{c}\n",
    "           b^2_1 \\\\\n",
    "           b^2_2 \\\\\n",
    "           b^2_3 \\\\\n",
    "          \\end{array}\\right]\\right).\n",
    "\\label{_auto16} \\tag{38}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector notation  and activation\n",
    "\n",
    "The activation of node $i$ in layer 2 is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto17\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y^2_i = f_2\\Bigr(w^2_{i1}y^1_1 + w^2_{i2}y^1_2 + w^2_{i3}y^1_3 + b^2_i\\Bigr) = \n",
    " f_2\\left(\\sum_{j=1}^3 w^2_{ij} y_j^1 + b^2_i\\right).\n",
    "\\label{_auto17} \\tag{39}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not just a convenient and compact notation, but also \n",
    "a useful and intuitive way to think about MLPs: The output is calculated by a series of matrix-vector multiplications\n",
    "and vector additions that are used as input to the activation functions. For each operation \n",
    "$\\mathrm{W}_l \\vec{y}_{l-1}$ we move forward one layer. \n",
    "\n",
    "\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "A property that characterizes a neural network, other than its connectivity, is the choice of activation function(s). \n",
    "As described in, the following restrictions are imposed on an activation function for a FFNN\n",
    "to fulfill the universal approximation theorem\n",
    "\n",
    "  * Non-constant\n",
    "\n",
    "  * Bounded\n",
    "\n",
    "  * Monotonically-increasing\n",
    "\n",
    "  * Continuous\n",
    "\n",
    "### Activation functions, Logistic and Hyperbolic ones\n",
    "\n",
    "The second requirement excludes all linear functions. Furthermore, in a MLP with only linear activation functions, each \n",
    "layer simply performs a linear transformation of its inputs.\n",
    "\n",
    "Regardless of the number of layers, \n",
    "the output of the NN will be nothing but a linear function of the inputs. Thus we need to introduce some kind of \n",
    "non-linearity to the NN to be able to fit non-linear functions\n",
    "Typical examples are the logistic *Sigmoid*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"sigmoidActivationFunction\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x) = \\frac{1}{1 + e^{-x}},\n",
    "\\label{sigmoidActivationFunction} \\tag{40}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the *hyperbolic tangent* function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"tanhActivationFunction\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x) = \\tanh(x)\n",
    "\\label{tanhActivationFunction} \\tag{41}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance\n",
    "\n",
    "The *sigmoid* function are more biologically plausible because \n",
    "the output of inactive neurons are zero. Such activation function are called *one-sided*. However,\n",
    "it has been shown  that the hyperbolic tangent \n",
    "performs better than the sigmoid for training MLPs. \n",
    "has become the most popular  for *deep neural networks*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
