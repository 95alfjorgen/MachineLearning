TITLE: Data Analysis and Machine Learning: Elements of Bayesian theory
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University
DATE: today


!split
===== What is Bayesian Statistics  =====
!bblock
Say something general here. Reminder about probabilities from the statistics section
o Product rule
o Binomial distribution
o Gaussian PDF
o other PDFs
!eblock

!split
===== Bayesian regression analysis =====
!bblock
!bt
\[
mathbf {y} -\mathbf {X} {\boldsymbol {\beta }})\right).} \rho(\mathbf{y}|\mathbf{X},\boldsymbol\beta,\sigma^{2}) \propto (\sigma^{2})^{-n/2} \exp\left(-\frac{1}{2{\sigma}^{2}}(\mathbf{y}- \mathbf{X} \boldsymbol\beta)^{\rm T}(\mathbf{y}- \mathbf{X} \boldsymbol\beta)\right).
\]
!et
The ordinary least squares solution is to estimate the coefficient vector using the Moore-Penrose pseudoinverse:
!bt
\[
{\hat {\boldsymbol {\beta }}=(\mathbf {X} ^{\rm {T}}\mathbf {X} )^{-1}\mathbf {X} ^{\rm {T}}\mathbf {y} }  \hat{\boldsymbol\beta} = (\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T}\mathbf{y}
\]
!et
where 
!bt
\[
{\displaystyle \mathbf {X} } \mathbf {X}  
\]
!et
is the ${n\times k n \times k$ design matrix, each row of which is a predictor vector $\mathbf {x} _{i}^{\rm {T}} \mathbf{x}_{i}^{\rm T}$ and $\mathbf {y}  \mathbf {y}$  is the column $n$-vector $[y_{1}\;\cdots \;y_{n}]^{\rm {T}} [y_1 \; \cdots \; y_n]^{\rm T}$.

!eblock

