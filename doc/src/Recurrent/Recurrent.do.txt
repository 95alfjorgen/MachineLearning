TITLE: Data Analysis and Machine Learning: Recurrent neural networks
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University
DATE: today


!split
===== Recurrent neural networks: Overarching view =====

We have mostly looked at feedforward neural networks, where the
activations flow only in one direction, from the input layer to the
output layer.

A recurrent neural network (RNN) looks very much like a feedforward
neural network, except it also has connections pointing
backward. Letâ€™s look at the simplest possible RNN, composed of just
one neuron receiving inputs, producing an output, and sending that
output back to itself.

RNNs are used to analyze time series data such as stock prices, and
tell you when to buy or sell. In autonomous driving systems, they can
anticipate car trajectories and help avoid accidents. More generally,
they can work on sequences of arbitrary lengths, rather than on
fixed-sized inputs like all the nets we have discussed so far. For
example, they can take sentences, documents, or audio samples as
input, making them extremely useful for natural language processing
systems such as automatic translation and speech-to-text.


!split
=====  Set up of an RNN =====

