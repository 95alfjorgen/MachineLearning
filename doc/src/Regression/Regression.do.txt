TITLE: Data Analysis and Machine Learning: Linear Regression and more Advanced Regression Analysis
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University
DATE: today


!split
===== Regression analysis, overarching aims  =====
!bblock

Regression modeling deals with the description of  the sampling distribution of a given random variable $y$ varies as function of another variable or a set of such variables $\hat{x} =[x_0, x_1,\dots, x_p]^T$. 
The first variable is called the _dependent_, the _outcome_ or the _response_ variable while the set of variables $\hat{x}$ is called the independent variable, or the predictor variable or the explanatory variable. 
 
A regression model aims at finding a likelihood function $p(y\vert \hat{x})$, that is the conditional distribution for $y$ with a given $\hat{x}$. The estimation of  $p(y\vert \hat{x})$ is made using a data set with 
* $n$ cases $i = 0, 1, 2, \dots, n-1$ 
* Response (dependent or outcome) variable $y_i$ with $i = 0, 1, 2, \dots, n-1$ 
* $p$ Explanatory (independent or predictor) variables $\hat{x}_i=[x_{i0}, x_{i1}, \dots, x_{ip}]$ with $i = 0, 1, 2, \dots, n-1$   
 The goal of the regression analysis is to extract/exploit relationship between $y_i$ and $\hat{x}_i$ in or to infer causal dependencies, approximations to the likelihood functions, functional relationships and to make predictions .
!eblock


!split
===== General linear models  =====
!bblock
Before we proceed let us study a case from linear algebra where we aim at fitting a set of data $\hat{y}=[y_0,y_1,\dots,y_{n-1}]$. We could think of these data as a result of an experiment or a complicated numerical experiment. These data are functions of a series of variables $\hat{x}=[x_0,x_1,\dots,x_{n-1}]$, that is $y_i = y(x_i)$ with $i=0,1,2,\dots,n-1$. The variables $x_i$ could represent physical quantities like time, temperature, position etc. We assume that $y(x)$ is a smooth function. 

Since obtaining these data points may not be trivial, we want to use these data to fit a function which can allow us to make predictions for values of $y$ which are not in the present set. The perhaps simplest approach is to assume we can parametrize our function in terms of a polynomial of degree $n-1$ with $n$ points, that is
!bt
\[
y=y(x) \rightarrow y(x_i)=\tilde{y}_i+\epsilon_i=\sum_{j=0}^{n-1} \beta_i x_i^j+\epsilon_i,
\]
!et
where $\epsilon_i$ is the error in our approximation. 

!eblock


!split
===== Rewriting the fitting procedure as a linear algebra problem  =====
!bblock
For every set of values $y_i,x_i$ we have thus the corresponding set of equations
!bt
\begin{align*}
y_0&=\beta_0+\beta_1x_0^1+\beta_2x_0^2+\dots+\beta_{n-1}x_0^{n-1}+\epsilon_0\\
y_1&=\beta_0+\beta_1x_1^1+\beta_2x_1^2+\dots+\beta_{n-1}x_1^{n-1}+\epsilon_1\\
y_2&=\beta_0+\beta_1x_2^1+\beta_2x_2^2+\dots+\beta_{n-1}x_2^{n-1}+\epsilon_2\\
\dots & \dots \\
y_{n-1}&=\beta_0+\beta_1x_{n-1}^1+\beta_2x_{n-1}^2+\dots+\beta_1x_{n-1}^{n-1}+\epsilon_{n-1}.\\
\end{align*}
!et
!eblock


!split
===== Rewriting the fitting procedure as a linear algebra problem, follows  =====
!bblock
Defining the vectors
!bt
\[
\hat{y} = [y_0,y_1, y_2,\dots, y_{n-1}]^T,
\]
!et
and
!bt
\[
\hat{\beta} = [\beta_0,\beta_1, \beta_2,\dots, \beta_{n-1}]^T,
\]
!et
and
!bt
\[
\hat{\epsilon} = [\epsilon_0,\epsilon_1, \epsilon_2,\dots, \epsilon_{n-1}]^T,
\]
!et
and the matrix
!bt
\[
\hat{X}=
\begin{bmatrix} 
1& x_{0}^1 &x_{0}^2& \dots & \dots &x_{0}^{n-1}\\
1& x_{1}^1 &x_{1}^2& \dots & \dots &x_{1}^{n-1}\\
1& x_{2}^1 &x_{2}^2& \dots & \dots &x_{2}^{n-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
1& x_{n-1}^1 &x_{n-1}^2& \dots & \dots &x_{n-1}^{n-1}\\
\end{bmatrix} 
\]
!et
we can rewrite our equations as
!bt
\[
\hat{y} = \hat{X}\hat{\beta}+\hat{\epsilon}.
\]
!et
!eblock


!split
===== Generalizing the fitting procedure as a linear algebra problem  =====
!bblock
We are obviously not limited to the above polynomial. We could replace the various powers of $x$ with elements of Fourier series, that is, instead of $x_i^j$ we could have $\cos{(j x_i)}$ or $\sin{(j x_i)}$, or time series or other orthogonal functions.
For every set of values $y_i,x_i$ we can then generalize the equations to 
!bt
\begin{align*}
y_0&=\beta_0x_{00}+\beta_1x_{01}+\beta_2x_{02}+\dots+\beta_{n-1}x_{0n-1}+\epsilon_0\\
y_1&=\beta_0x_{10}+\beta_1x_{11}+\beta_2x_{12}+\dots+\beta_{n-1}x_{1n-1}+\epsilon_1\\
y_2&=\beta_0x_{20}+\beta_1x_{21}+\beta_2x_{22}+\dots+\beta_{n-1}x_{2n-1}+\epsilon_2\\
\dots & \dots \\
y_{i}&=\beta_0x_{i0}+\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_{n-1}x_{in-1}+\epsilon_i\\
\dots & \dots \\
y_{n-1}&=\beta_0x_{n-1,0}+\beta_1x_{n-1,2}+\beta_2x_{n-1,2}+\dots+\beta_1x_{n-1,n-1}+\epsilon_{n-1}.\\
\end{align*}
!et
!eblock


!split
===== Generalizing the fitting procedure as a linear algebra problem  =====
!bblock
We redefine in turn the matrix $\hat{X}$ as
!bt
\[
\hat{X}=
\begin{bmatrix} 
x_{00}& x_{01} &x_{02}& \dots & \dots &x_{0,n-1}\\
x_{10}& x_{11} &x_{12}& \dots & \dots &x_{1,n-1}\\
x_{20}& x_{21} &x_{22}& \dots & \dots &x_{2,n-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
x_{n-1,0}& x_{n-1,1} &x_{n-1,2}& \dots & \dots &x_{n-1,n-1}\\
\end{bmatrix} 
\]
!et
and without loss of generality we rewrite again  our equations as
!bt
\[
\hat{y} = \hat{X}\hat{\beta}+\hat{\epsilon}.
\]
!et
The left-hand side of this equation forms know. Our error vector $\hat{\epsilon}$ and the parameter vector $\hat{\beta}$ are our unknow quantities. How can we obtain the optimal set of $\beta_i$ values? 
!eblock


!split
===== Optimizing our parameters  =====
!bblock
We have defined the matrix $\hat{X}$
!bt
\begin{align*}
y_0&=\beta_0x_{00}+\beta_1x_{01}+\beta_2x_{02}+\dots+\beta_{n-1}x_{0n-1}+\epsilon_0\\
y_1&=\beta_0x_{10}+\beta_1x_{11}+\beta_2x_{12}+\dots+\beta_{n-1}x_{1n-1}+\epsilon_1\\
y_2&=\beta_0x_{20}+\beta_1x_{21}+\beta_2x_{22}+\dots+\beta_{n-1}x_{2n-1}+\epsilon_1\\
\dots & \dots \\
y_{i}&=\beta_0x_{i0}+\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_{n-1}x_{in-1}+\epsilon_1\\
\dots & \dots \\
y_{n-1}&=\beta_0x_{n-1,0}+\beta_1x_{n-1,2}+\beta_2x_{n-1,2}+\dots+\beta_1x_{n-1,n-1}+\epsilon_{n-1}.\\
\end{align*}
!et
!eblock


!split
===== Optimizing our parameters, more details  =====
!bblock
We well use this matrix to define the approximation $\hat{\tilde{y}}$ via the unknown quantity $\hat{\beta}$ as
!bt
\[
\hat{\tilde{y}}= \hat{X}\hat{\beta},
\]
!et
and in order to find the optimal parameters $\beta_i$ instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values $y_i$ (which represent hopefully the exact values) and the parametrized values $\tilde{y}_i$, namely
!bt
\[
Q(\hat{\beta})=\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\left(\hat{y}-\hat{\tilde{y}}\right)^T\left(\hat{y}-\hat{\tilde{y}}\right),
\]
!et
or using the matrix $\hat{X}$ as
!bt
\[
Q(\hat{\beta})=\left(\hat{y}-\hat{X}\hat{\beta}\right)^T\left(\hat{y}-\hat{X}\hat{\beta}\right).
\]
!et
!eblock


!split
===== Interpretations and optimizing our parameters  =====
!bblock
The function 
!bt
\[
Q(\hat{\beta})=\left(\hat{y}-\hat{X}\hat{\beta}\right)^T\left(\hat{y}-\hat{X}\hat{\beta}\right),
\]
!et
can be linked to the variance of the quantity $y_i$ if we interpret the latter as the mean value of for example a numerical  experiment. When linking below with the maximum likelihood approach below, we will indeed interpret $y_i$ as a mean value
!bt
\[
y_{i}=\langle y_i \rangle = \beta_0x_{i,0}+\beta_1x_{i,1}+\beta_2x_{i,2}+\dots+\beta_{n-1}x_{i,n-1}+\epsilon_i,
\]
!et
where $\langle y_i \rangle$ is the mean value. Keep in mind also that till now  we have treated $y_i$ as the exact value. Normally, the response (dependent or outcome) variable $y_i$ the outcome of a numerical experiment or another type of experiment and is thus only an approximation to the true value. It is then always accompanied by an error estimate, often limited to a statistical error estimate given by the standard deviation discussed earlier. In the discussion here we will treat $y_i$ as our exact value for the response variable.

In order to find the parameters $\beta_i$ we will then minimize the spread of $Q(\hat{\beta})$ by requiring
!bt
\[
\frac{\partial Q(\hat{\beta})}{\partial \beta_j} = \frac{\partial }{\partial \beta_j}\left[ \sum_{i=0}^{n-1}\left(y_i-\beta_0x_{i,0}-\beta_1x_{i,1}-\beta_2x_{i,2}-\dots-\beta_{n-1}x_{i,n-1}\right)^2\right]=0, 
\]
!et
which results in
!bt
\[
\frac{\partial Q(\hat{\beta})}{\partial \beta_j} = -2\left[ \sum_{i=0}^{n-1}x_{ij}\left(y_i-\beta_0x_{i,0}-\beta_1x_{i,1}-\beta_2x_{i,2}-\dots-\beta_{n-1}x_{i,n-1}\right)\right]=0, 
\]
!et
or in a matrix-vector form as
!bt
\[
\frac{\partial Q(\hat{\beta})}{\partial \hat{\beta}} = 0 = \hat{X}^T\left( \hat{y}-\hat{X}\hat{\beta}\right).  
\]
!et


!eblock


!split
===== Interpretations and optimizing our parameters  =====
!bblock
We can rewrite
!bt
\[
\frac{\partial Q(\hat{\beta})}{\partial \hat{\beta}} = 0 = \hat{X}^T\left( \hat{y}-\hat{X}\hat{\beta}\right),  
\]
!et
as
!bt
\[
\hat{X}^T\hat{y} = \hat{X}^T\hat{X}\hat{\beta},  
\]
!et
and if the matrix $\hat{X}^T\hat{X}$ is invertible we have the solution
!bt
\[
\hat{\beta} =\left(\hat{X}^T\hat{X}\right)^{-1}\hat{X}^T\hat{y}.
\]
!et

!eblock

!split
===== Interpretations and optimizing our parameters  =====
!bblock
The residuals $\hat{\epsilon}$ are in turn given by
!bt
\[
\hat{\epsilon} = \hat{y}-\hat{\tilde{y}} = \hat{y}-\hat{X}\hat{\beta},
\]
!et
and with 
!bt
\[
\hat{X}^T\left( \hat{y}-\hat{X}\hat{\beta}\right)= 0, 
\]
!et
we have
!bt
\[
\hat{X}^T\hat{\epsilon}=\hat{X}^T\left( \hat{y}-\hat{X}\hat{\beta}\right)= 0, 
\]
!et
meaning that the solution for $\hat{\beta}$ is the one which minimizes the residuals.  Later we will link this with the maximum likelihood approach.

!eblock


!split
===== The $\chi^2$ function  =====
!bblock

Normally, the response (dependent or outcome) variable $y_i$ the outcome of a numerical experiment or another type of experiment and is thus only an approximation to the true value. It is then always accompanied by an error estimate, often limited to a statistical error estimate given by the standard deviation discussed earlier. In the discussion here we will treat $y_i$ as our exact value for the response variable.

Introducing the standard deviation $\sigma_i$ for each measurement $y_i$, we define now the $\chi^2$ function as  
!bt
\[
\chi^2(\hat{\beta})=\sum_{i=0}^{n-1}\frac{\left(y_i-\tilde{y}_i\right)^2}{\sigma_i^2}=\left(\hat{y}-\hat{\tilde{y}}\right)^T\frac{1}{\hat{\Sigma^2}}\left(\hat{y}-\hat{\tilde{y}}\right),
\]
!et
where the matrix $\hat{\Sigma}$ is a diagonal matrix with $\sigma_i$ as matrix elements. 

!eblock

!split
===== The $\chi^2$ function  =====
!bblock

In order to find the parameters $\beta_i$ we will then minimize the spread of $\chi^2(\hat{\beta})$ by requiring
!bt
\[
\frac{\partial \chi^2(\hat{\beta})}{\partial \beta_j} = \frac{\partial }{\partial \beta_j}\left[ \sum_{i=0}^{n-1}\left(\frac{y_i-\beta_0x_{i,0}-\beta_1x_{i,1}-\beta_2x_{i,2}-\dots-\beta_{n-1}x_{i,n-1}}{\sigma_i}\right)^2\right]=0, 
\]
!et
which results in
!bt
\[
\frac{\partial \chi^2(\hat{\beta})}{\partial \beta_j} = -2\left[ \sum_{i=0}^{n-1}\frac{x_{ij}}{\sigma_i}\left(\frac{y_i-\beta_0x_{i,0}-\beta_1x_{i,1}-\beta_2x_{i,2}-\dots-\beta_{n-1}x_{i,n-1}}{\sigma_i}\right)\right]=0, 
\]
!et
or in a matrix-vector form as
!bt
\[
\frac{\partial \chi^2(\hat{\beta})}{\partial \hat{\beta}} = 0 = \hat{A}^T\left( \hat{b}-\hat{A}\hat{\beta}\right).  
\]
!et
where we have defined the matrix $\hat{A} =\hat{X}/\hat{\Sigma}$ with matrix elements $a_{ij} = x_{ij}/\sigma_i$ and the vector $\hat{b}$ with elements $b_i = y_i/\sigma_i$.   
!eblock

!split
===== The $\chi^2$ function  =====
!bblock

We can rewrite
!bt
\[
\frac{\partial \chi^2(\hat{\beta})}{\partial \hat{\beta}} = 0 = \hat{A}^T\left( \hat{b}-\hat{A}\hat{\beta}\right),  
\]
!et
as
!bt
\[
\hat{A}^T\hat{b} = \hat{A}^T\hat{A}\hat{\beta},  
\]
!et
and if the matrix $\hat{A}^T\hat{A}$ is invertible we have the solution
!bt
\[
\hat{\beta} =\left(\hat{A}^T\hat{A}\right)^{-1}\hat{A}^T\hat{b}.
\]
!et
!eblock

!split
===== The $\chi^2$ function  =====
!bblock

If we then introduce the matrix
!bt
\[
\hat{H} =  \left(\hat{A}^T\hat{A}\right)^{-1},
\]
!et
we have then the following expression for the parameters $\beta_j$ (the matrix elements of $\hat{H}$ are $h_{ij}$)
!bt
\[
\beta_j = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}\frac{y_i}{\sigma_i}\frac{x_{ik}}{\sigma_i} = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}b_ia_{ik}
\]
!et
We state without proof the expression for the uncertainty  in the parameters $\beta_j$ as (we leave this as an exercise)
!bt
\[
\sigma^2(\beta_j) = \sum_{i=0}^{n-1}\sigma_i^2\left( \frac{\partial \beta_j}{\partial y_i}\right)^2, 
\]
!et
resulting in 
!bt
\[
\sigma^2(\beta_j) = \left(\sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}a_{ik}\right)\left(\sum_{l=0}^{p-1}h_{jl}\sum_{m=0}^{n-1}a_{ml}\right) = h_{jj}!
\]
!et
!eblock

!split
===== The $\chi^2$ function  =====
!bblock
The first step here is to approximate the function $y$ with a first-order polynomial, that is we write
!bt
\[
y=y(x) \rightarrow y(x_i) \approx \beta_0+\beta_1 x_i.
\]
!et
By computing the derivatives of $\chi^2$ with respect to $\beta_0$ and $\beta_1$ show that these are given by
!bt
\[
\frac{\partial \chi^2(\hat{\beta})}{\partial \beta_0} = -2\left[ \sum_{i=0}^{n-1}\left(\frac{y_i-\beta_0-\beta_1x_{i}}{\sigma_i^2}\right)\right]=0,
\]
!et
and
!bt
\[
\frac{\partial \chi^2(\hat{\beta})}{\partial \beta_0} = -2\left[ \sum_{i=0}^{n-1}x_i\left(\frac{y_i-\beta_0-\beta_1x_{i}}{\sigma_i^2}\right)\right]=0.
\]
!et
!eblock

!split
===== The $\chi^2$ function  =====
!bblock

For a linear fit we don't need to invert a matrix!!  
Defining
!bt
\[
\gamma =  \sum_{i=0}^{n-1}\frac{1}{\sigma_i^2},
\]
!et

!bt
\[
\gamma_x =  \sum_{i=0}^{n-1}\frac{x_{i}}{\sigma_i^2},
\]
!et
!bt
\[
\gamma_y = \sum_{i=0}^{n-1}\left(\frac{y_i}{\sigma_i^2}\right),
\]
!et
!bt
\[
\gamma_{xx} =  \sum_{i=0}^{n-1}\frac{x_ix_{i}}{\sigma_i^2},
\]
!et
!bt
\[
\gamma_{xy} = \sum_{i=0}^{n-1}\frac{y_ix_{i}}{\sigma_i^2},
\]
!et
we obtain
!bt
\[
\beta_0 = \frac{\gamma_{xx}\gamma_y-\gamma_x\gamma_y}{\gamma\gamma_{xx}-\gamma_x^2},
\]
!et
!bt
\[
\beta_1 = \frac{\gamma_{xy}\gamma-\gamma_x\gamma_y}{\gamma\gamma_{xx}-\gamma_x^2}.
\]
!et

This approach (different linear and non-linear regression) suffers often from both being underdetermined and overdetermined in the unknown coefficients $\beta_i$.  A better approach is to use the Singular Value Decomposition (SVD) method discussed below. Or using Lasso and Ridge regression. See below.
!eblock




!split
===== Simple regression model =====
We are now ready to write our first program which aims at solving the above linear regression equations. We start with data we have produced ourselves, in this case normally distributed random numbers along the $x$-axis. These numbers define then the value of a function $y(x)=4+3x+N(0,1)$. Thereafter we order the $x$ values and employ our linear regression algorithm to set up the best fit. Here we find it useful to use the numpy function $c\_$ arrays where arrays are stacked along their last axis after being upgraded to at least two dimensions with ones post-pended to the shape. The following examples help in understanding what happens 
!bc pycod 
import numpy as np
print(np.c_[np.array([1,2,3]), np.array([4,5,6])])
print(np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])])
!ec

!bc pycod
# Importing various packages
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt

x = 2*np.random.rand(100,1)
y = 4+3*x+np.random.randn(100,1)

xb = np.c_[np.ones((100,1)), x]
beta = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)
xnew = np.array([[0],[2]])
xbnew = np.c_[np.ones((2,1)), xnew]
ypredict = xbnew.dot(beta)

plt.plot(xnew, ypredict, "r-")
plt.plot(x, y ,'ro')
plt.axis([0,2.0,0, 15.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Linear Regression')
plt.show()

!ec

We see that, as expected, a linear fit gives a seemingly (from the graph) good representation of the data.




!split
===== Simple regression model, now using _scikit-learn_ =====


We can repeat the above algorithm using _scikit-learn_ as follows
!bc pycod
# Importing various packages
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = 2*np.random.rand(100,1)
y = 4+3*x+np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
xnew = np.array([[0],[2]])
ypredict = linreg.predict(xnew)

plt.plot(xnew, ypredict, "r-")
plt.plot(x, y ,'ro')
plt.axis([0,2.0,0, 15.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Random numbers ')
plt.show()
!ec


!split
===== Simple linear regression model using _scikit-learn_ =====

We start with perhaps our simplest possible example, using _scikit-learn_ to perform linear regression analysis on a data set produced by us. 
What follows is a simple Python code where we have defined  function $y$ in terms of the variable $x$. Both are defined as vectors of dimension $1\times 100$. The entries to the vector $\hat{x}$  are given by random numbers generated with a uniform distribution with entries $x_i \in [0,1]$ (more about probability distribution functions later). These values are then used to define a function $y(x)$ (tabulated again as a vector) with a linear dependence on $x$ plus a random noise added via the normal distribution.


The Numpy functions are imported used the _import numpy as np_
statement and the random number generator for the uniform distribution
is called using the function _np.random.rand()_, where we specificy
that we want $100$ random variables.  Using Numpy we define
automatically an array with the specified number of elements, $100$ in
our case.  With the Numpy function _randn()_ we can compute random
numbers with the normal distribution (mean value $\mu$ equal to zero and
variance $\sigma^2$ set to one) and produce the values of $y$ assuming a linear
dependence as function of $x$

!bt
\[
y = 2x+N(0,1),
\]
!et

where $N(0,1)$ represents random numbers generated by the normal
distribution.  From _scikit-learn_ we import then the
_LinearRegression_ functionality and make a prediction $\tilde{y} =
\alpha + \beta x$ using the function _fit(x,y)_. We call the set of
data $(\hat{x},\hat{y})$ for our training data. The Python package
_scikit-learn_ has also a functionality which extracts the above
fitting parameters $\alpha$ and $\beta$ (see below). Later we will
distinguish between training data and test data.

For plotting we use the Python package
"matplotlib":"https://matplotlib.org/" which produces publication
quality figures. Feel free to explore the extensive
"gallery":"https://matplotlib.org/gallery/index.html" of examples. In
this example we plot our original values of $x$ and $y$ as well as the
prediction _ypredict_ ($\tilde{y}$), which attempts at fitting our
data with a straight line.

The Python code follows here.
!bc pycod
# Importing various packages
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = np.random.rand(100,1)
y = 2*x+np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
xnew = np.array([[0],[1]])
ypredict = linreg.predict(xnew)

plt.plot(xnew, ypredict, "r-")
plt.plot(x, y ,'ro')
plt.axis([0,1.0,0, 5.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Simple Linear Regression')
plt.show()
!ec


!split
===== Simple linear regression model  =====

This example serves several aims. It allows us to demonstrate several
aspects of data analysis and later machine learning algorithms. The
immediate visualization shows that our linear fit is not
impressive. It goes through the data points, but there are many
outliers which are not reproduced by our linear regression.  We could
now play around with this small program and change for example the
factor in front of $x$ and the normal distribution.  Try to change the
function $y$ to

!bt
\[
y = 10x+0.01 \times N(0,1),
\]
!et

where $x$ is defined as before.  


!split
===== Less noise =====

Does the fit look better? Indeed, by
reducing the role of the normal distribution we see immediately that
our linear prediction seemingly reproduces better the training
set. However, this testing 'by the eye' is obviouly not satisfactory in the
long run. Here we have only defined the training data and our model, and 
have not discussed a more rigorous approach to the _cost_ function.


!split
===== How to study our fits =====

We need more rigorous criteria in defining whether we have succeeded or
not in modeling our training data.  You will be surprised to see that
many scientists seldomly venture beyond this 'by the eye' approach. A
standard approach for the *cost* function is the so-called $\chi^2$
function 

!bt 
\[ \chi^2 = \frac{1}{n}
\sum_{i=0}^{n-1}\frac{(y_i-\tilde{y}_i)^2}{\sigma_i^2}, 
\] 
!et 

where $\sigma_i^2$ is the variance (to be defined later) of the entry
$y_i$.  We may not know the explicit value of $\sigma_i^2$, it serves
however the aim of scaling the equations and make the cost function
dimensionless.  


!split
===== Minimizing the cost function =====

Minimizing the cost function is a central aspect of
our discussions to come. Finding its minima as function of the model
parameters ($\alpha$ and $\beta$ in our case) will be a recurring
theme in these series of lectures. Essentially all machine learning
algorithms we will discuss center around the minimization of the
chosen cost function. This depends in turn on our specific
model for describing the data, a typical situation in supervised
learning. Automatizing the search for the minima of the cost function is a
central ingredient in all algorithms. Typical methods which are
employed are various variants of _gradient_ methods. These will be
discussed in more detail later. Again, you'll be surprised to hear that
many practitioners minimize the above function ''by the eye', popularly dubbed as 
'chi by the eye'. That is, change a parameter and see (visually and numerically) that 
the  $\chi^2$ function becomes smaller. 

!split
===== Relative error =====

There are many ways to define the cost function. A simpler approach is to look at the relative difference between the training data and the predicted data, that is we define 
the relative error as

!bt
\[
\epsilon_{\mathrm{relative}}= \frac{\vert \hat{y} -\hat{\tilde{y}}\vert}{\vert \hat{y}\vert}.
\]
!et
We can modify easily the above Python code and plot the relative error instead
!bc pycod
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = np.random.rand(100,1)
y = 5*x+0.01*np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
ypredict = linreg.predict(x)

plt.plot(x, np.abs(ypredict-y)/abs(y), "ro")
plt.axis([0,1.0,0.0, 0.5])
plt.xlabel(r'$x$')
plt.ylabel(r'$\epsilon_{\mathrm{relative}}$')
plt.title(r'Relative error')
plt.show()
!ec

Depending on the parameter in front of the normal distribution, we may
have a small or larger relative error. Try to play around with
different training data sets and study (graphically) the value of the
relative error.


!split
===== The richness of _scikit-learn_ =====

As mentioned above, _scikit-learn_ has an impressive functionality.
We can for example extract the values of $\alpha$ and $\beta$ and
their error estimates, or the variance and standard deviation and many
other properties from the statistical data analysis. 

Here we show an
example of the functionality of scikit-learn.
!bc pycod 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_error

x = np.random.rand(100,1)
y = 2.0+ 5*x+0.5*np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
ypredict = linreg.predict(x)
print('The intercept alpha: \n', linreg.intercept_)
print('Coefficient beta : \n', linreg.coef_)
# The mean squared error                               
print("Mean squared error: %.2f" % mean_squared_error(y, ypredict))
# Explained variance score: 1 is perfect prediction                                 
print('Variance score: %.2f' % r2_score(y, ypredict))
# Mean squared log error                                                        
print('Mean squared log error: %.2f' % mean_squared_log_error(y, ypredict) )
# Mean absolute error                                                           
print('Mean absolute error: %.2f' % mean_absolute_error(y, ypredict))
plt.plot(x, ypredict, "r-")
plt.plot(x, y ,'ro')
plt.axis([0.0,1.0,1.5, 7.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Linear Regression fit ')
plt.show()

!ec


!split
===== Functions in _scikit-learn_ =====

The function _coef_ gives us the parameter $\beta$ of our fit while _intercept_ yields 
$\alpha$. Depending on the constant in front of the normal distribution, we get values near or far from $alpha =2$ and $\beta =5$. Try to play around with different parameters in front of the normal distribution. The function _meansquarederror_ gives us the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss defined as
!bt 
\[ MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
\] 
!et 

The smaller the value, the better the fit. Ideally we would like to
have an MSE equal zero.  The attentive reader has probably recognized
this function as being similar to the $\chi^2$ function defined above.

!split
===== Other functions in  _scikit-learn_ =====

The _r2score_ function computes $R^2$, the coefficient of
determination. It provides a measure of how well future samples are
likely to be predicted by the model. Best possible score is 1.0 and it
can be negative (because the model can be arbitrarily worse). A
constant model that always predicts the expected value of $\hat{y}$,
disregarding the input features, would get a $R^2$ score of $0.0$.

If $\tilde{\hat{y}}_i$ is the predicted value of the $i-th$ sample and $y_i$ is the corresponding true value, then the score $R^2$ is defined as
!bt
\[
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
\]
!et
where we have defined the mean value  of $\hat{y}$ as
!bt
\[
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\]
!et

!split
===== The mean absolute error and other functions in  _scikit-learn_ =====

Another quantity will meet again in our discussions of regression analysis is 
 mean absolute error (MAE), a risk metric corresponding to the expected value of the absolute error loss or what we call the $l1$-norm loss. In our discussion above we presented the relative error.
The MAE is defined as follows
!bt
\[
\text{MAE}(\hat{y}, \hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1} \left| y_i - \tilde{y}_i \right|.
\]
!et
Finally we present the 
squared logarithmic (quadratic) error
!bt
\[
\text{MSLE}(\hat{y}, \hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n - 1} (\log_e (1 + y_i) - \log_e (1 + \tilde{y}_i) )^2,
\]
!et

where $\log_e (x)$ stands for the natural logarithm of $x$. This error
estimate is best to use when targets having exponential growth, such
as population counts, average sales of a commodity over a span of
years etc. 


!split
===== Cubic polynomial in  _scikit-learn_ =====

We will discuss in more
detail these and other functions in the various lectures.  We conclude this part with another example. Instead of 
a linear $x$-dependence we study now a cubic polynomial and use the polynomial regression analysis tools of scikit-learn. 
Add description of the various python commands.

!bc pycod
import matplotlib.pyplot as plt
import numpy as np
import random
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression

x=np.linspace(0.02,0.98,200)
noise = np.asarray(random.sample((range(200)),200))
y=x**3*noise
yn=x**3*100
poly3 = PolynomialFeatures(degree=3)
X = poly3.fit_transform(x[:,np.newaxis])
clf3 = LinearRegression()
clf3.fit(X,y)

Xplot=poly3.fit_transform(x[:,np.newaxis])
poly3_plot=plt.plot(x, clf3.predict(Xplot), label='Cubic Fit')
plt.plot(x,yn, color='red', label="True Cubic")
plt.scatter(x, y, label='Data', color='orange', s=15)
plt.legend()
plt.show()

def error(a):
    for i in y:
        err=(y-yn)/yn
    return abs(np.sum(err))/len(err)

print (error(y))
!ec

Using _R_, we can perform similar studies. 






!split
===== Polynomial Regression =====
!bc pycod
# Importing various packages
from math import exp, sqrt
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt

m = 100
x = 2*np.random.rand(m,1)+4.
y = 4+3*x*x+ +x-np.random.randn(m,1)

xb = np.c_[np.ones((m,1)), x]
theta = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)
xnew = np.array([[0],[2]])
xbnew = np.c_[np.ones((2,1)), xnew]
ypredict = xbnew.dot(theta)

plt.plot(xnew, ypredict, "r-")
plt.plot(x, y ,'ro')
plt.axis([0,2.0,0, 15.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Random numbers ')
plt.show()

!ec

!split 
===== Linking the regression analysis with a statistical interpretation =====

Before we proceed, and to link with our discussions of Bayesian statistics to come, it is useful the derive the standard regression analysis equations using a statistical interpretation. This allows us also to derive quantities like the variance and other expectation values in a rather straightforward way. 

It is assumed that $\varepsilon_i
\sim \mathcal{N}(0, \sigma^2)$ and the $\varepsilon_{i}$ are
independent, i.e.: \begin{align*} \mbox{Cov}(\varepsilon_{i_1},
\varepsilon_{i_2}) & = \left\{ \begin{array}{lcc} \sigma^2 & \mbox{if}
& i_1 = i_2, \\ 0 & \mbox{if} & i_1 \not= i_2.  \end{array} \right.
\end{align*} The randomness of $\varepsilon_i$ implies that
$\mathbf{Y}_i$ is also a random variable. In particular,
$\mathbf{Y}_i$ is normally distributed, because $\varepsilon_i \sim
\mathcal{N}(0, \sigma^2)$ and $\mathbf{X}_{i,\ast} \, \beta$ is a
non-random scalar. To specify the parameters of the distribution of
$\mathbf{Y}_i$ we need to calculate its first two moments. Its
expectation equals: \begin{align*} \mathbb{E}(Y_i) & =
\mathbb{E}(\mathbf{X}_{i, \ast} \, \beta) + \mathbb{E}(\varepsilon_i)
\, \, \, = \, \, \, \mathbf{X}_{i, \ast} \, \beta, \end{align*} while
its variance is: \begin{align*} \mbox{Var}(Y_i) & = \mathbb{E} \{ [Y_i
- \mathbb{E}(Y_i)]^2 \} \, \, \, = \, \, \, \mathbb{E} ( Y_i^2 ) -
[\mathbb{E}(Y_i)]^2 # \\ # & = \mathbb{E} [ ( \mathbf{X}_{i, \ast} \,
\beta + \varepsilon_i )^2] - ( \mathbf{X}_{i, \ast} \, \beta)^2 \\ &
= \mathbb{E} [ ( \mathbf{X}_{i, \ast} \, \beta)^2 + 2 \varepsilon_i
\mathbf{X}_{i, \ast} \, \beta + \varepsilon_i^2 ] - ( \mathbf{X}_{i,
\ast} \, \beta)^2 \\ # & = ( \mathbf{X}_{i, \ast} \, \beta)^2 + 2
\mathbb{E}(\varepsilon_i) \mathbf{X}_{i, \ast} \, \beta +
\mathbb{E}(\varepsilon_i^2 ) - ( \mathbf{X}_{i, \ast} \, \beta)^2 #
\\ & = \mathbb{E}(\varepsilon_i^2 ) \, \, \, = \, \, \,
\mbox{Var}(\varepsilon_i) \, \, \, = \, \, \, \sigma^2.  \end{align*}
Hence, $Y_i \sim \mathcal{N}( \mathbf{X}_{i, \ast} \, \beta,
\sigma^2)$. This formulation (in terms of the normal distribution) is
equivalent to the formulation of model (ref{form.linRegressionModel}),
as both capture the assumptions involved: the linearity of the
functional part and the normality of the error.

Model (ref{form.linRegressionModel}) is often written in a more condensed matrix form:


!bt
\begin{align}
\mathbf{Y} & =  \mathbf{X} \, \beta + \vvarepsilon, label{form.linRegressionModelinMatrix}
\end{align}
!et
where $\vvarepsilon = (\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n)^{\top}$ and distributed as $\vvarepsilon \sim \mathcal{N}(\mathbf{0}_{p}, \sigma^2 \mathbf{I}_{nn})$. As above model (ref{form.linRegressionModelinMatrix}) can be expressed as a multivariate normal distribution: $\mathbf{Y} \sim \mathcal{N}(\mathbf{X} \, \beta, \sigma^2 \mathbf{I}_{nn})$.

Model (ref{form.linRegressionModelinMatrix}) is a so-called
hierarchical model. This terminology emphasizes that $\mathbf{X}$ and
$\mathbf{Y}$ are not on a par, they play different roles in the
model. The former is used to explain the latter. In model
(ref{form.linRegressionModel}) $\mathbf{X}$ is referred as the
*explanatory* or *independent* variable, while the variable
$\mathbf{Y}$ is generally referred to as the *response* or *dependent*
variable.

The covariates, the columns of $\mathbf{X}$, may themselves be
random. To apply the linear model they are temporarily assumed
fixed. The linear regression model is then to be interpreted as
$\mathbf{Y} \, | \, \mathbf{X} \sim \mathcal{N}(\mathbf{X} \, \beta,
\sigma^2 \mathbf{I}_{nn})$


The linear regression model (ref{form.linRegressionModel}) involves the unknown parameters: $\beta$ and $\sigma^2$, which need to be learned from the data. The parameters of the regression model, $\beta$ and $\sigma^2$ are estimated by means of likelihood maximization. Recall that $Y_i \sim \mathcal{N}( \mathbf{X}_{i,\ast} \, \beta, \sigma^2)$ with corresponding density: $ f_{Y_i}(y_i)  =  (2 \, \pi \, \sigma^2)^{-1/2} \, \exp[ - (y_i - \mathbf{X}_{i\ast} \, \beta)^2 / 2 \sigma^2 ]$. The likelihood thus is:


!bt
\begin{align*}
L(\mathbf{Y}, \mathbf{X}; \beta, \sigma^2) & =  
# f_{\mathbf{Y}}(\mathbf{y}) \, \, \, = \, \, \, \prod_{i=1}^n f_{Y_i}(y_i) \, \, \, = \, \, \, 
\prod_{i=1}^n  \frac{1}{\sqrt{2 \, \pi} \, \sigma} \, \exp[ - (Y_i - \mathbf{X}_{i, \ast} \, \beta)^2 / 2 \sigma^2 ],
\end{align*}
!et

in which the independence of the observations has been used. Because
of the concavity of the logarithm, the maximization of the likelihood
coincides with the maximum of the logarithm of the likelihood (called
the log-likelihood). Hence, to obtain maximum likelihood (ML)
estimates of the parameter it is equivalent to find the maximum of the
log-likelihood. The log-likelihood is:

!bt
\begin{align*}
\mathcal{L}(\mathbf{Y}, \mathbf{X}; \beta, \sigma^2) & = 
\log[ L(\mathbf{Y}, \mathbf{X}; \beta, \sigma^2) ]
\, \, \, = \, \, \, % \log \Big[  \prod_{i=1}^n f_{Y_i}(y_i) \Big]
#  \, \, \, = \, \, \, \sum_{i=1}^n \log[ f_{Y_i}(y_i) ]
#  \\
#  & =  \sum_{i=1}^n [ -\log(\sqrt{2 \, \pi} \, \sigma) -  (y_i - \mathbf{X}_{i\ast} \, \beta)^2 / 2 \sigma^2  ]
#  \\
#  & =   
-n \, \log(\sqrt{2 \, \pi} \, \sigma) -  \frac{1}{ 2 \sigma^2} \sum_{i=1}^n (y_i - \mathbf{X}_{i, \ast} \, \beta)^2.
\end{align*}
After noting that  $\sum_{i=1}^n (Y_i - \mathbf{X}_{i, \ast} \, \beta)^2  =  \| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2 \, \, \, = \, \, \, (\mathbf{Y} - \mathbf{X} \, \beta)^{\top} \, (\mathbf{Y} - \mathbf{X} \, \beta)$, the log-likelihood can be written as:
\begin{align*}
\mathcal{L}(\mathbf{Y}, \mathbf{X}; \beta, \sigma^2) & =  -n \, \log(\sqrt{2 \, \pi} \, \sigma) -  \frac{1}{ 2 \sigma^2} \, \| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2.
\end{align*}
!et

In order to find the maximum of the log-likelihood, take its derivate with respect to $\beta$:
!bt
\begin{align*}
\frac{\partial }{\partial \, \beta} \mathcal{L}(\mathbf{Y}, \mathbf{X}; \beta, \sigma^2) & =  -  \frac{1}{ 2 \sigma^2} \, \frac{\partial }{\partial \, \beta} \| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2 \, \, \, = \, \, \, \frac{1}{\sigma^2} \, \mathbf{X}^{\top} (\mathbf{Y} - \mathbf{X} \, \beta).
\end{align*}
!et

Equate this derivative to zero gives the estimating equation for $\beta$:

!bt
\begin{align} label{form.normalEquation}
\mathbf{X}^{\top} \mathbf{X} \, \beta & =   \mathbf{X}^{\top} \mathbf{Y}.
\end{align}
!et

Equation (ref{form.normalEquation}) is called to the *normal equation*. Pre-multiplication of both sides of the normal equation by $(\mathbf{X}^{\top} \mathbf{X})^{-1}$ now yields the ML estimator of the regression parameter: $\hat{\beta}  =   (\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{Y}$, in which it is assumed that $(\mathbf{X}^{\top} \mathbf{X})^{-1}$ is well-defined.

Along the same lines one obtains the ML estimator of the residual variance. Take the partial derivative of the log-likelihood with respect to $\sigma^2$:

!bt
\begin{align*}
\frac{\partial }{\partial \, \sigma} \mathcal{L}(\mathbf{Y}, \mathbf{X}; \beta, \sigma^2) & =  -  \frac{n}{\sigma}  +  \frac{1}{\sigma^3} \| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2.
\end{align*}
!et


Equate the right-hand side to zero and solve for $\sigma^2$ to find
$\hat{\sigma}^2 = \tfrac{1}{n} \| \mathbf{Y} - \mathbf{X} \, \beta
\|^2_2$. In this expression $\beta$ is unknown and the ML estimate of
$\beta$ is plugged-in.  \\ \\ With explicit expressions of the ML
estimators at hand, we can study their properties. The expectation of
the ML estimator of the regression parameter $\beta$ is:

!bt
\begin{align*} 
\mathbb{E}(\hat{\beta}) & = \mathbb{E}[
(\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{Y}]
\, \, \, = \, \, \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \,
\mathbf{X}^{\top} \mathbb{E}[ \mathbf{Y}] # \\ \, \, \, = \, \, \,
(\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{X} \,
\beta \, \, \, \, \, = \, \, \, \beta.  
\end{align*} 
!et

Hence, the ML
estimator of the regression coefficients is unbiased.

The variance of the ML estimator of $\beta$ is:

!bt
\begin{align*}
\mbox{Var}(\hat{\beta}) & =  \mathbb{E} \{ [\hat{\beta} - \mathbb{E}(\hat{\beta})] [\hat{\beta} - \mathbb{E}(\hat{\beta})]^{\top} \}
\\
& =  \mathbb{E} \{ [(\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{Y} - \beta] \, [(\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{Y} - \beta]^{\top} \}
\\
#  & =  \mathbb{E} \{ [(\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{Y}] \, [(\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{Y}]^{\top} \} - \beta \, \beta^{\top}
#  \\
#  & =  \mathbb{E} \{ (\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{Y} \, \mathbf{Y}^{\top} \, \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1}  \} - \beta \, \beta^{\top}
#  \\
& =  (\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \, \mathbb{E} \{ \mathbf{Y} \, \mathbf{Y}^{\top} \} \, \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} - \beta \, \beta^{\top}
\\
& =  (\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \, \{ \mathbf{X} \, \beta \, \beta^{\top} \,  \mathbf{X}^{\top} + \SSigma \} \, \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} - \beta \, \beta^{\top}
#  \\
#  & =  (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \, \mathbf{X} \, \beta \, \beta^T \,  \mathbf{X}^T \, \mathbf{X} \, (\mathbf{X}^T % \mathbf{X})^{-1}
#  \\
#  & & + \, \, \sigma^2 \, (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T  \, \mathbf{X} \, (\mathbf{X}^T \mathbf{X})^{-1} - \beta \beta^T
\\
& =  \beta \, \beta^{\top}  + \sigma^2 \, (\mathbf{X}^{\top} \mathbf{X})^{-1} - \beta \, \beta^{\top}
\, \, \, = \, \, \, \sigma^2 \, (\mathbf{X}^{\top} \mathbf{X})^{-1},
\end{align*}
!et

in which we have used that $\mathbb{E} (\mathbf{Y} \mathbf{Y}^{\top})
= \mathbf{X} \, \beta \, \beta^{\top} \, \mathbf{X}^{\top} + \sigma^2
\, \mathbf{I}_{nn}$. From $\mbox{Var}(\hat{\beta}) = \sigma^2 \,
(\mathbf{X}^{\top} \mathbf{X})^{-1}$, one obtains an estimate of the
variance of the estimate of the $j$-th regression coefficient:
$\hat{\sigma}^2 (\hat{\beta}_j ) = \hat{\sigma}^2 \sqrt{
[(\mathbf{X}^{\top} \mathbf{X})^{-1}]_{jj} }$. This may be used to
construct a confidence interval for the estimates or test the
hypothesis $H_0: \beta_j = 0$. In the latter $\hat{\sigma}^2$ should
not be the maximum likelihood estimator, as it is biased. It is then
to be replaced by the residual sum-of-squares divided by $n-p$ rather
than $n$.


The prediction of $Y_i$, denoted $\widehat{Y}_i$, is the expected
value of $Y_i$ according the linear regression model (with its
parameters replaced by their estimates). The prediction of $Y_i$ thus
equals $\mathbb{E}(Y_i; \hat{\beta}, \hat{\sigma}^2) = \mathbf{X}_{i,
\ast} \hat{\beta}$. In matrix notation the prediction is:

!bt
\begin{align*}
\widehat{\mathbf{Y}} & =  \mathbf{X} \, \hat{\beta} \, \, \, = \, \, \,  \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y} \, \, \, := \, \, \,  \mathbf{H} \mathbf{Y},
\end{align*}
!et

where $\mathbf{H}$ is the *hat matrix*, as it `puts the hat' on
$\mathbf{Y}$. Note that the hat matrix is a projection matrix,
i.e. $\mathbf{H}^2 = \mathbf{H}$ for

!bt
\begin{align*}
\mathbf{H}^2 & =  \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \, \, \, = \, \, \, \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}.
\end{align*}
!et

Thus, the prediction $\widehat{\mathbf{Y}}$ is an orthogonal
projection of $\mathbf{Y}$ onto the space spanned by the columns of
$\mathbf{X}$.

With $\widehat{\beta}$ available, an estimate of the errors
$\hat{\varepsilon}_i$, dubbed the *residuals* are obtained via:

!bt
\begin{align*}
\hat{\vvarepsilon} & =  \mathbf{Y} - \widehat{\mathbf{Y}} \, \, \, = \, \, \, \mathbf{Y} - \mathbf{X} \, \hat{\beta} \, \, \, = \, \, \, \mathbf{Y} - \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y} \, \, \, = \, \, \, [ \mathbf{I} - \mathbf{X} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} ] \, \mathbf{Y}.
\end{align*}
!et

Thus, the residuals are a projection of $\mathbf{Y}$ onto the
orthogonal complement of the space spanned by the columns of
$\mathbf{X}$. The residuals are to be used in diagnostics,
e.g. checking of the normality assumption by means of a normal
probability plot.  






!split
===== The singular value decompostion  =====
!bblock


A general
$m\times n$ matrix $\hat{A}$ can be written in terms of a diagonal
matrix $\hat{D}$ of dimensionality $n\times n$ and two orthognal
matrices $\hat{U}$ and $\hat{V}$, where the first has dimensionality
$m \times m$ and the last dimensionality $n\times n$. 
We have then 
!bt
\[ 
\hat{A} = \hat{U}\hat{D}\hat{V}^T 
\] 
!et 
!eblock


!split
===== Ridge regression =====


!split
===== Lasso regression =====



!split
===== Code examples for Ridge and Lasso Regression =====

!bc pycod
import matplotlib.pyplot as plt
import numpy as np
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

#creating data with random noise
x=np.arange(50)

delta=np.random.uniform(-2.5,2.5, size=(50))
np.random.shuffle(delta)
y =0.5*x+5+delta

#arranging data into 2x50 matrix
a=np.array(x) #inputs
b=np.array(y) #outputs

#Split into training and test
X_train=a[:37, np.newaxis]
X_test=a[37:, np.newaxis]
y_train=b[:37]
y_test=b[37:]

print ("X_train: ", X_train.shape)
print ("y_train: ", y_train.shape)
print ("X_test: ", X_test.shape)
print ("y_test: ", y_test.shape)

print ("------------------------------------")

print ("Ordinary Least Squares")
#Add Ordinary Least Squares fit
reg=LinearRegression()
reg.fit(X_train, y_train)
pred=reg.predict(X_test)
print ("Prediction Shape: ", pred.shape)

print('Coefficients: \n', reg.coef_)
# The mean squared error
print("Mean squared error: %.2f"
      % mean_squared_error(y_test, pred))
# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(y_test, pred))

#plot
plt.scatter(X_test,y_test,color='green', label="Training Data")
plt.plot(X_test, pred, color='black', label="Fit Line")
plt.legend()
plt.show()

print ("------------------------------------")

print ("Ridge Regression")

ridge=linear_model.RidgeCV(alphas=[0.1,1.0,10.0])
ridge.fit(X_train,y_train)
print ("Ridge Coefficient: ",ridge.coef_)
print ("Ridge Intercept: ", ridge.intercept_)
#Look into graphing with Ridge fit

print ("------------------------------------")

print ("Lasso")
lasso=linear_model.Lasso(alpha=0.1)
lasso.fit(X_train,y_train)
predl=lasso.predict(X_test)
print("Lasso Coefficient: ", lasso.coef_)
print("Lasso Intercept: ", lasso.intercept_)
plt.scatter(X_test,y_test,color='green', label="Training Data")
plt.plot(X_test, predl, color='blue', label="Lasso")
plt.legend()
plt.show()
!ec





!split
=====  Logistic regression =====






Consider an experiment in which $p$ characteristics of $n$ samples are
measured. The data from this experiment are denoted $\mathbf{X}$, with
$\mathbf{X}$ as above. The matrix $\mathbf{X}$ is called the *design
matrix*. Additional information of the samples is available in the
form of $\mathbf{Y}$ (also as above). The variable $\mathbf{Y}$ is
generally referred to as the *response variable*. The aim of
regression analysis is to explain $\mathbf{Y}$ in terms of
$\mathbf{X}$ through a functional relationship like $Y_i =
f(\mathbf{X}_{i,\ast})$. When no prior knowledge on the form of
$f(\cdot)$ is available, it is common to assume a linear relationship
between $\mathbf{X}$ and $\mathbf{Y}$. This assumption gives rise to
the *linear regression model*:


!bt
\begin{align}
Y_{i} & =  \mathbf{X}_{i,\ast} \, \beta + \varepsilon_i label{form.linRegressionModel}
\\
\nonumber
& =  \beta_1 \, X_{i,1}  + \ldots   + \beta_{p} \, X_{i, p} +  \varepsilon_i.
\end{align}
!et

In model (ref{form.linRegressionModel}) $\beta = (\beta_1, \ldots,
\beta_p)^{\top}$ is the *regression parameter*. The parameter
$\beta_j$, $j=1, \ldots, p$, represents the effect size of covariate
$j$ on the response. That is, for each unit change in covariate $j$
(while keeping the other covariates fixed) the observed change in the
response is equal to $\beta_j$.  The second summand on the right-hand
side of the model, $\varepsilon_i$, is referred to as the error. It
represents the part of the response not explained by the functional
part $\mathbf{X}_{i,\ast} \, \beta$ of the model
(ref{form.linRegressionModel}). In contrast to the functional part,
which is considered to be systematic (i.e. non-random), the error is
assumed to be random. Consequently, $Y_{i_1,\ast}$ need not equal
$Y_{i_2,\ast}$ for $i_1 \not= i_2$, even if $\mathbf{X}_{i_1,\ast}=
\mathbf{X}_{i_2,\ast}$. To complete the formulation of model
(ref{form.linRegressionModel}) we need to specify the probability
distribution of $\varepsilon_i$. 

======= Ridge regression =======

When the design matrix is high-dimensional, the covariates (the
columns of $\mathbf{X}$) are super-collinear. Recall *collinearity* in
regression analysis refers to the event of two (or multiple)
covariates being highly linearly related. Consequently, the subspace
spanned by collinear covariates may not be (or close to not being) of
full rank.  When the subspace (onto which $\mathbf{Y}$ is projected)
is (close to) rank deficient, it is (almost) impossible to separate
the contribution of the individual covariates. The uncertainty with
respect to the covariate responsible for the variation explained in
$\mathbf{Y}$ is often reflected in the fit of the linear regression
model to data by a large error of the estimates of the regression
parameters corresponding to the collinear covariates.


Consider the design matrix:

!bt
\begin{align*}
\mathbf{X} & =  \left(
\begin{array}{rrr}
1 & -1 & 2
\\
1 & 0 & 1
\\
1 & 2  & -1
\\
1 & 1  & 0
\end{array} \right)
\end{align*}
!et

The columns of $\mathbf{X}$ are linearly dependent: the first column
is the row-wise sum of the other two columns. The rank (more correct,
the column rank) of a matrix is the dimension of space spanned by the
column vectors. Hence, the rank of $\mathbf{X}$ is equal to the number
of linearly independent columns: $\mbox{rank}(\mathbf{X}) = 2$.




Super-collinearity of an $(n \times p)$-dimensional design matrix $\mathbf{X}$ implies[^footnote1][^footnote1]: If the (column) rank of $\mathbf{X$ is smaller than $p$, there exists a non-trivial $\mathbf{v} \in \mathbb{R}^p$ such that $\mathbf{X} \mathbf{v} = \mathbf{0}_{p}$. Multiplication of this inequality by $\mathbf{X}^{\top}$ yields $\mathbf{X}^{\top} \mathbf{X} \mathbf{v} = \mathbf{0}_{p}$. As $\mathbf{v} \not= \mathbf{0}_{p}$, this implies that $\mathbf{X}^{\top} \mathbf{X}$ is not invertible.} that the rank of the $(p \times p)$-dimensional matrix $\mathbf{X}^{\top} \mathbf{X}$ is smaller than $p$, and, consequently, it is singular. A square matrix that does not have an inverse is called *singular*. A matrix $\mathbf{A}$ is singular if and only if its determinant is zero: $\mbox{det}(\mathbf{A}) = 0$.



Consider the matrix $\mathbf{A}$ given by:
!bt
\begin{align*}
\mathbf{A} & =  \left(
\begin{array}{rr}
1 & 2
\\
2 & 4
\end{array} \right)
\end{align*}
!et
Clearly, $\mbox{det}(\mathbf{A}) = a_{11} a_{22} - a_{12} a_{21} = 1 \times 4 - 2 \times 2 = 0$. Hence, $\mathbf{A}$ is singular and its inverse is undefined.

As $\mbox{det}(\mathbf{A})$ is equal to the product of the eigenvalues
$\nu_j$ of $\mathbf{A}$, the matrix $\mathbf{A}$ is singular if one
(or more) of the eigenvalues of $\mathbf{A}$ is zero. To see this,
consider the spectral decomposition of $\mathbf{A}$:

!bt
\begin{align*}
\mathbf{A} & =  \sum_{j=1}^p \nu_j \, \mathbf{v}_j \, \mathbf{v}_j^{\top},
\end{align*}
!et
where $\mathbf{v}_j$ is the eigenvector corresponding to $\nu_j$. The inverse of $\mathbf{A}$ is then:
!bt
\begin{align*}
\mathbf{A}^{-1} & =  \sum_{j=1}^p \nu_j^{-1} \, \mathbf{v}_j \, \mathbf{v}_j^{\top}.
\end{align*}
!et

The right-hand side is undefined if $\nu_j =0$ for any $j$.


Matrix $\mathbf{A}$ has eigenvalues $\nu_1 =5$ and $\nu_2=0$. According to the spectral decomposition, the inverse of $\mathbf{A}$ is:
!bt
\begin{align*}
\mathbf{A}^{-1} & =  \frac{1}{5} \, \mathbf{v}_1 \, \mathbf{v}_1^{\top} + \frac{1}{0} \, \mathbf{v}_2 \, \mathbf{v}_2^{\top}.
\end{align*}
!et

This expression is undefined as we divide by zero in the second summand on the right-hand side.



In summary, the columns of a high-dimensional design matrix
$\mathbf{X}$ are linearly dependent and this super-collinearity causes
$\mathbf{X}^{\top} \mathbf{X}$ to be singular. Now recall the ML
estimator of the parameter of the linear regression model:

!bt
\begin{align}
\hat{\beta} & =  (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}.
\end{align}
!et


This estimator is only well-defined if $(\mathbf{X}^{\top}
\mathbf{X})^{-1}$ exits. Hence, when $\mathbf{X}$ is high-dimensional
the regression parameter $\beta$ cannot be estimated.


Above only the practical consequence of high-dimensionality is presented: the expression $( \mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}$ cannot be evaluated numerically. But the problem arising from the high-dimensionality of the data is more fundamental. To appreciate this, consider the normal equations:

!bt
\begin{align*}
\mathbf{X}^{\top} \mathbf{X} \beta & =  \mathbf{X}^{\top} \mathbf{Y}.
\end{align*}
!et

The matrix $\mathbf{X}^{\top} \mathbf{X}$ is of rank $n$, while $\beta$ is a vector of length $p$. Hence, while there are $p$ unknowns, the system of linear equations from which these are to be solved effectively comprises $n$ degrees of freedom. If $p > n$, the vector $\beta$ cannot uniquely be determined from this system of equations. To make this more specific let $U$ be the $n$-dimensional space spanned by the columns of $\mathbf{X}$ and the $p-n$-dimensional space $V$ be orthogonal complement of $U$, i.e. $V = U^{\perp}$. Then, $\mathbf{X} \mathbf{v} = \mathbf{0}_{p}$ for all $\mathbf{v} \in V$. So, $V$ is the non-trivial null space of $\mathbf{X}$. Consequently, as $\mathbf{X}^{\top} \mathbf{X} \mathbf{v} =
\mathbf{X}^{\top} \mathbf{0}_{p} = \mathbf{0}_{n}$, the solution of the normal equations is:

!bt
\begin{align*}
\hat{\beta} & =  ( \mathbf{X}^{\top} \mathbf{X})^{-} \mathbf{X}^{\top} \mathbf{Y} + \mathbf{v} \qquad \mbox{for all } \mathbf{v} \in V,
\end{align*}
!et

where $\mathbf{A}^{-}$ denotes the Moore-Penrose inverse of the matrix $\mathbf{A}$, which is defined as:

!bt
\begin{align*}
\mathbf{A}^{-} & =  \sum_{j=1}^p \nu_j^{-1} \, I_{\{ \nu_j \not= 0 \} } \, \mathbf{v}_j \, \mathbf{v}_j^{\top}.
\end{align*}
!et

The solution of the normal equations is thus only determined up to an
element from a non-trivial space $V$, and there is no unique estimator
of the regression parameter.

To obtain an estimate of the regression parameter $\beta$ when
$\mathbf{X}$ is (close to) super-collinearity, cite{Hoer1970} proposed
an ad-hoc fix to resolve the (almost) singularity of
$\mathbf{X}^{\top} \mathbf{X}$. Simply replace $\mathbf{X}^{\top}
\mathbf{X}$ by $\mathbf{X}^{\top} \mathbf{X} + \lambda
\mathbf{I}_{pp}$ with $\lambda \in [0, \infty)$. The scalar $\lambda$
is a tuning parameter, henceforth called the *penalty parameter*.



Recall the super-collinear design matrix $\mathbf{X}$ of Example ref{example.supercollinearity}. Then, for (say) $\lambda = 1$:
!bt
\begin{align*}
\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} & =  \left(
\begin{array}{rrr}
5 & 2 & 	2
\\
2 & 7 & -4
\\
2 & -4  & 7
\end{array} \right).
\end{align*}
!et
The eigenvalues of this matrix are 11, 7, and 1. Hence, $\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}$ has no zero eigenvalue and its inverse is well-defined.

With the ad-hoc fix for the singularity of $\mathbf{X}^{\top} \mathbf{X}$, cite{Hoer1970} proceed to define the *ridge regression estimator*:


!bt
\begin{align} label{form.ridgeRegressionEstimator}
\hat{\beta}(\lambda) & =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y},
\end{align}
!et

for $\lambda \in [0, \infty)$. Clearly, this is - for $\lambda$
strictly positive - a well-defined estimator, even if $\mathbf{X}$ is
high-dimensional. However, each choice of $\lambda$ leads to a
different ridge regression estimate. The set of all ridge regression
estimates $\{ \hat{\beta}(\lambda) \, : \, \lambda \in [0, \infty) \}$
is called the *solution* or *regularization path* of the ridge
estimator.




Recall the super-collinear design matrix $\mathbf{X}$ of Example ref{example.supercollinearity}. Suppose that the corresponding response vector is $\mathbf{Y} = (1.3, -0.5, 2.6, 0.9)^{\top}$.	
The ridge regression estimates for, e.g. $\lambda = 1, 2$, and $10$ are then:
\begin{align*}
\hat{\beta}(1) & =   (0.614, 0.548, 0.066)^{\top},
\\
\hat{\beta}(2) & =   (0.537, 0.490, 0.048)^{\top},
\\
\hat{\beta}(10) & =    (0.269, 0.267, 0.002)^{\top}.
\end{align*}
The full solution path of the ridge estimator is plotted in Figure ref{fig.ridgeSolPathPlusVar}.


Having obtained an estimate of the regression parameter $\beta$, one can define the fit $\widehat{\mathbf{Y}}$. It is defined analogous to the standard case:

!bt
\begin{align*}
\widehat{\mathbf{Y}}(\lambda) & =  \mathbf{X} \hat{\beta}(\lambda)
\, \, \, = \, \, \, \mathbf{X} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\, \, \, := \, \, \, \mathbf{H}(\lambda) \mathbf{Y}.
\end{align*}
!et

Previously, when using the ML estimator, the fit could be understood as a projection of $\mathbf{Y}$ onto the subspace spanned by the columns of $\mathbf{X}$. The fit $\widehat{\mathbf{Y}}(\lambda)$ corresponding to the ridge estimator is not a projection of $\mathbf{Y}$ onto $\mathbf{X}$ (confer Exercise ref{question.ridgeResidualsProjection} $\!$a). Consequently, the `ridge residuals' $\mathbf{Y} - \widehat{\mathbf{Y}}(\lambda)$ are not orthogonal to the fit $\widehat{\mathbf{Y}}(\lambda)$ (confer Exercise ref{question.ridgeResidualsProjection} $\!$b).


======= Eigenvalue shrinkage =======

The effect of the ridge penalty may also studied from the perspective of singular values. Let the singular value decomposition of the $(n \times p)$-dimensional design matrix $\mathbf{X}$ be:

!bt
\begin{align*}
\mathbf{X} & =  \mathbf{U}_x \mathbf{D}_x \mathbf{V}_x^{\top},
\end{align*}
!et

where $\mathbf{D}_x$ an $(n \times n)$-dimensional diagonal matrix
with the singular values, $\mathbf{U}_x$ an $(n \times n)$-dimensional
matrix with columns containing the left singular vectors (denoted
$\mathbf{u}_i$), and $\mathbf{V}_x$ a $(p \times n)$-dimensional
matrix with columns containing the right singular vectors (denoted
$\mathbf{v}_i$). The columns of $\mathbf{U}_x$ and $\mathbf{V}_x$ are
orthogonal: $\mathbf{U}_x^{\top} \mathbf{U}_x = \mathbf{I}_{nn} =
\mathbf{V}_x^{\top} \mathbf{V}_x$.

The OLS estimator can then be rewritten in terms of the SVD-matrices as:


\begin{align*}
\hat{\beta} & =  (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =  (\mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{U}_x \mathbf{D}_x \mathbf{V}_x^{\top})^{-1} \mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y}
\\
& =  (\mathbf{V}_x \mathbf{D}_x^2 \mathbf{V}_x^{\top})^{-1} \mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y}
\\
& =  \mathbf{V}_x \mathbf{D}_x^{-2} \mathbf{V}_x^{\top}  \mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y}
\\
& =  \mathbf{V}_x \mathbf{D}_x^{-2}  \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y},
\end{align*}
where $\mathbf{D}_x^{-2}  \mathbf{D}_x$  is not simplified further to emphasize the effect of the ridge penalty. Similarly, the ridge estimator can be rewritten in terms of the SVD-matrices as:
\begin{align*}
\hat{\beta} & =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =  (\mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{U}_x \mathbf{D}_x \mathbf{V}_x^{\top}  + \lambda \mathbf{I}_{pp})^{-1} \mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y}
\\
& =  (\mathbf{V}_x \mathbf{D}_x^2 \mathbf{V}_x^{\top} + \lambda \mathbf{V}_x  \mathbf{V}_x^{\top})^{-1} \mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y}
\\
& =  \mathbf{V}_x (\mathbf{D}_x^{2} + \lambda \mathbf{I}_{nn})^{-1} \mathbf{V}_x^{\top}  \mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y}
\\
& =  \mathbf{V}_x (\mathbf{D}_x^{2} + \lambda \mathbf{I}_{nn})^{-1}  \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y}.
\end{align*}
Combining the two results and writing $(\mathbf{D}_x)_{jj} = d_{x,jj}$ we have:
\begin{align*}
d_{x,jj}^{-1} & \geq & \frac{d_{x,jj}}{d_{x,jj}^2 + \lambda} \qquad \mbox{ for all } \lambda > 0.
\end{align*}
Thus, the ridge penalty shrinks the singular values.
\\
\\
Return to the problem of the super-collinearity of $\mathbf{X}$ in the high-dimensional setting ($p > n$). The super-collinearity implies the singularity of $\mathbf{X}^{\top} \mathbf{X}$ and prevents the calculation of the OLS estimator of the regression coefficients. However, $\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}$ is non-singular, with inverse:
\begin{align*}
(\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} & =  \sum_{j=1}^p (d_{x,jj}^2 + \lambda)^{-1} \mathbf{v}_j \mathbf{v}_j^{\top}.
\end{align*}
The right-hand side is well-defined for $\lambda > 0$.


===== Principal components regression =====

Principal component regression is a close relative to ridge regression that can also be applied in a high-dimensional context. Principal components regression explains the response not by the covariates themselves but by linear combinations of the covariates as defined by the principal components of $\mathbf{X}$. Let $\mathbf{U} \mathbf{D} \mathbf{V}^{\top}$ be the singular value decomposition of $\mathbf{X}$. The $i$-th principal component of $\mathbf{X}$ is then $\mathbf{X} \mathbf{v}_i$, henceforth denoted $\mathbf{z}_i$. Let $\mathbf{Z}_k$ be the matrix of the first $k$ principal components, i.e. $\mathbf{Z}_k = \mathbf{X} \mathbf{V}_k$ where $\mathbf{V}_k$ contains the first $k$ right singular vectors as columns. Principal components regression then amounts to regressing the response $\mathbf{Y}$ onto $\mathbf{Z}_{k}$, that is, it fits the model $\mathbf{Y} = \mathbf{Z}_k \ggamma + \vvarepsilon$. The least squares estimator of $\ggamma$ then is (with some abuse of notation):
\begin{align*}
\hat{\ggamma} & =  (\mathbf{Z}_k^{\top} \mathbf{Z}_k)^{-1} \mathbf{Z}_k^{\top} \mathbf{Y} \, \, \, = \, \, \, (\mathbf{V}_k^{\top} \mathbf{X}^{\top} \mathbf{X} \mathbf{V}_k)^{-1} \mathbf{V}_k^{\top} \mathbf{X}^{\top} \mathbf{Y}
\\
& =  (\mathbf{V}_k^{\top} \mathbf{V} \mathbf{D} \mathbf{U}^{\top} \mathbf{U} \mathbf{D} \mathbf{V}^{\top} \mathbf{V}_k)^{-1} \mathbf{V}_k^{\top} \mathbf{V} \mathbf{D} \mathbf{U}^{\top} \mathbf{Y}
\\
& =  (\mathbf{I}_{kn} \mathbf{D}^2 \mathbf{I}_{nk})^{-1} \mathbf{I}_{kn} \mathbf{D} \mathbf{U}^{\top} \mathbf{Y}
\\
& =  \mathbf{D}_k^{-2} \widetilde{\mathbf{D}}_k \mathbf{U}^{\top} \mathbf{Y} \, \, \, = \, \, \ \widetilde{\mathbf{D}}_k^{-1} \mathbf{U}^{\top} \mathbf{Y},
\end{align*}
where $\mathbf{D}_k$ and $\widetilde{\mathbf{D}}_k$ are submatrices of $\mathbf{D}$. The matrix $\mathbf{D}_k$ is obtained from $\mathbf{D}$ by removal of the last $n-p$  rows and columsn, while for $\widetilde{\mathbf{D}}_k$ only the last $n-k$ rows are dropped. Similarly, $\mathbf{I}_{kn}$ and $\mathbf{I}_{nk}$ are obtained from $\mathbf{I}_{nn}$ by removal of the last $n-k$ rows and columns, respectively. The principal component regression estimator of $\beta$ then is $\hat{\beta}_{\mbox{{\tiny pcr}}} = \mathbf{V}_k \widetilde{\mathbf{D}}_k^{-1} \mathbf{U}^{\top} \mathbf{Y}$. When $k$ is set equal to the column rank of $\mathbf{X}$, and thus to the rank of $\mathbf{X}^{\top} \mathbf{X}$, the
principal component regression estimator $\hat{\beta}_{\mbox{{\tiny pcr}}} = (\mathbf{X}^{\top} \mathbf{X})^- \mathbf{X}^{\top} \mathbf{Y}$, where $\mathbf{A}^-$ denotes the Moore-Penrose inverse of matrix $\mathbf{A}$. 

The relation between ridge and principal component regression becomes clear when their corresponding estimators are written in terms of the singular value decomposition of $\mathbf{X}$:
\begin{align*}
\hat{\beta}_{\mbox{{\tiny pcr}}} & =  \mathbf{V}_x (\mathbf{I}_{nk} \mathbf{D}_x \mathbf{I}_{kn})^{-1} \mathbf{U}_x^{\top} \mathbf{Y},
\\
\hat{\beta} (\lambda) & =  \mathbf{V}_x (\mathbf{D}_x^2 + \lambda \mathbf{I}_{nn})^{-1} \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{Y}.
\end{align*}
Both operate on the singular values of the design matrix. But where principal component regression thresholds the singular values of $\mathbf{X}$, ridge regression shrinks them (depending on their size). Hence, one applies a discrete map on the singular values while the other a continuous one.





======= Moments =======

The first two moments of the ridge regression estimator are derived. Next the performance of the ridge regression estimator is studied in terms of the  mean squared error, which combines the first two moments.


===== Expectation =====

The left panel of Figure ref{fig.ridgeSolPathPlusVar} shows ridge estimates of the regression parameters converging to zero as the penalty parameter tends to infinity. This behaviour of the ridge estimator does not depend on the specifics of the data set. To see this study the expectation of the ridge estimator:
\begin{align*}
\mathbb{E} \big[ \hat{\beta}(\lambda) \big] & =  \mathbb{E} \big[ (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y} \big]
\\
& =  \mathbb{E} \big[ (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X}) (\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{X}^{\top} \mathbf{Y} \big]
\\
& =  \mathbb{E} \big[ (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X}) \, \hat{\beta} \big]
\\
& =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X}) \, \mathbb{E} (  \hat{\beta} )
\\
& =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X}) \, \beta.
\end{align*}
Clearly, $\mathbb{E} \big[ \hat{\beta}(\lambda) \big] \not= \beta$ for any $\lambda > 0$. Hence, the ridge estimator is biased.

From the expression above it is clear that the expectation of the ridge estimator vanishes as $\lambda$ tends to infinity:
\begin{align*}
\lim_{\lambda \rightarrow \infty} \mathbb{E} \big[ \hat{\beta}(\lambda) \big] & =  \lim_{\lambda \rightarrow \infty}  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X}) \, \beta \, \, \, = \, \, \, \mathbf{0}_{p}.
\end{align*}
Hence, all regression coefficients are shrunken towards zero as the penalty parameter increases. This also holds for $\mathbf{X}$ with $p > n$. Furthermore, this behaviour is not strictly monotone in $\lambda$: $\lambda_{a} > \lambda_b$ does not necessarily imply  $|\hat{\beta}_j (\lambda_a) | < |\hat{\beta}_j (\lambda_b) |$. Upon close inspection this can be witnessed from the ridge solution path of $\beta_3$ in Figure ref{fig.ridgeSolPathPlusVar}.


\begin{example} *Orthonormal design matrix* label{example.orthoronormalDesign}
\\
Consider an orthonormal design matrix $\mathbf{X}$, i.e.:
\begin{align*}
\mathbf{X}^{\top} \mathbf{X} & =  \mathbf{I}_{pp} \, \, \, = \, \, \, (\mathbf{X}^{\top} \mathbf{X})^{-1}.
\end{align*}
An example of an orthonormal design matrix would be:
\begin{align*}
\mathbf{X} & =  \frac{1}{2} \left(
\begin{array}{rr}
-1 & -1
\\
-1 & 1
\\
1 & -1
\\
1 & 1
\end{array} \right).
\end{align*}
This design matrix is orthonormal as $\mathbf{X}^{\top} \mathbf{X} = \mathbf{I}_{22}$, which is easily verified:
\begin{align*}
\mathbf{X}^{\top} \mathbf{X} & =  \frac{1}{4}
\left(
\begin{array}{rrrr}
-1 & -1 & 1 & 1
\\
-1 & 1 & -1 & 1
\end{array} \right)
\left(
\begin{array}{rr}
-1 & -1
\\
-1 & 1
\\
1 & -1
\\
1 & 1
\end{array} \right) \, \, \, = \, \, \,
\frac{1}{4}
\left(
\begin{array}{rr}
4 & 0
\\
0 & 4
\end{array} \right) \, \, \, = \, \, \, \mathbf{I}_{22}.
\end{align*}
In case of an orthonormal design matrix the relation between the OLS and ridge estimator is:
\begin{align*}
\hat{\beta}(\lambda) & =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\, \, \, = \, \, \, (\mathbf{I}_{pp} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =  (1 + \lambda)^{-1} \mathbf{I}_{pp} \mathbf{X}^{\top} \mathbf{Y}
\qquad \, \, = \, \, \, (1 + \lambda)^{-1} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =   (1 + \lambda)^{-1} \hat{\beta}.
\end{align*}
Hence, the ridge estimator scales the OLS estimator by a factor. When taking the expectation on both sides, it is evident that the ridge estimator converges to zero as $\lambda \rightarrow \infty$.
\end{example}



===== Variance =====

As for the ML estimate of the regression parameter $\beta$ of model (ref{form.linRegressionModelinMatrix}), we derive the second moment of the ridge estimator. Hereto define:
\begin{align*}
\mathbf{W}_{\lambda} & =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{X}.
\end{align*}
Using $\mathbf{W}_{\lambda}$ the ridge estimator $\hat{\beta}(\lambda)$ can be expressed as $\mathbf{W}_{\lambda} \hat{\beta}$ for:
\begin{align*}
\mathbf{W}_{\lambda} \hat{\beta} & =  \mathbf{W}_{\lambda} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =  \{ (\mathbf{X}^{\top} \mathbf{X})^{-1}   [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ] \}^{-1}  (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =   [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1} \mathbf{X}^{\top} \mathbf{X}  (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =   [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =   \hat{\beta}(\lambda).
\end{align*}
The linear operator $\mathbf{W}_{\lambda}$ thus transforms the ML estimator of the regression parameter into the ridge estimator.

It is now easily seen that:
\begin{align*}
\mbox{Var}[ \hat{\beta}(\lambda) ] & =  \mbox{Var}[ \mathbf{W}_{\lambda} \hat{\beta} ] \qquad \qquad \, \, \, \, \, \, = \, \, \,   \mathbf{W}_{\lambda}  \mbox{Var}[\hat{\beta} ]  \mathbf{W}_{\lambda}^{\top}
\\
& =   \sigma^2 \mathbf{W}_{\lambda}  (\mathbf{X}^{\top} \mathbf{X})^{-1}  \mathbf{W}_{\lambda}^{\top} \, \, \, = \, \, \,  \sigma^2 [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1}  \mathbf{X}^{\top} \mathbf{X} \{ [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1}\}^{\top},
\end{align*}
in which we have used $\mbox{Var}(\mathbf{A} \mathbf{Y}) = \mathbf{A} \mbox{Var}( \mathbf{Y}) \mathbf{A}^{\top}$ for a non-random matrix $\mathbf{A}$, the fact that $\mathbf{W}_{\lambda}$ is non-random, and $ \mbox{Var}[\hat{\beta} ] = \sigma^2 (\mathbf{X}^{\top} \mathbf{X})^{-1}$.


Like the expectation the variance of the ridge estimator vanishes as $\lambda$ tends to infinity:
\begin{align*}
\lim_{\lambda \rightarrow \infty} \mbox{Var} \big[ \hat{\beta}(\lambda) \big] & =  \lim_{\lambda \rightarrow \infty}  \sigma^2 \mathbf{W}_{\lambda}  (\mathbf{X}^{\top} \mathbf{X})^{-1}  \mathbf{W}_{\lambda}^{\top} \, \, \, = \, \, \, \mathbf{0}_{pp}.
\end{align*}
Hence, the variance of the ridge regression coefficient estimates decreases towards zero as the penalty parameter becomes large. This is illustrated in the right panel of Figure ref{fig.ridgeSolPathPlusVar} for the data of Example ref{example.supercollinearity}.


With an explicit expression of the variance of the ridge estimator at hand, we can compare it to that of the OLS estimator:
\begin{align*}
\mbox{Var}[ \hat{\beta} ] - \mbox{Var}[ \hat{\beta}(\lambda) ] & =  \sigma^2 [(\mathbf{X}^{\top} \mathbf{X})^{-1} - \mathbf{W}_{\lambda}  (\mathbf{X}^{\top} \mathbf{X})^{-1}  \mathbf{W}_{\lambda}^{\top} ]
\\
& =  \sigma^2 \mathbf{W}_{\lambda}  \{ [\mathbf{I} + \lambda (\mathbf{X}^{\top} \mathbf{X})^{-1} ] (\mathbf{X}^{\top} \mathbf{X})^{-1}  [\mathbf{I} + \lambda (\mathbf{X}^{\top} \mathbf{X})^{-1} ]^{\top} - (\mathbf{X}^{\top} \mathbf{X})^{-1} \} \mathbf{W}_{\lambda}^{\top}
\\
& =  \sigma^2 \mathbf{W}_{\lambda}   [ 2 \, \lambda \, (\mathbf{X}^{\top} \mathbf{X})^{-2} + \lambda^2 (\mathbf{X}^{\top} \mathbf{X})^{-3} ] \mathbf{W}_{\lambda}^{\top}
\\
& =  \sigma^2 [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1}   [ 2 \, \lambda \, \mathbf{I}_{pp} + \lambda^2 (\mathbf{X}^{\top} \mathbf{X})^{-1} ] \{ [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1}\}^{\top}.
\end{align*}
The difference is non-negative definite as each component in the matrix product is non-negative definite. Hence, the variance of the ML estimator exceeds (in the positive definite ordering) that of the ridge estimator:

!bt

!bt
\begin{align} label{form.VarInequalityMLandRidge}
\mbox{Var}[ \hat{\beta} ] & \succeq & \mbox{Var}[ \hat{\beta}(\lambda) ],
\end{align}
!et
!et
with the inequality being strict if $\lambda > 0$. In other words, the variance of the ML estimator is larger than that of the ridge estimator (in the sense that their difference is non-negative definite). The variance inequality (ref{form.VarInequalityMLandRidge}) can be interpreted in terms of the stochastic behaviour of the estimator. This is illustrated by the next example.

# original latex figure with scale=0.45, angle=0

FIGURE: [varMLellipses.eps, width=400 frac=1.0] Level sets of the distribution of the ML (left panel) and ridge (right panel) regression estimators.} label{fig.varOLSandRidge
#  \afterpage{}


\begin{example} *Variance comparison*
\\
Consider the design matrix:
\begin{align*}
\mathbf{X} & =  \left(
\begin{array}{rr}
-1 & 2
\\
0 & 1
\\
2  & -1
\\
1  & 0
\end{array} \right).
\end{align*}
The variances of the ML and ridge (with $\lambda=1$) estimates of the regression coefficients then are:
\begin{align*}
\mbox{Var}(\hat{\beta}) & =  \sigma^2 \left(
\begin{array}{rr}
0.3 & 0.2
\\
0.2 & 0.3
\end{array} \right)
\qquad \mbox{and} \qquad
\mbox{Var}[\hat{\beta}(\lambda)] \, \, \, = \, \, \, \sigma^2 \left(
\begin{array}{rr}
0.1524 & 0.0698
\\
0.0698 & 0.1524
\end{array} \right).
\end{align*}
These variances can be used to construct levels sets of the distribution of the estimates. The level sets that contain 50\%, 75\% and 95\% of the distribution of the ML and ridge estimates are plotted in Figure ref{fig.varOLSandRidge}. In line with inequality (ref{form.VarInequalityMLandRidge}) the level sets of the ridge estimate are smaller than that of the ML estimate: it thus varies less.
\end{example}




\begin{contexample}_ref{example.orthoronormalDesign_} *Orthonormal design matrix (continued)*
\\
Assume the design matrix $\mathbf{X}$ is orthonormal. Then, $\mbox{Var}[ \hat{\beta} ] = \sigma^2 \mathbf{I}_{pp}$ and
\begin{align*}
\mbox{Var}[ \hat{\beta}(\lambda) ] & =  \sigma^2 \mathbf{W}_{\lambda}  (\mathbf{X}^{\top} \mathbf{X})^{-1}  \mathbf{W}_{\lambda}^{\top}
\, \, \,  = \, \, \,  \sigma^2 [\mathbf{I}_{pp} + \lambda \mathbf{I}_{pp} ]^{-1}   \mathbf{I}_{pp} \{ [\mathbf{I}_{pp} + \lambda \mathbf{I}_{pp} ]^{-1} \}^{\top}
\, \, \, = \, \, \, \sigma^2 (1 + \lambda )^{-2}   \mathbf{I}_{pp} .
\end{align*}
As the penalty parameter $\lambda$ is non-negative the former exceeds the latter. In particular, this expression vanishes as $\lambda \rightarrow \infty$.
\end{contexample}

# #if FORMAT in ("latex", "pdflatex")
\noindent
# #endif
The full distribution of the ridge regression estimator is now known. The estimator, $\hat{\beta}(\lambda) = (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}$  is a linear estimator, linear in $\mathbf{Y}$. As $\mathbf{Y}$ is normally distributed, so is $\hat{\beta}(\lambda)$. Moreover, the normal distribution is fully characterized by its  first two moments, which are available. Hence:
\begin{align*}
\hat{\beta}(\lambda) & \sim & \mathcal{N} \big( (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{X} \, \beta,  \sigma^2 [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1}  \mathbf{X}^{\top} \mathbf{X} \{ [  \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1}\}^{\top} \big).
\end{align*}
Given $\lambda$ and $\beta$, the random behavior of the estimator is thus known.



===== Mean squared error} label{sect:ridgeMSE =====
Previously, we motivated the ridge estimator as an ad hoc solution to collinearity. An alternative motivation comes from studying the Mean Squared Error (MSE) of the ridge regression estimator: for a suitable choice of $\lambda$ the ridge regression estimator may outperform the ML regression estimator in terms of the MSE. Before we prove this, we first derive the MSE of the ridge estimator and quote some auxiliary results.

Recall that (in general) for any estimator of a parameter $\theta$:
\begin{align*}
\mbox{MSE}( \hat{\theta} ) & =  \mathbb{E} [ ( \hat{\theta} - \theta)^2 ]
\, \, \, = \, \, \,  \mbox{Var}( \hat{ \theta} ) + [\mbox{Bias} ( \hat{\theta} )]^2.
\end{align*}
Hence, the MSE is a measure of the quality of the estimator.

The MSE of the ridge estimator is:

!bt

!bt
\begin{align}
\mbox{MSE}[\hat{\beta}(\lambda)] & =  \mathbb{E} [ (\mathbf{W}_{\lambda} \, \hat{\beta} - \beta)^{\top} \, (\mathbf{W}_{\lambda} \, \hat{\beta} - \beta) ]  \nonumber
\\
& =  \mathbb{E} ( \hat{\beta}^{\top} \mathbf{W}_{\lambda}^{\top} \,\mathbf{W}_{\lambda} \, \hat{\beta} ) - \mathbb{E} ( \beta^{\top}  \, \mathbf{W}_{\lambda} \, \hat{\beta}) - \mathbb{E} ( \hat{\beta}^{\top} \mathbf{W}_{\lambda}^{\top} \, \beta) + \mathbb{E} ( \beta^{\top} \beta)  \nonumber
\\
& =  \mathbb{E} ( \hat{\beta}^{\top} \mathbf{W}_{\lambda}^{\top} \,\mathbf{W}_{\lambda} \, \hat{\beta} ) - \mathbb{E} ( \beta^{\top} \, \mathbf{W}_{\lambda}^{\top} \mathbf{W}_{\lambda} \, \hat{\beta}) - \mathbb{E} ( \hat{\beta}^{\top} \mathbf{W}_{\lambda}^{\top} \, \mathbf{W}_{\lambda} \beta) + \mathbb{E} ( \beta^{\top} \mathbf{W}_{\lambda}^{\top} \,\mathbf{W}_{\lambda} \, \beta )  \nonumber
\\
& & - \mathbb{E} ( \beta^{\top} \mathbf{W}_{\lambda}^{\top} \,\mathbf{W}_{\lambda} \, \beta ) + \mathbb{E} ( \beta^{\top} \, \mathbf{W}_{\lambda}^{\top} \mathbf{W}_{\lambda} \, \hat{\beta}) + \mathbb{E} ( \hat{\beta}^{\top} \mathbf{W}_{\lambda}^{\top} \, \mathbf{W}_{\lambda} \beta)  \nonumber
\\
& &  - \mathbb{E} ( \beta^{\top}  \, \mathbf{W}_{\lambda} \, \hat{\beta}) - \mathbb{E} ( \hat{\beta}^{\top} \mathbf{W}_{\lambda}^{\top} \, \beta) + \mathbb{E} ( \beta^{\top} \beta)  \nonumber
\\
& =  \mathbb{E} [ ( \hat{\beta} - \beta )^{\top} \mathbf{W}_{\lambda}^{\top} \, \mathbf{W}_{\lambda} \, (\hat{\beta} - \beta) ]  \nonumber
\\
& & - \beta^{\top} \mathbf{W}_{\lambda}^{\top} \,\mathbf{W}_{\lambda} \, \beta  + \beta^{\top} \, \mathbf{W}_{\lambda}^{\top} \mathbf{W}_{\lambda} \, \beta + \beta^{\top} \mathbf{W}_{\lambda}^{\top} \, \mathbf{W}_{\lambda} \beta  \nonumber
\\
& &  - \beta^{\top}  \, \mathbf{W}_{\lambda} \, \beta -  \beta^{\top} \mathbf{W}_{\lambda}^{\top} \, \beta + \beta^{\top} \beta  \nonumber
\\
& =  \mathbb{E} \big\{ ( \hat{\beta} - \beta )^{\top} \mathbf{W}_{\lambda}^{\top} \, \mathbf{W}_{\lambda} \, (\hat{\beta} - \beta) \big\} +  \beta^{\top} (\mathbf{W}_{\lambda} - \mathbf{I}_{pp})^{\top} (\mathbf{W}_{\lambda} - \mathbf{I}_{pp}) \, \beta \nonumber
\\
& =  \sigma^2 \, \mbox{tr}\big\{ \mathbf{W}_{\lambda} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{W}_{\lambda}^{\top} \big\} + \beta^{\top} (\mathbf{W}_{\lambda} - \mathbf{I}_{pp})^{\top} (\mathbf{W}_{\lambda} - \mathbf{I}_{pp}) \, \beta. label{form.ridgeMSE}
\end{align}
!et
!et
In the last step we have used $\hat{\beta} \sim  \mathcal{N}( \beta, \sigma^2 \, [\mathbf{X}^{\top} \mathbf{X}]^{-1} )$ and the expectation of the quadratic form of a multivariate random variable $\vvarepsilon \sim \mathcal{N}(\mmu_{\varepsilon}, \SSigma_{\varepsilon})$ for some nonrandom symmetric positive definite matrix $\LLambda$ is (cf. \citealt{Math1992}):
\begin{align*}
\mathbb{E} ( \vvarepsilon^{\top} \, \LLambda \, \vvarepsilon) & =  \mbox{tr} ( \LLambda \, \SSigma_{\varepsilon}) + \mmu_{\varepsilon}^{\top} \, \LLambda \, \mmu_{\varepsilon},
\end{align*}
of course replacing $\vvarepsilon$ by $\hat{\beta}$ in this expectation. The first summand in the final derived expression for $\mbox{MSE}[\hat{\beta}(\lambda)]$ is the sum of the variances of the ridge estimator, while the second summand can be thought of the ``squared bias'' of the ridge estimator. In particular, $\lim_{\lambda \rightarrow \infty} \mbox{MSE}[\hat{\beta}(\lambda)] =  \beta^{\top} \beta$, which is the squared biased for an estimator that equals zero (as does the ridge estimator in the limit).



\begin{example} *Orthonormal design matrix*
\\
Assume the design matrix $\mathbf{X}$ is orthonormal. Then, $\mbox{MSE}[ \hat{\beta} ]  =  p \, \sigma^2$ and
\begin{align*}
\mbox{MSE}[ \hat{\beta}(\lambda) ] & =  \frac{p \, \sigma^2}{ (1+ \lambda)^{2}} +  \frac{\lambda^2}{ (1+ \lambda)^{2}} \beta^{\top}  \beta.
\end{align*}
The latter achieves its minimum at: $\lambda = p \sigma^2 / \beta^{\top}  \beta$.
\end{example}



The following theorem and proposition are required for the proof of the main result.

\begin{theorem} *(Theorem 1 of \citealp{Theo1974*}) label{theo.Theobald1}
\\
Let $\hat{\ttheta}_1$ and $\hat{\ttheta}_2$ be (different) estimators of $\ttheta$ with second order moments:
\begin{align*}
\mathbf{M}_k & =  \mathbb{E} [ (\hat{\ttheta}_k - \ttheta) (\hat{\ttheta}_k - \ttheta)^{\top} ] \qquad \mbox{for } k=1,2,
\end{align*}
and
\begin{align*}
\mbox{MSE}(\hat{\ttheta}_k) & =  \mathbb{E} [ (\hat{\ttheta}_k - \ttheta)^{\top} \mathbf{A} (\hat{\ttheta}_k - \ttheta) ] \qquad \mbox{for } k=1,2,
\end{align*}
where $\mathbf{A} \succeq 0$. Then, $\mathbf{M}_1 - \mathbf{M}_2 \succeq 0$ if and only if $\mbox{MSE}(\hat{\ttheta}_1) - \mbox{MSE}(\hat{\ttheta}_2) \geq 0$ for all $\mathbf{A} \succeq 0$.
\end{theorem}


\begin{proposition} *(\citealp{Fare1976*}) label{prop.Farebrother}
\\
Let $\mathbf{A}$ be a $(p \times p)$-dimensional, positive definite matrix, $\mathbf{b}$ be a nonzero $p$ dimensional vector, and $c \in \mathbb{R}_+$. Then, $c \mathbf{A} - \mathbf{b} \mathbf{b}^{\top} \succ 0$ if and only if $\mathbf{b}^{\top} \mathbf{A}^{-1} \mathbf{b} > c$.
\end{proposition}

We are now ready to proof the main result, formalized as Theorem ref{theo.Theobald2}, that for some $\lambda$ the ridge regression estimator yields a lower MSE than the ML regression estimator.

\begin{theorem} *(Theorem 2 of \citealp{Theo1974*}) label{theo.Theobald2}
\\
There exists $\lambda > 0$ such that $\mbox{MSE}[\hat{\beta}(\lambda)] < \mbox{MSE}[\hat{\beta}(0)] = \mbox{MSE}[\hat{\beta}]$.
\end{theorem}

\begin{proof}
The second order moment matrix of the ridge estimator is:
\begin{align*}
\mathbf{M} (\lambda) & := & \mathbb{E} [ (\hat{\beta}(\lambda) - \beta) (\hat{\beta} (\lambda) - \beta)^{\top} ]
\\
& =  \mathbb{E} \{ \hat{\beta}(\lambda) [\hat{\beta}(\lambda)]^{\top} \}  - \mathbb{E} [ \hat{\beta}(\lambda) ] \{ \mathbb{E} [ \hat{\beta}(\lambda) ] \}^{\top} + \mathbb{E} [\hat{\beta} (\lambda) - \beta)] \{ \mathbb{E} [\hat{\beta} (\lambda) - \beta)] \}^{\top}
\\
& =  \mbox{Var}[ \hat{\beta}(\lambda) ] + \mathbb{E} [\hat{\beta} (\lambda) - \beta)] \{ \mathbb{E} [\hat{\beta} (\lambda) - \beta)] \}^{\top}.
\end{align*}
Then:
\begin{align*}
\mathbf{M} ( 0 ) - \mathbf{M}(\lambda) & =  \sigma^2 (\mathbf{X}^{\top} \mathbf{X})^{-1} - \sigma^2 \mathbf{W}_{\lambda}  (\mathbf{X}^{\top} \mathbf{X})^{-1}  \mathbf{W}_{\lambda}^{\top} ]
\\
& &  - (\mathbf{W}_{\lambda} - \mathbf{I}_{pp}) \beta \beta^{\top} (\mathbf{W}_{\lambda} -\mathbf{I}_{pp})^{\top}
\\
& =  \sigma^2 \mathbf{W}_{\lambda}   [ 2 \, \lambda \, (\mathbf{X}^{\top} \mathbf{X})^{-2} + \lambda^2 (\mathbf{X}^{\top} \mathbf{X})^{-3} ] \mathbf{W}_{\lambda}^{\top}
\\
& & - \lambda^2 [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}]^{-1} \beta \beta^{\top} \{ [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}]^{-1} \}^{\top}
\\
& =  \sigma^2  [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}]^{-1}  [ 2 \, \lambda \, \mathbf{I}_{pp} + \lambda^2 (\mathbf{X}^{\top} \mathbf{X})^{-1} ] \{ [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}]^{-1} \}^{\top}
\\
& & - \lambda^2 [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}]^{-1} \beta \beta^{\top} \{ [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}]^{-1} \}^{\top}
\\
& =   \lambda [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}]^{-1}  [ 2 \, \sigma^2  \, \mathbf{I}_{pp} + \lambda \sigma^2 (\mathbf{X}^{\top} \mathbf{X})^{-1}  - \lambda \beta \beta^{\top}  ] \{ [ \mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}]^{-1} \}^{\top}.
\end{align*}
This is positive definite if and only if $ 2 \, \sigma^2  \, \mathbf{I}_{pp} + \lambda \sigma^2 (\mathbf{X}^{\top} \mathbf{X})^{-1}  - \lambda \beta \beta^{\top}  \succ 0$. Hereto it suffices to show that $2 \, \sigma^2  \, \mathbf{I}_{pp} - \lambda \beta \beta^{\top} \succ 0$. By Proposition ref{prop.Farebrother} this holds for $\lambda$ such that $2 \sigma^2 (\beta^{\top} \beta)^{-1} > \lambda$. For these $\lambda$, we thus have $\mathbf{M} ( 0 ) - \mathbf{M}(\lambda)$. Application of Theorem ref{theo.Theobald1} now concludes the proof.
\end{proof}
This result of cite{Theo1974} is generalized by cite{Fare1976} to the class of design matrices $\mathbf{X}$ with $\mbox{rank}(\mathbf{X}) < p$.
\\
\\
Theorem ref{theo.Theobald2} can be used to illustrate that the ridge regression estimator strikes a balance between the bias and variance. This is illustrated in the left panel of Figure ref{fig.MSEridge}. For small $\lambda$, the variance of the ridge estimator dominates the MSE. This may be understood when realizing that in this domain of $\lambda$ the ridge estimator is close to the unbiased ML regression estimator. For large $\lambda$, the variance vanishes and the bias dominates the MSE. For small enough values of $\lambda$, the decrease in variance of the ridge regression estimator exceeds the increase in its bias.   As the MSE is the sum of these two, the MSE first decreases as $\lambda$ moves away from zero. In particular, as $\lambda = 0$ corresponds to the ML regression estimator, the ridge regression estimator yields a lower MSE for these values of $\lambda$. In the right panel of Figure ref{fig.MSEridge} $\mbox{MSE}[ \hat{\beta}(\lambda)] < \mbox{MSE}[ \hat{\beta}(0)]$ for $\lambda < 7$ (roughly) and the ridge estimator outperforms the ML estimator.
# original latex figure with scale=0.45, angle=0

FIGURE: [MSE_ridge.eps, width=400 frac=1.0] Left panel: mean squared error, and its `bias' and `variance' parts, of the ridge regression estimator (for artificial data). Right panel: mean squared error of the ridge and ML estimator of the regression coefficient vector (for the same artificial data).} label{fig.MSEridge
#  \afterpage{}
\\
\\
# #if FORMAT in ("latex", "pdflatex")
\noindent
# #endif
Besides another motivation behind the ridge regression estimator, the use of Theorem ref{theo.Theobald2} is limited. The optimal choice of $\lambda$ depends on the quantities $\beta$ and $\sigma^2$. These are unknown in practice. Then, the penalty parameter is chosen in a data-driven fashion by means of cross-validation (see Section ref{subsect.crossvalidation}).
\\
\\
\begin{remark} \mbox{ }
\\
Theorem ref{theo.Theobald2} can also be used to conclude on the biasedness of the ridge regression estimator. The Gauss-Markov theorem \citep{Rao1973} states (under some assumptions) that the ML regression estimator is the best linear unbiased estimator (BLUE) with the smallest MSE. As the ridge regression estimator is a linear estimator and outperforms (in terms of MSE) this ML estimator, it must be biased (for it would otherwise refute the Gauss-Markov theorem).
\end{remark}




======= Constrained estimation} label{sect.constrainedEstimation =======
The ad-hoc fix of cite{Hoer1970} to super-collinearity of the design matrix (and, consequently the singularity of the matrix $\mathbf{X}^{\top} \mathbf{X}$) has been motivated post-hoc.
The ridge estimator minimizes the *ridge loss function*, which is defined as:

!bt

!bt
\begin{align}
\mathcal{L}_{\mbox{{\footnotesize ridge}}}(\beta; \lambda) & =  \| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2 + \lambda \| \beta \|^2_2 
\, \, \,  = \, \, \, \sum_{i=1}^n (Y_i - \mathbf{X}_{i\ast} \, \beta)^2 + \lambda \sum_{j=1}^p \beta_j^2. label{form.ridgeLossFunction}
\end{align}
!et
!et
This loss function is the traditional sum-of-squares augmented with a *penalty*. The particular form of the penalty, $\lambda \| \beta \|^2_2$ is referred to as the *ridge penalty* and $\lambda$ as the *penalty parameter*. For $\lambda=0$, minimization of the ridge loss function yields the ML estimator. For any $\lambda > 0$, the ridge penalty contributes to the loss function, affecting its minimum and its location. The minimum of the sum-of-squares is well-known. The minimum of the ridge penalty is attained at $\beta = \mathbf{0}_{p}$ whenever $\lambda > 0$. The $\beta$ that minimizes $\mathcal{L}_{\mbox{{\footnotesize ridge}}}(\beta; \lambda)$ then balances the sum-of-squares and the penalty. The effect of the penalty in this balancing act is to shrink the regression coefficients towards zero, its minimum. In particular, the larger $\lambda$, the larger the contribution of the penalty to the loss function, the stronger the tendency to shrink non-zero regression coefficients to zero (and decrease the contribution of the penalty to the loss function). This motivates the name `penalty' as non-zero elements of $\beta$ increase (or penalize) the loss function.


To verify that the ridge estimator indeed minimizes the ridge loss function, proceed as usual. Take the derivative with respect to $\beta$:
\begin{align*}
\frac{\partial}{\partial \beta} \mathcal{L}_{\mbox{{\footnotesize ridge}}}(\beta; \lambda) & =  -2 \, \mathbf{X}^{\top} (\mathbf{Y} - \mathbf{X} \beta)  + 2 \, \lambda \, \mathbf{I}_{pp} \, \beta \, \, \, = \, \, \, -2 \, \mathbf{X}^{\top} \mathbf{Y} + 2 \, ( \mathbf{X}^{\top} \mathbf{X} + \lambda \, \mathbf{I}_{pp}) \beta.
\end{align*}
Equate the derivative to zero and solve for $\beta$. This yields the ridge regression estimator.

The ridge estimator is thus a stationary point of the ridge loss function. A stationary point corresponds to a minimum if the Hessian matrix with second order partial derivatives is positive definite. The Hessian of the ridge loss function is
\begin{align*}
\frac{\partial^2}{\partial \beta \, \partial \beta^{\top}} \mathcal{L}_{\mbox{{\footnotesize ridge}}}(\beta; \lambda) & =  2 \, ( \mathbf{X}^{\top} \mathbf{X} + \lambda \, \mathbf{I}_{pp}).
\end{align*}
This Hessian is the sum of the semi-positive definite matrix $\mathbf{X}^{\top} \mathbf{X}$ and the positive definite matrix $\lambda \, \mathbf{I}_{pp}$. Lemma 14.2.4 of cite{Harv2008} then states that the sum of these matrices is itself a positive definite matrix. Hence, the Hessian is positive definite and the ridge loss function has a stationary point at the ridge estimator, which is a minimum.

The ridge regression estimator minimizes the ridge loss function. It rests to verify that it is a global minimum. To this end we introduce the concept of a convex function. As a prerequisite, a set $\mathcal{S} \subset \mathbb{R}^p$ is called *convex* if for all $\beta_1, \beta_2 \in \mathcal{S}$ their weighted average $\beta_{\theta}  =  (1 - \theta) \beta_1 + \theta \beta_2$ for all $\theta \in [0, 1]$ is itself an element of $\mathcal{S}$, thus $\beta_{\theta} \in \mathcal{S}$. If for all $\theta \in (0, 1)$, the weighted average $\beta_{\theta}$ is inside $\mathcal{S}$ and not on its boundary, the set is called *strict convex*. Examples of (strict) convex and nonconvex sets are depicted in Figure ref{fig.ridgeAsConstrainedEst}. A function $f(\cdot)$ is *(strict) convex* if the set $\{ y \, : \, y \geq f(\beta) \mbox{ for all } \beta \in \mathcal{S} \mbox{ for any convex } \mathcal{S} \}$, called the epigraph of $f(\cdot)$, is (strict) convex. Examples of (strict) convex and nonconvex functions are depicted in Figure ref{fig.ridgeAsConstrainedEst}. The ridge loss function is the sum of two parabola's: one at least convex and the other a strict convex function in $\beta$. The sum of convex and strict convex function is itself strict convex (confer Lemma 9.4.2 of \citealt{Flet2008}). The ridge loss function is thus strict convex. Theorem 9.4.1 of \citealt{Flet2008} then warrants, by the strict convexity of the ridge loss function, that the ridge estimator is a global minimum.
\\
\\
From the ridge loss function the limiting behavior of the variance of the ridge regression estimator can be understood. The ridge penalty with its minimum $\beta = \mathbf{0}_{p}$ does not involve data and, consequently, the variance of its minimum equals zero. With the ridge regression being a compromise between the ML estimator and the minimum of the penalty, so is its variance a compromise of their variances. As $\lambda$ tends to infinity, the ridge estimator and its variance converge to the minimum and the variance of the minimum, respectively. Hence, in the limit (large $\lambda$) the variance of the ridge regression estimator vanishes. Understandably, as the penalty now fully dominates the loss function and, consequently, it does no longer involve data (i.e. randomness).
# original latex figure with scale=0.38, angle=0

FIGURE: [convexSets.eps, width=400 frac=1.0] Top panels show examples of convex (left) and nonconvex (right) sets. Middle panels show examples of convex (left) and nonconvex (right) functions.  The left bottom panel illustrates the ridge estimation as a constrained estimation problem. The ellipses represent the contours of the ML loss function, with the blue dot at the center the ML estimate. The circle is the ridge parameter constraint. The red dot is the ridge estimate. It is at the intersection of the ridge constraint and the smallest contour with a non-empty intersection with the constraint. The right bottom panel shows the data corresponding to Example ref{exam.overfitting}. The grey line represents the `true' relationship, while the black line the fitted one. } label{fig.ridgeAsConstrainedEst
\afterpage{}
\\
\\
Above it has been shown that the ridge estimator can be defined as:

!bt

!bt
\begin{align} label{form.ridgeEstViaPenEst}
\hat{\beta}(\lambda) & =  \arg \min_{\beta} \| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2 + \lambda \| \beta \|^2_2.
\end{align}
!et
!et
This minimization problem can be reformulated into the following constrained optimization problem (illustrated in Figure ref{fig.ridgeAsConstrainedEst}):

!bt

!bt
\begin{align} label{form.constrEstProblemRidge}
\hat{\beta}(\lambda) & =   \arg \min_{\| \beta \|_2^2 \leq c} \| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2,
\end{align}
!et
!et
for some suitable $c > 0$. The constrained optimization problem  (ref{form.constrEstProblemRidge}) can be solved by means of the Karush-Kuhn-Tucker (KKT) multiplier method, which minimizes a function subject to inequality constraints. The KKT multiplier method states that, under some regularity conditions (all met here), there exists a constant $\nu \geq 0$, called the *multiplier*, such that the solution $\hat{\beta}(\nu)$ of the constrained minimization problem (ref{form.constrEstProblemRidge}) satisfies the so-called KKT conditions. The first KKT condition (referred to as the stationarity condition) demands that the gradient (with respect to $\beta$) of the Lagrangian associated with the minimization problem equals zero at the solution $\hat{\beta}(\nu)$. The Lagrangian for problem (ref{form.constrEstProblemRidge}) is:
\begin{align*}
\| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2 + \nu ( \| \beta \|^2_2 - c).
\end{align*}
The second KKT condition (the complementarity condition) requires that $\nu (\| \hat{\beta}(\nu) \|_2^2 - c) = 0$. If $\nu = \lambda$ and $c = \| \hat{\beta}(\lambda) \|_2^2$, the ridge estimator $\beta (\lambda)$ satisfies both KKT conditions. Hence, both problems have the same solution when  $c = \| \hat{\beta}(\lambda) \|_2^2$.
\\
\\
The relevance of viewing the ridge regression estimator as the solution to a constrained estimation problem becomes obvious when considering a typical threat to high-dimensional data analysis: overfitting. *Overfitting* refers to the phenomenon of modelling the noise rather than the signal. In case the true model is parsimonious (few covariates driving the response) and data on many covariates are available, it is likely that a linear combination of all covariates yields a higher likelihood than a combination of the few that are actually related to the response. As only the few covariates related to the response contain the signal, the model involving all covariates then cannot but explain more than the signal alone: it also models the error. Hence, it overfits the data. In high-dimensional settings overfitting is a real threat. The number of explanatory variables exceeds the number of observations. It is thus possible to form a linear combination of the covariates that perfectly explains the response, including the noise.

Large estimates of regression coefficients are often an indication of overfitting. Augmentation of the estimation procedure with a constraint on the regression coefficients is a simple remedy to large parameter estimates. As a consequence it decreases the probability of overfitting. Overfitting is illustrated in the next example.


\begin{example} *(Overfitting)* label{exam.overfitting}
\\
Consider an artificial data set comprising of ten observations on a response $Y_i$ and nine covariates $X_{i,j}$. All covariate data are sampled from the standard normal distribution: $X_{i,j} \sim \mathcal{N}(0, 1)$. The response is generated by $Y_i = X_{i,1} + \varepsilon_i$ with $\varepsilon_{i} \sim \mathcal{N}(0, 1/4)$. Hence, only the first covariate contributes to the response.

The regression model $Y_i = \sum_{j=1}^9  X_{i,j} \beta_j+ \varepsilon_i$ is fitted to the artificial data using `R`. This yields the regression parameter estimates:
\begin{align*}
\hat{\beta}^{\top} & =  (0.048, -2.386, -5.528, 6.243, -4.819, 0.760, -3.345, -4.748, 2.136).
\end{align*}
As $\beta^{\top} = (1, 0, \ldots, 0)$, many regression coefficient are clearly over-estimated.

The fitted values $\widehat{Y}_i = \mathbf{X}_i \hat{\beta}$ are plotted against the values of the first covariates in the right bottom panel of Figure ref{fig.ridgeAsConstrainedEst}. As a reference the line $x=y$ is added, which represents the `true' model. The fitted model follows the `true' relationship. But it also captures the deviations from this line that represent the errors.
\end{example}



======= Bayesian regression} label{sect:Bayes =======
Ridge regression has a close connection to Bayesian linear regression. Bayesian linear regression assumes the parameters $\beta$ and $\sigma^2$ to be the random variables, while at the same time considering $\mathbf{X}$ and $\mathbf{Y}$ as fixed. Within the regression context, the conjugate priors of $\beta$ and $\sigma^2$ are:
\begin{align*}
\beta \, | \, \sigma^2 \sim \mathcal{N}(\mathbf{0}_p,  \sigma^2 \lambda^{-1} \mathbf{I}_{pp}) \qquad \mbox{and} \qquad  \sigma^2 \sim \mathcal{IG}(\alpha_0, \beta_0),
\end{align*}
where $\mathcal{IG}$ denotes the inverse Gamma distribution with shape parameter $\alpha_0$ and scale parameter $\beta_0$. The penalty parameter can be interpreted as the precision of the prior, determining how informative the prior should be. A smaller penalty (i.e. precision) corresponds to a wider prior, and a larger penalty to a more informative, concentrated prior (Figure ref{fig.ridgePriorOfBeta}).
# original latex figure with scale=0.45, angle=0

FIGURE: [ridgePriorOfBeta.eps, width=400 frac=1.0] Conjugate prior of the regression parameter $\beta$ for various choices of $\lambda$, the penalty parameters c.q. precision.} label{fig.ridgePriorOfBeta

Under the assumption of the conjugate priors above, the joint posterior distribution of $\beta$ and $\sigma^2$ is then:
\begin{align*}
f_{\beta, \sigma^2} (\beta, \sigma^2 \, | \, \mathbf{Y}, \mathbf{X}) & =  f_Y (\mathbf{Y} \, | \, \mathbf{X}, \beta, \sigma^2) \, f_{\beta}(\beta | \sigma^2) \, f_{\sigma}(\sigma^2)
\\
& \propto & \sigma^{-n} \exp \Big[ - \frac{1}{2\sigma^2} ( \mathbf{Y} - \mathbf{X} \beta)^{\top} ( \mathbf{Y} - \mathbf{X} \beta) \Big]
\\
&  & \times \, \, \sigma^{-p} \exp \Big[ - \frac{1}{2\sigma^2} \lambda \beta^{\top} \beta \Big] \, \times \, \, [\sigma^2]^{-\alpha_0-1} \exp \Big[ - \frac{\beta_0}{2\sigma^2} \Big].
\end{align*}
As
\begin{align*}
& & \hspace{-1.5cm} ( \mathbf{Y} - \mathbf{X} \beta)^{\top} ( \mathbf{Y} - \mathbf{X} \beta) + \lambda \beta^{\top} \beta
\\
& =  \mathbf{Y}^{\top} \mathbf{Y} -
\beta^{\top} \mathbf{X}^{\top} \mathbf{Y} - \mathbf{Y}^{\top} \mathbf{X} \beta + \beta^{\top} \mathbf{X}^{\top} \mathbf{X} \beta + \lambda \beta^{\top} \beta
\\
& =  \mathbf{Y}^{\top} \mathbf{Y} - \beta^{\top} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& & - \, \mathbf{Y}^{\top} \mathbf{X} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) \beta + \beta^{\top} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) \beta
\\
& =  \mathbf{Y}^{\top} \mathbf{Y} - \beta^{\top} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})
\hat{\beta} (\lambda)
\\
& & - \, [ \hat{\beta} (\lambda) ]^{\top} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) \beta + \beta^{\top} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) \beta
\\
& =  \mathbf{Y}^{\top} \mathbf{Y} - \mathbf{Y}^{\top} \mathbf{X} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& & + \, \big[ \beta - \hat{\beta}(\lambda) \big]^{\top} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) \big[ \beta - \hat{\beta}(\lambda) \big],
\end{align*}
the posterior distribution can be rewritten to:
\begin{align*}
f_{\beta, \sigma^2} (\beta, \sigma^2 \, | \, \mathbf{Y}, \mathbf{X})
& \propto & g_{\beta} (\beta \, | \, \sigma^2, \mathbf{Y}, \mathbf{X}) \,
g_{\sigma^2} (\sigma^2 \, | \, \mathbf{Y}, \mathbf{X})
\end{align*}
with
\begin{align*}
g_{\beta} (\beta \, | \, \sigma^2, \mathbf{Y}, \mathbf{X})
& \propto & \exp \Big\{ - \frac{1}{2\sigma^2} \big[ \beta - \hat{\beta}(\lambda) \big]^{\top} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) \big[ \beta - \hat{\beta}(\lambda) \big] \Big\}.
\end{align*}
Then, clearly the conditional posterior mean of $\beta$ is $\mathbb{E}(\beta \, | \, \sigma^2, \mathbf{Y}, \mathbf{X}) = \hat{\beta}(\lambda)$. Hence, the ridge regression estimator can be viewed as the Bayesian posterior mean estimator of $\beta$ when imposing a Gaussian prior on the regression parameter. 

With little extra work we may also obtain the conditional posterior of $\sigma^2$ from the joint posterior distribution:
\begin{align*}
f_{\sigma^2} (\sigma^2 \, | \, \beta, \mathbf{Y}, \mathbf{X}) & \propto & (\sigma^2)^{-[(n+p)/2 + \alpha_0 + 1]}
\exp [ - \frac{1}{2\sigma^2} ( \| \mathbf{Y} - \mathbf{X} \beta \|_2^2  +
\lambda \| \beta \|_2^2 + \beta_0)  ],
\end{align*}
in which one can recognize the shape of an inverse gamma distribution. 
\\
\\
A Bayesian estimator of a parameter $\ttheta$ is the estimator that minimizes the Bayes risk over a prior distribution of the parameter $\ttheta$. The Bayes risk is defined as $\int_{\ttheta} \mathbb{E} [(\hat{\ttheta} - \ttheta)^{\top} (\hat{\ttheta} - \ttheta)] \pi_{\ttheta}(\ttheta; \aalpha) d\ttheta$, where $\pi_{\ttheta}(\ttheta; \aalpha)$ is the prior distribution of $\ttheta$ with hyperparameter $\aalpha$. It is thus a weighted average of the Mean Squared Error, with weights specified through the prior. The Bayes risk is minimized by the mean posterior $\mathbb{E}_{\ttheta}(\ttheta \, | \,  \mbox{data})$  (cf., e.g., \citealp{Bijm2017}). The Bayesian estimator of $\ttheta$ thus yields the smallest possible expected MSE, under the assumption of the employed prior.

The Bayes risk of the ridge estimator over the normal prior $\beta \sim \mathcal{N}(\mathbf{0}_p, \sigma^2 \lambda^{-1} \mathbf{I}_{pp})$ is:
\begin{align*}
\mathbb{E}_{\beta} \{\mbox{MSE}[\hat{\beta}(\lambda)]
\, | \, \sigma^2, \mathbf{Y}, \mathbf{X} \}  & =  \sigma^2 \, \mbox{tr}\big\{ \mathbf{W}_{\lambda} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{W}_{\lambda}^{\top} \big\} + \mathbb{E}_{\beta} [ \beta^{\top} (\mathbf{W}_{\lambda} - \mathbf{I}_{pp})^{\top} (\mathbf{W}_{\lambda} - \mathbf{I}_{pp}) \, \beta]
\\
& =  \sigma^2 \, \big\{ \mbox{tr} \big[ \mathbf{W}_{\lambda} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \, \mathbf{W}_{\lambda}^{\top} \big] +
\lambda^{-1} \mbox{tr} [(\mathbf{W}_{\lambda} - \mathbf{I}_{pp})^{\top} (\mathbf{W}_{\lambda} - \mathbf{I}_{pp})] \big\}
\\
& =  \sigma^2 \sum_{j=1}^p (d_{jj}^2 + \lambda)^{-1},
\end{align*}
in which we have used *i)* the previously derived explicit expression (ref{form.ridgeMSE}) of the ridge estimator's MSE, *ii)* the expectation of the quadratic form of a multivariate random variable \citep{Math1992}, *iii)* the singular value decomposition of $\mathbf{X}$ with singular values $d_{jj}$, and *iv)* the fact that the trace of a square matrix equals the sum of its eigenvalues. As the ridge estimator coincides with the posterior mean, this is the minimal achievable MSE under a zero-centered normal prior with an uncorrelated and equivariant covariance matrix.

Above the Bayes risk of the ridge estimator factorizes with respect to $\sigma^2$ and $\lambda$. Hence, the larger the hyperparameter $\lambda$ the lower the Bayes risk of the ridge estimator. In particular, its Bayes risk converges to zero as $\lambda \rightarrow \infty$. This can be understood as follows. The limit corresponds to an infinite precision of the prior, thus reducing the variance contribution to the MSE. Moreover, as the ridge estimator shrinks towards zero and the prior distribution of $\beta$ has a zero mean, the bias too vanishes as $\lambda \rightarrow \infty$.

The calculation of the Bayes risk above relates the Bayesian and frequentist statements on the MSE of the ridge estimator. For the latter revisit Theorem ref{theo.Theobald2} of Section ref{sect:ridgeMSE}, which states the existence of a $\lambda$ such that the resulting ridge estimator has a superior MSE over that of the ML estimator. This result made no assumption on (the distribution of) $\beta$. In fact, it can be viewed as a statement of the MSE conditional on $\beta$. The Bayesian result integrates out the uncertainty - specified by the prior - in $\beta$ from the (frequentist's) conditional MSE to arrive at the unconditional MSE.



#  The Bayesian result states that the ridge regression estimator minimizes the weighted MSE (over a normal prior on the parameter $\beta$). It thus integrates out the uncertainty in $\beta$ from the (frequentist's) conditional MSE to arrive at the unconditional MSE.

======= Degrees of freedom =======

The degrees of freedom consumed by ridge regression is calculated. The degrees of freedom may be used in combination with an information criterion to decide on the value of the penalty parameter. Recall from ordinary regression that:
\begin{align*}
\widehat{\mathbf{Y}} & =  \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y} \, \, \, = \, \, \, \mathbf{H} \mathbf{Y},
\end{align*}
where $\mathbf{H}$ is the hat matrix. The degrees of freedom used in the regression is then equal to $\mbox{tr}(\mathbf{H})$, the trace of $\mathbf{H}$. In particular, if $ \mathbf{X}$ is of full rank, i.e. $\mbox{rank}(\mathbf{X}) = p$, then $\mbox{tr}(\mathbf{H}) = p$.

By analogy, the ridge-version of the hat matrix is:
\begin{align*}
\mathbf{H}(\lambda) & =  \mathbf{X} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top}.
\end{align*}
Continuing this analogy, the degrees of freedom of ridge regression is given by the trace of the ridge hat matrix $\mathbf{H}(\lambda)$:
\begin{align*}
\mbox{tr}[ \mathbf{H}(\lambda)] & =  \mbox{tr}[ \mathbf{X} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} ] \, \, \, = \, \, \, \sum_{j=1}^p \frac{d_{jj}^2}{d_{jj}^2 + \lambda}.
\end{align*}
The degrees of freedom consumed by ridge regression is monotone decreasing in $\lambda$. In particular:
\begin{align*}
\lim_{\lambda \rightarrow \infty} \mbox{tr}[ \mathbf{H}(\lambda)] & =  0.
\end{align*}
That is, in the limit no information from $\mathbf{X}$ is used. Indeed, $\beta$ is forced to equal $\mathbf{0}_{p}$ which is not derived from data.



======= Efficient calculation =======

In the high-dimensional setting the number of covariates $p$ is large compared to the number of samples $n$. In a microarray experiment $p = 40000$ and $n= 100$ is not uncommon. To perform ridge regression in this context, the following expression needs to be evaluated numerically:
\begin{align*}
(\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}.
\end{align*}
For $p=40000$ this requires the inversion of a $40000 \times 40000$ dimensional matrix. This is not feasible on most desktop computers. However, there is a workaround.

Revisit the singular value decomposition of $\mathbf{X} = \mathbf{U}_x \mathbf{D}_x \mathbf{V}_x^{\top}$ and write $\mathbf{R}_x = \mathbf{U}_x \mathbf{D}_x$. As both $\mathbf{U}_x$ and $\mathbf{D}_x$ are $(n \times n)$-dimensional matrices, so is $\mathbf{R}_x$. Consequently, $\mathbf{X}$ is now decomposed as $\mathbf{X} = \mathbf{R}_x  \mathbf{V}_x^{\top}$. The ridge estimator can be rewritten in terms of $\mathbf{R}_x$ and $\mathbf{V}_x$:
\begin{align*}
\hat{\beta}(\lambda) & =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =  (\mathbf{V}_x \mathbf{R}_x^{\top} \mathbf{R}_x \mathbf{V}_x^{\top}  + \lambda \mathbf{I}_{pp})^{-1} \mathbf{V}_x \mathbf{R}_x^{\top} \mathbf{Y}
\\
& =  (\mathbf{V}_x \mathbf{R}_x^{\top} \mathbf{R}_x \mathbf{V}_x^{\top} + \lambda \mathbf{V}_x  \mathbf{V}_x^{\top})^{-1} \mathbf{V}_x \mathbf{R}_x^{\top} \mathbf{Y}
\\
& =  \mathbf{V}_x (\mathbf{R}_x^{\top} \mathbf{R}_x  + \lambda \mathbf{I}_{nn})^{-1} \mathbf{V}_x^{\top}  \mathbf{V}_x \mathbf{R}_x^{\top} \mathbf{Y}
\\
& =  \mathbf{V}_x (\mathbf{R}_x^{\top} \mathbf{R}_x  + \lambda \mathbf{I}_{nn})^{-1} \mathbf{R}_x^{\top} \mathbf{Y}.
\end{align*}
Hence, the reformulated ridge estimator involves the inversion of an $(n \times n)$-dimensional matrix. With $n= 100$ this is feasible on most standard computers.

cite{Hast2004} point out that the number of computation operations reduces from $\mathcal{O}(p^3)$ to $\mathcal{O}(p n^2)$. In addition, they point out that this computational short-cut can be used in combination with other loss functions, for instance that of standard generalized linear models.
\\
\\
Avoidance of the inversion of the $(p \times p)$-dimensional matrix may be achieved in an other way. Hereto one needs the Woodbury identity. Let $\mathbf{A}$, $\mathbf{U}$ and $\mathbf{V}$ be $(p \times p)$-, $(p \times n)$- and $(n \times p)$-dimensional matrices, respectively.
The (simplified form of the) Woodbury identity then is:
\begin{align*}
(\mathbf{A} + \mathbf{U} \mathbf{V})^{-1} & =  \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{U} (\mathbf{I}_{nn} + \mathbf{V} \mathbf{A}^{-1} \mathbf{U})^{-1} \mathbf{V} \mathbf{A}^{-1}.
\end{align*}
Application of the Woodbury identity to the matrix inverse in the ridge estimator of the regression parameter gives:
\begin{align*}
(\lambda \mathbf{I}_{pp} + \mathbf{X}^{\top} \mathbf{X})^{-1} & =  \lambda^{-1} \mathbf{I}_{pp} - \lambda^{-2} \mathbf{X}^{\top} (\mathbf{I}_{nn} + \lambda^{-1} \mathbf{X} \mathbf{X}^{\top})^{-1} \mathbf{X}.
\end{align*}
This gives:
\begin{align*}
(\lambda \mathbf{I}_{pp} + \mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y} & =  \lambda^{-1} \mathbf{X}^{\top} \mathbf{Y} - \lambda^{-2} \mathbf{X}^{\top} (\mathbf{I}_{nn} + \lambda^{-1} \mathbf{X} \mathbf{X}^{\top})^{-1} \mathbf{X} \mathbf{X}^{\top}  \mathbf{Y}
\\
& =  \lambda^{-1} \mathbf{X}^{\top} \left[  \mathbf{Y} - \lambda^{-1} \mathbf{X}^{\top} (\mathbf{I}_{nn} + \lambda^{-1} \mathbf{X} \mathbf{X}^{\top})^{-1} \mathbf{X} \mathbf{Y} \right].
\end{align*}
The inversion of the $(p \times p)$-dimensional matrix $\lambda \mathbf{I}_{pp} + \mathbf{X}^{\top} \mathbf{X}$ is thus replaced by that of the $(n \times n)$-dimensional matrix $\mathbf{I}_{nn} + \lambda^{-1} \mathbf{X} \mathbf{X}^{\top}$. In addition, this expression of the ridge regression estimator avoids the singular value decomposition of $\mathbf{X}$, which may in some cases introduce additional numerical errors (e.g. at the level of machine precision).



======= Choice of the penalty parameter =======

Throughout the introduction of ridge regression and the subsequent discussion of its properties the penalty parameter is considered known or `given'. In practice, it is unknown and the user needs to make an informed decision on its value. Several strategies to facilitate such a decision are presented.

===== Information criterion =====

A popular strategy is to choose a penalty parameter that yields a good but parsimonious model. Information criteria measure the balance between model fit and model complexity. Here we present the Aikaike's information criterion (AIC), but many other criteria have been presented in the literature (e.g. \citealp{Akai1974}, \citealp{Schw1978}). The AIC measures model fit by the log-likelihood and model complexity is measured by the number of parameters used by the model. The number of model parameters in regular regression simply corresponds to the number of covariates in the model. Or, by the degrees of freedom consumed by the model, which is equivalent to the trace of the hat matrix. For ridge regression it thus seems natural to define model complexity
analogously by the trace of the ridge hat matrix. This yields the AIC for the linear regression model with ridge estimates:
\begin{align*}
\mbox{AIC}(\lambda) & =  2 \, p - 2 \log(\hat{L})
\\
& =  2 \, \mbox{tr} [\mathbf{H}(\lambda)] - 2 \log\{L[\hat{\beta}(\lambda), \hat{\sigma}^2(\lambda)]\}
\\
& =  2 \, \sum_{j=1}^p \frac{d_{jj}^2}{d_{jj}^2 + \lambda}
+ 2 n \, \log[\sqrt{2 \, \pi} \, \hat{\sigma}(\lambda)] +  \frac{1}{\hat{\sigma}^2(\lambda)} \sum_{i=1}^n [y_i - \mathbf{X}_{i, \ast} \, \hat{\beta}(\lambda)]^2.
\end{align*}
The value of $\lambda$ which minimizes $\mbox{AIC}(\lambda)$ corresponds to the `optimal' balance of model complexity and overfitting.

Information criteria guide the decision process when having to decide among various different models. Different models use different sets of explanatory variables to explain the behaviour of the response variable. In that sense, the use of information criteria for the deciding on the ridge penalty parameter may be considered inappropriate: ridge regression uses the same set of explanatory variables irrespective of the value of the penalty parameter.  Moreover, often ridge regression is employed to predict a response and not to provide an insightful explanatory model. The latter need not yield the best predictions. Finally, empirically we observe that the AIC often does not show an optimum *inside* the domain of the ridge penalty parameter. Henceforth, we refrain from the use of the AIC (or any of its relatives) in determining the optimal ridge penalty parameter.


===== Cross-validation} label{subsect.crossvalidation =====
Instead of choosing the penalty parameter to balance model fit with model complexity, cross-validation requires it (i.e. the penalty parameter) to yield a model with good prediction performance. Commonly, this performance is evaluated on novel data. Novel data need not be easy to come by and one has to make do with the data at hand. The setting of `original' and novel data is then mimicked by sample splitting: the data set is divided into two (groups of samples). One of these two data sets, called the *training set*, plays the role of `original' data on which the model is built. The second of these data sets, called the *test set*, plays the role of the `novel' data and is used to evaluate the prediction performance (often operationalized as the log-likelihood or the prediction error) of the model built on the training data set. This procedure (model building and prediction evaluation on training and test set, respectively) is done for a collection of possible penalty parameter choices. The penalty parameter that yields the model with the best prediction performance is to be preferred. The thus obtained performance evaluation depends on the actual split of the data set. To remove this dependence the data set is split many times into a training and test set. For each split the model parameters are estimated for all choices of $\lambda$ using the training data and estimated parameters are evaluated on the corresponding test set. The penalty parameter that on average over the test sets performs best (in some sense) is then selected.

When the repetitive splitting of the data set is done randomly,  samples may accidently end up in a fast majority of the splits in either training or test set. Such samples may have an unbalanced influence on either model building or prediction evaluation. To avoid this $k$-fold cross-validation structures the data splitting. The samples are divided into $k$ more or less equally sized exhaustive and mutually exclusive subsets. In turn (at each split) one of these subsets plays the role of the test set while the union of the remaining subsets constitutes the training set. Such a splitting warrants a balanced representation of each sample in both training and test set over the splits. Still the division into the $k$ subsets involves a degree of randomness. This may be fully excluded when choosing $k=n$. This particular case is referred to as leave-one-out cross-validation (LOOCV). For illustration purposes the LOOCV procedure is detailed fully below:
\begin{compactitem}
\item[0)] Define a range of interest for the penalty parameter.

\item[1)] Divide the data set into training and test set comprising samples $\{1, \ldots, n\} \setminus i$ and $\{ i \}$, respectively.

\item[2)] Fit the linear regression model by means of ridge estimation  for each $\lambda$ in the grid using the training set. This yields:
\begin{align*}
\hat{\beta}_{-i}(\lambda) & =  ( \mathbf{X}_{-i, \ast}^{\top}
\mathbf{X}_{-i, \ast} + \lambda \mathbf{I}_{pp})^{-1}
\mathbf{X}_{-i, \ast}^{\top} \mathbf{Y}_{-i}
\end{align*}
and the corresponding estimate of the error variance $\hat{\sigma}_{-i}^2(\lambda)$.

\item[3)] Evaluate the prediction performance of these models on the test set by $\log\{L[Y_i, \mathbf{X}_{i, \ast}; \hat{\beta}_{-i}(\lambda), \hat{\sigma}_{-i}^2(\lambda)]\}$. Or, by the prediction error $|Y_i - \mathbf{X}_{i, \ast} \hat{\beta}_{-i}(\lambda)|$, possibly squared.

\item[4)] Repeat steps 1) to 3) such that each sample plays the role of the test set once.

\item[5)] Average the prediction performances of the test sets at each grid point of the penalty parameter:
\begin{align*}
\frac{1}{n} \sum_{i = 1}^n \log\{L[Y_i, \mathbf{X}_{i, \ast}; \hat{\beta}_{-i}(\lambda), \hat{\sigma}_{-i}^2(\lambda)]\}.
\end{align*}
The quantity above is called the *cross-validated log-likelihood*. It is an estimate of the prediction performance of the model corresponding to this value of the penalty parameter on novel data.

\item[6)] The value of the penalty parameter that maximizes the cross-validated log-likelihood is the value of choice.
\end{compactitem}
The procedure is straightforwardly adopted to $k$-fold cross-validation, a different criterion, and different estimators.

In the LOOCV procedure above resampling can be avoided when the prediction performance is measured by Allen's PRESS (Predicted Residual Error Sum of Squares) statistic \citep{Alle1974}. For then, the LOOCV prediction performance can be expressed analytically in terms of the known quantities derived from the design matrix and response (as pointed out but not detailed in \citealt{Golu1979}). Define the optimal penalty parameter to minimize Allen's PRESS statistic:
\begin{align*}
\lambda_{\mbox{{\tiny opt}}} = \arg \min_{\lambda} \frac{1}{n} \sum_{i=1}^n [Y_i - \mathbf{X}_{i, \ast} \hat{\beta}_{-i}(\lambda)]^2.
\end{align*}
To derive an analytic expression for the right-hand side first
rewrite $(\mathbf{X}_{- i, \ast}^{\top} \mathbf{X}_{- i, \ast} + \lambda \mathbf{I}_{pp})^{-1}$ by means of the Woodbury identity as:
\begin{align*}
(\mathbf{X}_{- i, \ast}^{\top} \mathbf{X}_{- i, \ast} + \lambda \mathbf{I}_{pp})^{-1}   & =   (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp} - \mathbf{X}_{i, \ast}^{\top} \mathbf{X}_{i, \ast})^{-1}
\\
& =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
+ (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\mathbf{X}_{i, \ast}^{\top}  [ 1 - \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}  \mathbf{X}_{i, \ast}^{\top}]^{-1}
\\
& & \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\\
& =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
+ (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\mathbf{X}_{i, \ast}^{\top}  [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\end{align*}
with $\mathbf{H}_{ii}(\lambda) = \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}  \mathbf{X}_{i, \ast}^{\top}$.
Furthermore, $\mathbf{X}_{-i}^{\top} \mathbf{Y}_{-i} = \mathbf{X}^{\top} \mathbf{Y} - \mathbf{X}_{i, \ast}^{\top} Y_i$. Substitute both in the leave-one-out ridge regression estimator and manipulate:
\begin{align*}
\hat{\beta}_{- i}(\lambda) & =  (\mathbf{X}_{- i, \ast}^{\top} \mathbf{X}_{- i, \ast} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{- i, \ast}^{\top} \mathbf{Y}_{- i}
\\
& =  \{(\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
+ (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\mathbf{X}_{i, \ast}^{\top}  [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}\}
\\
& & \times (\mathbf{X}^{\top} \mathbf{Y} - \mathbf{X}_{i, \ast}^{\top} Y_i )
\\
& =  (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y} - (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top} Y_i
\\
& & + (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\mathbf{X}_{i, \ast}^{\top}  [ 1 - \mathbf{H}_{ii} (\lambda)]^{-1} \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\mathbf{X}^{\top} \mathbf{Y}
\\
& & - (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\mathbf{X}_{i, \ast}^{\top}  [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}
\mathbf{X}_{i, \ast}^{\top} Y_i
\\
& =  \hat{\beta}(\lambda) - (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top} [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} [ 1 - \mathbf{H}_{ii}(\lambda)] Y_i
\\
& & + (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top}  [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} \mathbf{X}_{i, \ast} \hat{\beta}(\lambda)
\\
& & - (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top}  [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} \mathbf{H}_{ii} (\lambda) Y_i
\\
& =  \hat{\beta}(\lambda) - (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top} [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} \{ [ 1 - \mathbf{H}_{ii}(\lambda)] Y_i -  \mathbf{X}_{i, \ast} \hat{\beta}(\lambda) + \mathbf{H}_{ii}(\lambda) Y_i \}
\\
& =  \hat{\beta}(\lambda) - (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top} [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} [ Y_i - \mathbf{X}_{i, \ast} \hat{\beta}(\lambda) ].
\end{align*}
The latter enables the reformulation of the prediction error as:
\begin{align*}
Y_i - \mathbf{X}_{i, \ast} \hat{\beta}_{-i}(\lambda) & =  Y_i - \mathbf{X}_{i, \ast} \{ \hat{\beta}(\lambda) - (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top} [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} [ Y_i - \mathbf{X}_{i, \ast} \hat{\beta}(\lambda)]  \}
\\
& =  Y_i - \mathbf{X}_{i, \ast} \hat{\beta}(\lambda) +  \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top} [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} [ Y_i - \mathbf{X}_{i, \ast} \hat{\beta}(\lambda) ]
\\
& =  Y_i - \mathbf{X}_{i, \ast} \hat{\beta}(\lambda) +  \mathbf{H}_{ii}  (\lambda)[ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} [ Y_i - \mathbf{X}_{i, \ast} \hat{\beta}(\lambda) ]
\\
& =  [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} [ Y_i - \mathbf{X}_{i, \ast}^{\top} \hat{\beta}(\lambda) ],
\end{align*}
which in turn results in the re-expression of Allen's PRESS statistic:
\begin{align*}
\lambda_{\mbox{{\tiny opt}}} & =  \arg \min_{\lambda} \frac{1}{n} \sum_{i=1}^n [Y_i - \mathbf{X}_{i, \ast} \hat{\beta}_{-i}(\lambda)]^2
\, \, \, = \, \, \, \arg \min_{\lambda}  \tfrac{1}{n} \| \mathbf{B}(\lambda) [\mathbf{I}_{nn} - \mathbf{H}(\lambda)] \mathbf{Y} \|_ F^2,
\end{align*}
where $\mathbf{B}(\lambda)$ is diagonal with $[\mathbf{B}(\lambda)]_{ii} = [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1}$. Hence, the prediction performance for a given $\lambda$ can be assessed directly from the ridge hat matrix and the response vector without the recalculation of the $n$ leave-one-out ridge estimators. Computationally, this is a considerable gain.



======= Simulations =======

Simulations are presented that illustrate properties of the ridge estimator not discussed explicitly in the previous sections of this chapter.

===== Role of the variance of the covariates} label{ridge:covariateVariances =====
In many applications of high-dimensional data the covariates are standardized prior to the execution of the ridge regression. Before we discuss whether this is appropriate, we first illustrate the effect of ridge penalization on covariates with distinct variances using simulated data.

The simulation involves one response to be (ridge) regressed on fifty covariates. Data (with $n=1000$) for the covariates, denoted $\mathbf{X}$, are drawn from a multivariate normal distribution: $\mathbf{X} \sim \mathcal{N}(\mathbf{0}_{50}, \mathbf{\Sigma})$ with $\mathbf{\Sigma}$ diagonal and $(\mathbf{\Sigma})_{jj} = j / 10$. From this the response is generated through $\mathbf{Y} = \mathbf{X} \beta + \vvarepsilon$ with $\beta = \mathbf{1}_{50}$ and $\vvarepsilon \sim \mathcal{N}(\mathbf{0}_{50}, \mathbf{I}_{50 \times 50})$.

With the simulated data at hand the ridge regression estimates of $\beta$ are evaluated for a large grid of the penalty parameter $\lambda$. The resulting ridge regularization paths of the regression coefficients are plotted (Figure ref{fig.effectOfRidge_effectOfVariances}). All paths start ($\lambda=0$) close to one and vanish as $\lambda \rightarrow \infty$. However, ridge regularization paths of regression coefficients corresponding to covariates with a large variance dominate those with a low variance.


# original latex figure with scale=0.45, angle=0

FIGURE: [covariatesWithDifferentVariance.eps, width=400 frac=1.0] Top panel: Ridge regularization paths for coefficients of the 50 uncorrelated covariates with distinct variances. Color and line type indicated the grouping of the covariates by their variance. Bottom panels: Graphical illustration of the effect of a covariate's variance on the ridge estimator. The grey circle depicts the ridge parameter constraint. The dashed black ellipsoids are the level sets of the least squares loss function. The red dot is the ridge regression estimate. Left and right panels represent the cases with equal and unequal, respectively, variances of the covariates.} label{fig.effectOfRidge_effectOfVariances





Ridge regression's preference of covariates with a large variance can intuitively be understood as follows. First note that the ridge regression estimator now can be written as:
\begin{align*}
\beta (\lambda) & =  [ \mbox{Var}(\mathbf{X}) + \lambda \mathbf{I}_{50 \times 50}]^{-1} \mbox{Cov}(\mathbf{X}, \mathbf{Y})
\\
& =  ( \SSigma + \lambda \mathbf{I}_{50 \times 50})^{-1} \SSigma [ \mbox{Var}(\mathbf{X}) ]^{-1} \mbox{Cov}(\mathbf{X}, \mathbf{Y})
\\
& =  ( \SSigma + \lambda \mathbf{I}_{50 \times 50})^{-1} \SSigma \beta.
\end{align*}
Plug in the employed parametrization of $\mathbf{\Sigma}$, which gives:
\begin{align*}
[\beta (\lambda)]_j & =  \frac{j}{j + 50 \lambda} \, (\beta)_j.
\end{align*}
Hence, the larger the covariate's variance (corresponding to the larger $j$), the larger its ridge regression coefficient estimate. Ridge regression thus prefers (among a set of covariates with comparable effect sizes) those with larger variances.



The reformulation of ridge penalized estimation as a constrained estimation problem offers a geometrical interpretation of this phenomenon. Let $p=2$ and the design matrix $\mathbf{X}$ be orthogonal, while both covariates contribute equally to the response. Contrast the cases with $\mbox{Var}(X_1) \approx \mbox{Var}(X_2)$ and $\mbox{Var}(X_1) \gg \mbox{Var}(X_2)$. The level sets of the least squares loss function associated with the former case are circular, while that of the latter are strongly ellipsoidal (see Figure ref{fig.effectOfRidge_effectOfVariances}). The diameters along the principal axes (that - due to the orthogonality of $\mathbf{X}$ - are parallel to that of the $\beta_1$- and $\beta_2$-axes) of both circle and ellipsoid are reciprocals of the variance of the covariates. When the variances of both covariates are equal, the level sets of the loss function expand equally fast along both axis. With the two covariates having the same regression coefficient, the point of these level sets closest to the parameter constraint is to be found on the line $\beta_1 = \beta_2$ (Figure ref{fig.effectOfRidge_effectOfVariances}, left panel). Consequently, the ridge regression estimate satisfies $\hat{\beta}_1 (\lambda) \approx \hat{\beta}_2(\lambda)$. With unequal variances between the covariates, the ellipsoidal level sets of the loss function have diameters of rather different sizes. In particular, along the $\beta_1$-axis it is narrow (as $\mbox{Var}(X_1)$ is large), and - vice versa - wide along the $\beta_2$-axis. Consequently, the point of these level sets closest to the circular parameter constraint will be closer to the $\beta_1$- than to the $\beta_2$-axis (Figure ref{fig.effectOfRidge_effectOfVariances}, left panel). For the ridge estimates of the regression parameter this implies $0 \ll \hat{\beta}_1 (\lambda) < 1$ and $0 < \hat{\beta}_2 (\lambda) \ll 1$. Hence, the covariate with a larger variance yields the larger ridge regression estimate.


Should one thus standardize the covariates prior to ridge regression analysis?  When dealing with gene expression data from microarrays, the data have been subjected to a series of pre-processing steps (e.g. quality control, background correction, within- and between-normalization). The purpose of these steps is to make the expression levels of genes comparable both within and between hybridizations. The preprocessing should thus be considered an inherent part of the measurement. As such it is to be done independently of whatever down-stream analysis is to follow and further tinkering with the data is preferably to be avoided (as it may mess up the `comparable-ness' of the expression levels as achieved by the preprocessing). For other data types different considerations may apply.

Among the considerations to decide on standardization of the covariates, one  should also include the fact that ridge estimates prior and posterior to scaling do not simply differ by a factor. To see this assume that the covariates have been centered. Scaling of the covariates amounts to post-multiplication of the design matrix by a $(p \times p)$-dimensional diagonal matrix $\mathbf{A}$ with the reciprocals of the covariates' scale estimates on its diagonal \citep{Sard2008}. Hence, the ridge estimator (for the rescaled data) is then given by:
\begin{align*}
\min_{\beta} \| \mathbf{Y} - \mathbf{X} \mathbf{A} \beta \|_2^2 + \lambda \| \beta \|_ 2^2.
\end{align*}
Apply the change-of-variable $\ggamma = \mathbf{A} \beta$ and obtain:
\begin{align*}
\min_{\ggamma} \| \mathbf{Y} - \mathbf{X} \gamma \|_2^2 + \lambda \| \mathbf{A}^{-1} \ggamma \|_ 2^2 & =  \min_{\beta} \| \mathbf{Y} - \mathbf{X} \gamma \|_2^2 + \sum_{j=1}^p \lambda [(\mathbf{A})_{jj}]^{-2} \gamma_j^2.
\end{align*}
Effectively, the scaling is equivalent to covariate-wise penalization. The `scaled' ridge estimator may then be derived along the same lines as before in Section ref{sect.constrainedEstimation}:
\begin{align*}
\hat{\beta}^{\mbox{{\tiny (scaled)}}} (\lambda) & =  \mathbf{A}^{-1} \hat{\ggamma} (\lambda)
\, \, \,  = \, \, \, \mathbf{A}^{-1} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{A}^{-2})^{-1} \mathbf{X}^{\top} \mathbf{Y}.
\end{align*}
In general, this is unequal to the ridge estimator without the rescaling of the columns of the design matrix. Moreover, it should be clear that $\hat{\beta}^{\mbox{{\tiny (scaled)}}} (\lambda) \not= \mathbf{A} \hat{\beta}(\lambda)$.

===== Ridge regression and collinearity} label{sect:collinearCovariates =====
Initially, ridge regression was motivated as an ad-hoc fix of (super)-collinear covariates in order to obtain a well-defined estimator. We now study the effect of this ad-hoc fix on the regression coefficient estimates of collinear covariates. In particular, their ridge regularization paths are contrasted to those of `non-collinear' covariates.

To this end, we consider a simulation in which one response is regressed on 50 covariates. The data of these covariates, stored in a design matrix denoted $\mathbf{X}$, are sampled from a multivariate normal distribution, with mean zero and a $5 \times 5$ blocked covariance matrix:
\begin{align*}
\mathbf{\Sigma} & =  \left(
\begin{array}{ccccc}
\mathbf{\Sigma}_{11} & \mathbf{0}_{10 \times 10} & \mathbf{0}_{10 \times 10}
& \mathbf{0}_{10 \times 10} & \mathbf{0}_{10 \times 10}
\\
\mathbf{0}_{10 \times 10} & \mathbf{\Sigma}_{22} & \mathbf{0}_{10 \times 10}
& \mathbf{0}_{10 \times 10} & \mathbf{0}_{10 \times 10}
\\
\mathbf{0}_{10 \times 10} & \mathbf{0}_{10 \times 10}
& \mathbf{\Sigma}_{33} & \mathbf{0}_{10 \times 10} & \mathbf{0}_{10 \times 10}
\\
\mathbf{0}_{10 \times 10} & \mathbf{0}_{10 \times 10}
& \mathbf{0}_{10 \times 10} & \mathbf{\Sigma}_{44} & \mathbf{0}_{10 \times 10}
\\
\mathbf{0}_{10 \times 10} & \mathbf{0}_{10 \times 10}
& \mathbf{0}_{10 \times 10} & \mathbf{0}_{10 \times 10} & \mathbf{\Sigma}_{55}
\end{array}
\right)
\end{align*}
with
\begin{align*}
\mathbf{\Sigma}_{kk} & =  \frac{k-1}{5} \, \mathbf{1}_{10 \times 10} + \frac{6-k}{5} \, \mathbf{I}_{10 \times 10}.
\end{align*}
The data of the response variable $\mathbf{Y}$ are then obtained through: $\mathbf{Y}  = \mathbf{X} \beta + \vvarepsilon$, with $\vvarepsilon \sim \mathcal{N}( \mathbf{0}_{n}, \mathbf{I}_{nn})$ and $\beta = \mathbf{1}_{50}$. Hence, all covariates contribute equally to the response. Would the columns of $\mathbf{X}$ be orthogonal, little difference in the ridge estimates of the regression coefficients is expected.

The results of this simulation study with sample size $n=1000$ are presented in Figure ref{fig.ridgeEstimates_correlatedCovariates}. All 50 regularization paths start close to one as $\lambda$ is small and converge to zero as $\lambda \rightarrow \infty$. But the paths of covariates of the same block of the covariance matrix $\mathbf{\Sigma}$ quickly group, with those corresponding to a block with larger off-diagonal elements above those with smaller ones. Thus, ridge regression prefers (i.e. shrinks less) coefficient estimates of strongly positively correlated covariates.

# original latex figure with scale=0.45, angle=0

FIGURE: [correlatedCovariates.eps, width=400 frac=1.0]  Left panel: Ridge regularization paths for coefficients of the 50 covariates, with various degree of collinearity but equal variance. Color and line type correspond to the five blocks of the covariate matrix $\mathbf{\Sigma}$. Right panel: Graphical illustration of the effect of the collinearity among  covariates on the ridge estimator. The solid and dotted grey circles depict the ridge parameter constraint for the collinear and orthogonal cases, respectively. The dashed black ellipsoids are the level sets of the sum-of-squares squares loss function. The red dot and violet diamond are the ridge regression for the positive collinear and orthogonal case, respectively.}  label{fig.ridgeEstimates_correlatedCovariates
#  \afterpage{}



Intuitive understanding of the observed behaviour may be obtained from the $p=2$ case. Let $U$, $V$ and $\varepsilon$ be independent random variables with zero mean. Define $X_1 = U + V$, $X_2 = U - V$, and $Y = \beta_1 X_1 + \beta_2 X_2 + \varepsilon$ with  $\beta_1$ and $\beta_2$ constants. Hence, $\mathbb{E}(Y) = 0$. Then:
\begin{align*}
#  Y & =  \beta_1 X_1 + \beta_2 X_2 + \varepsilon
#  \\
#  & =  \beta_1 (U + V)  + \beta_2 (U - V) + \varepsilon
#  \\
Y & =  (\beta_1 + \beta_2) U  + (\beta_1 - \beta_2) V + \varepsilon
\\
& =  \gamma_u U + \gamma_v V + \varepsilon
\end{align*}
and $\mbox{Cor}(X_{1}, X_{2}) = [\mbox{Var}(U) - \mbox{Var}(V)] / [ \mbox{Var}(U) + \mbox{Var}(V) ]$.
#  \begin{align*}
#  \mbox{Cor}(X_{1}, X_{2})
#  & =  \frac{ \mbox{Cov}(X_{1}, X_{2}) }{ \sqrt{\mbox{Var}(X_{1})} \sqrt{\mbox{Var}(X_{2})} }
#  \\
#  & =   \frac{ \mbox{Cov}(U + V, U - V) }{ \sqrt{\mbox{Var}(U + V)} \sqrt{\mbox{Var}(U - V)} }
#  \\
#  & = 
#  \frac{ \mbox{Var}(U) - \mbox{Var}(V) }{ \mbox{Var}(U) + \mbox{Var}(V) }.
#  \end{align*}
The random variables $X_1$ and $X_2$ are strongly positively correlated if  $\mbox{Var}(U) \gg \mbox{Var}(V)$.

The ridge regression estimator associated with regression of $Y$ on $U$ and $V$ is:
\begin{align*}
\ggamma(\lambda) & =  \left(
\begin{array}{rr}
\mbox{Var}(U) + \lambda & 0
\\
0 &  \mbox{Var}(V) + \lambda
\end{array}
\right)^{-1}
\left(
\begin{array}{r}
\mbox{Cov}(U, Y)
\\
\mbox{Cov}(V, Y)
\end{array}
\right).
\end{align*}
For large enough $\lambda$
\begin{align*}
\ggamma(\lambda)
#  & & \frac{1}{\lambda} \left( \begin{array}{rr} \mbox{Var}(U)  & 0 \\ 0 \mbox{Var}(V) \end{array} \right) \left( \begin{array}{rr} \mbox{Var}(U) & 0
#  \\
#  0 &  \mbox{Var}(V)
#  \end{array}
#  \right)^{-1}
#  \left(
#  \begin{array}{r}
#  \mbox{Cov}(U, Y)
#  \\
#  \mbox{Cov}(V, Y)
#  \end{array}
#  \right)
#  \\
& \approx  \frac{1}{\lambda}
\left(
\begin{array}{rr}
\mbox{Var}(U)  & 0
\\
0 &  \mbox{Var}(V)
\end{array}
\right)
\left(
\begin{array}{r}
\beta_1 + \beta_2
\\
\beta_1 - \beta_2
\end{array}
\right).
\end{align*}
When $\mbox{Var}(U) \gg \mbox{Var}(V)$ and $\beta_1 \approx \beta_2$, the ridge estimate of $\gamma_v$ vanishes for large $\lambda$. Hence, ridge regression prefers positively covariates with similar effect sizes.


This phenomenon too can be explained geometrically. For the illustration consider ridge estimation with $\lambda=1$ of the linear model $\mathbf{Y} = \mathbf{X} \beta + \vvarepsilon$ with $\beta = (3, 3)^{\top}$, $\vvarepsilon \sim \mathcal{N}(\mathbf{0}_2, \mathbf{I}_{22})$ and the columns of $\mathbf{X}$ strongly and positively collinear. The level sets of the sum-of-squares loss, $\| \mathbf{Y} - \mathbf{X} \beta \|_2^2$, are plotted in the right panel of Figure ref{fig.ridgeEstimates_correlatedCovariates}. Recall that the ridge estimate is found by looking for the smallest loss level set that hits the ridge contraint. The sought-for estimate is then the point of intersection between this level set and the constraint, and - for the case at hand - is on the $x=y$-line. This is no different from the case with orthogonal $\mathbf{X}$ columns. Yet their estimates differ, even though the same $\lambda$ is applied. The difference is to due to fact that the radius of the ridge constraint depends on $\lambda$, $\mathbf{X}$ and $\mathbf{Y}$. This is immediate from the fact that the radius of the constraint equals $\| \hat{\beta}(\lambda) \|_2^2$ (see Section ref{sect.constrainedEstimation}). To study the effect of $\mathbf{X}$ on the radius, we remove its dependence on $\mathbf{Y}$ by considering its expectation, which is:
\begin{align*}
\mathbb{E}[ \| \hat{\beta}(\lambda) \|_2^2 ] & =  \mathbb{E} \{ [(\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X}) \, \hat{\beta}]^{\top} \, (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} (\mathbf{X}^{\top} \mathbf{X}) \, \hat{\beta} \} \nonumber % \, \, \, = \, \, \, \mathbb{E} ( \hat{\beta}^{\top} \mathbf{W}_{\lambda}^{\top} \,\mathbf{W}_{\lambda} \, \hat{\beta} ) \nonumber
\\
#  & =  \mathbb{E} [ \mathbf{Y}^{\top} \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{W}_{\lambda}^{\top} \,\mathbf{W}_{\lambda} \, (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y} ] \nonumber
#  \\
#  & =  \mathbb{E} [ \mathbf{Y}^{\top} \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} (\mathbf{X}^{\top} \mathbf{X}) (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp}) (\mathbf{X}^{\top} \mathbf{X})(\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y} ] \nonumber
#  \\
& =  \mathbb{E} [ \mathbf{Y}^{\top} \mathbf{X} 
(\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-2} \mathbf{X}^{\top} \mathbf{Y} ] \nonumber
\\
& =  \sigma^2 \, \mbox{tr}\big\{ \mathbf{X} 
(\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-2} \mathbf{X}^{\top} \big\} + \beta^{\top} \mathbf{X}^{ \top} \mathbf{X} 
(\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-2} \mathbf{X}^{\top} \, \mathbf{X} \beta. % label{form.expectationPenalty}
\end{align*}
In the last step we have used $\mathbf{Y} \sim  \mathcal{N}( \mathbf{X} \beta, \sigma^2 \mathbf{I}_{pp})$ and the expectation of the quadratic form of a multivariate random variable $\vvarepsilon \sim \mathcal{N}(\mmu_{\varepsilon}, \SSigma_{\varepsilon})$ is $\mathbb{E} ( \vvarepsilon^{\top} \, \LLambda \, \vvarepsilon)  = \mbox{tr} ( \LLambda \, \SSigma_{\varepsilon}) + \mmu_{\varepsilon}^{\top} \, \LLambda \, \mmu_{\varepsilon}$ (cf. \citealp{Math1992}). The expression for the expectation of the radius of the ridge constraint can now be evaluated for the orthogonal $\mathbf{X}$ and the strongly, positively collinear $\mathbf{X}$. It turns out that the latter is larger than the former. This results in a larger ridge constraint. For the larger ridge constraint there is a smaller level set that hits it first. The point of intersection, still on the $x=y$-line, is now thus closer to $\beta$ and further from the origin (cf. right panel of Figure ref{fig.ridgeEstimates_correlatedCovariates}). The resulting estimate is thus larger than that from the orthogonal case.

The above needs some attenuation. Among others it depends on: *i)* the number of covariates in each block, *ii)* the size of the effects, i.e. regression coefficients of each covariate, and
*iii)* the degree of collinearity. Possibly, there are more factors influencing the behaviour of the ridge estimator presented in this subsection.

This behaviour of ridge regression is to be understood when using (say) gene expression data to predict a certain clinical outcome. Genes work in concert to fulfil a certain function in the cell. Consequently, one expects their expression levels to be correlated. Indeed, gene expression studies exhibit many co-expressed genes, that is, genes with correlating transcript levels.




======= Illustration} label{sect.ridgeRegressionDataIllustration =======
The application of ridge regression to actual data aims to illustrate its use in practice.

===== MCM7 expression regulation by microRNAs =====

Recently, a new class of RNA was discovered, referred to as microRNA. MicroRNAs are non-coding, single stranded RNAs of approximately 22 nucleotides. Like mRNAs, microRNAs are encoded in and transcribed from the DNA. MicroRNAs play an important role in the regulatory mechanism of the cell. MicroRNAs down-regulate gene expression by either of two post-transcriptional mechanisms: mRNA cleavage or transcriptional repression. This depends on the degree of complementarity between the microRNA and the target. Perfect or nearly perfect complementarity of the mRNA to the microRNA will lead to cleavage and degradation of the target mRNA. Imperfect complementarity will repress the productive translation and reduction in protein levels without affecting the mRNA levels.
A single microRNA can bind to and regulate many different mRNA targets. Conversely, several microRNAs can bind to and cooperatively control a single mRNA target (\citealp{Bart2004}; \citealp{Esqu2006}; \citealp{Kim2006}).

In this illustration we wish to confirm the regulation of mRNA expression by microRNAs in an independent data set. We cherry pick an arbitrary finding from literature reported in cite{Ambs2008}, which focusses on the microRNA regulation of the MCM7 gene in prostate cancer. The MCM7 gene is involved in DNA replication \citep{Tye1999}, a cellular process often derailed in cancer. Furthermore, MCM7 interacts with the tumor-suppressor gene RB1 \citep{Ster1998}. Several studies indeed confirm the involvement of MCM7 in prostate cancer \citep{Padm2004}. And recently, it has been reported that in prostate cancer MCM7 may be regulated by microRNAs \citep{Ambs2008}.

We here assess whether the MCM7 down-regulation by microRNAs can be observed in a data set other than the one upon which the microRNA-regulation of MCM7 claim has been based. To this end we download from the Gene Expression Omnibus (GEO) a prostate cancer data set (presented by \citealp{Wang2009GeneNetworks}). This data set (with GEO identifier: GSE20161) has both mRNA and microRNA profiles for all samples available. The preprocessed (as detailed in \citealp{Wang2009GeneNetworks}) data are downloaded and require only minor further manipulations to suit our purpose. These manipulations comprise *i)* averaging of duplicated profiles of several samples, *ii)* gene- and mir-wise zero-centering of the expression data, *iii)* averaging the expression levels of the probes that interrogate MCM7. Eventually, this leaves 90 profiles each comprising of 735 microRNA expression measurements.



\lstinputlisting{prostate_ridgeAnalysis.r}


With this prostate data set at hand we now investigate whether MCM7 is regulated by microRNAs. Hereto we fit a linear regression model regressing the expression levels of MCM7 onto those of the microRNAs. As the number of microRNAs exceeds the number of samples, ordinary least squares fails and we resort to the ridge estimator of the regression coefficients. First, an informed choice of the penalty parameter is made through maximization of the LOOCV log-likelihood, resulting in $\lambda_{\mbox{{\tiny opt}}} = 1812.826$. Having decided on the value of the to-be-employed penalty parameter, the ridge regression estimator can now readily be evaluated. The thus fitted model allows for the evaluation of microRNA-regulation of MCM7. E.g., by the proportion of variation of the MCM7 expression levels by the microRNAs as expressed in coefficient of determination: $R^2 = 0.4492$. Alternatively, but closely related, observed expression levels may be related to the linear predictor of the MCM7 expression levels: $\hat{\mathbf{Y}}(\lambda_{\mbox{{\tiny opt}}}) = \mathbf{X} \hat{\beta} (\lambda_{\mbox{{\tiny opt}}})$. The Spearman correlation of response and predictor equals 0.6295. A visual inspection is provided by the left panel of Figure ref{fig.RidgeProstateExample}. Note the difference in scale of the $x$- and $y$-axes. This is due to the fact that the regression coefficients have been estimated in penalized fashion, consequently shrinking estimates of the regression coefficients towards zero leading to small estimates and in turn compressing the range of the linear prediction. The above suggests there is indeed association between the microRNA expression levels and those of MCM7.



# original latex figure with scale=0.45, angle=0

FIGURE: [prostate_ridgeFit.eps, width=400 frac=1.0] Left panel: Observed vs. (ridge) fitted MCM7 expression values. Right panel: Histogram of the ridge regression coefficient estimates.} label{fig.RidgeProstateExample


The overall aim of this illustration was to assess whether microRNA-regulation of MCM7 could also be observed in this prostate cancer data set. In this endeavour the dogma (stating this regulation should be negative) has nowhere been used. A first simple assessment of the validity of this dogma studies the signs of the estimated regression coefficients. The ridge regression estimate has 394 out of the 735 microRNA probes with a negative coefficient. Hence, a small majority has a sign in line with the `microRNA $\downarrow$ mRNA' dogma. When, in addition, taking the size of these coefficients into account (Figure ref{fig.RidgeProstateExample}, right panel), the negative regression coefficient estimates do not substantially differ from their positive counterparts (as can be witnessed from their almost symmetrical distribution around zero). Hence, the value of the `microRNA $\downarrow$ mRNA' dogma is not confirmed by this ridge regression analysis of the MCM7-regulation by microRNAs. Nor is it refuted.


The implementation of ridge regression in the {\tt penalized}-package offers the possibility to fully obey the dogma on negative regulation of mRNA expression by microRNAs. This requires all regression coefficients to be negative. Incorporation of the requirement into the ridge estimation augments the constrained estimation problem with an additional constraint:
\begin{align*}  % label{form.constrEstProblemRidge}
\hat{\beta}(\lambda) & =   \arg \min_{\| \beta \|_2^2 \leq c (\lambda) \atop \beta_j \leq 0 \, \mbox{{\tiny  for all $j$}}} \| \mathbf{Y} - \mathbf{X} \, \beta \|^2_2.
\end{align*}
With the additional non-positivity constraint on the parameters, there is no explicit solution for the estimator. The ridge estimate of the regression parameters is then found by numerical optimization using e.g. the Newton-Raphson algorithm or a gradient descent approach. The next listing gives the R-code for ridge estimation with the non-positivity constraint of the linear regression model.

\lstinputlisting{prostate_ridgeAnalysisWithConstraint.r}

The linear regression model linking MCM7 expression to that of the microRNAs is fitted by ridge regression while simultaneously obeying the `negative regulation of mRNA by microRNA'-dogma to the prostate cancer data. In the resulting model 401 out of 735 microRNA probes have a nonzero (and negative) coefficient. There is a large overlap in microRNAs with a negative coefficient between those from this and the previous fit. The models are also compared in terms of their fit to the data. The Spearman rank correlation coefficient between response and predictor for the model without positive regression coefficients equals 0.679 and its coefficient of determination 0.524 (confer the left panel of ref{fig.RidgeProstateExample_constrainedAnalysis} for a visualization). This is a slight improvement upon the unconstrained ridge estimated model. The improvement may be small but it should be kept in mind that the number of parameters used by both models is 401 (for the model without positive regression coefficients) vs. 735. Hence, with close to half the number of parameters the dogma-obeying model gives a somewhat better description of the data. This may suggest that there is some value in the dogma as inclusion of this prior information leads to a more parsimonious model without any loss in fit.



# original latex figure with scale=0.45, angle=0

FIGURE: [prostate_ridgeFitConstrained.eps, width=400 frac=1.0] Left panel: Observed vs. (ridge) fitted MCM7 expression values (with the non-positive constraint on the parameters in place). Right panel: Histogram of the ridge regression coefficient estimates (from the non-positivity constrained analysis).} label{fig.RidgeProstateExample_constrainedAnalysis



The dogma-obeying model selects 401 microRNAs that aid in the explanation of the variation in the gene expression levels of MCM7. There is an active field of research, called *target prediction*, trying to identify which microRNAs target the mRNA of which genes. Within {\tt R} there is a collection of packages that provide the target prediction of known microRNAs. The packages differ on the method (e.g. experimental or sequence comparison) that has been used to arrive at the prediction. These target predictions may be used to evaluate the value of the found 401 microRNAs. Ideally, there would be a substantial amount of overlap. The {\tt R}-script that loads the target predictions and does the comparison is below.

\lstinputlisting{prostate_link2predictionTools.r}


|-----------------------------------------------------------------|
| # #if FORMAT in ("latex", "pdflatex")
\vspace{-7pt}
# #endif       |                     |                     |
|---------r--------------------r--------------------r-------------|
| # #if FORMAT in ("latex", "pdflatex")
\vspace{3pt}
# #endif        | $\hat{\beta}_j = 0$ | $\hat{\beta}_j < 0$ |
| # #if FORMAT in ("latex", "pdflatex")
\vspace{-4pt}
# #endif       |                     |                     |
| microRNA not target | 323                 | 390                 |
| microRNA target     | 11                  | 11                  |
| # #if FORMAT in ("latex", "pdflatex")
\vspace{-9pt}
# #endif       |                     |                     |
|-----------------------------------------------------------------|


\caption{Cross-tabulation of the microRNAs being a potential target of MCM7 vs. the value of its regression coefficient in the dogma-obeying model.} label{table.mir2mcm7prediction}


With knowledge available on each microRNA whether it is predicted (by at least one target prediction package) to be a potential target of MCM7, it may be cross-tabulated against its corresponding regression coefficient estimate in the dogma-obeying model being equal to zero or not. Table ref{table.mir2mcm7prediction} contains the result.  Somewhat superfluous considering the data, we may test whether the targets of MCM7 are overrepresented in the group of strictly negatively estimated regression coefficients. The corresponding chi-squared test (with Yates' continuity correction) yields the test statistic $\chi^2 = 0.0478$ with a $p$-value equal to 0.827. Hence, there is no enrichment among the 401 microRNAS of those that have been predicted to target MCM7. This may seem worrisome. However, the microRNAs have been selected for their predictive power of the expression levels of MCM7. Variable selection has not been a criterion (although the sign constraint implies selection). Moreover, criticism on the value of the microRNA target prediction has been accumulating in recent years.


======= Conclusion =======

We discussed ridge regression as a modification of linear regression to  overcome the empirical non-identifiability of the latter when confronted with high-dimensional data. The means to this end was the addition of a (ridge) penalty to the sum-of-squares loss function of the linear regression model, which turned out to be equivalent to constraining the parameter domain. This warranted the identification of the regression coefficients, but came at the cost of introducing bias in the estimates. Several properties of ridge regression like moments, MSE, and its Bayesian interpretation have been reviewed. Finally, its behaviour and use have been illustrated in simulation and omics data.


======= Exercises =======
\begin{question}[^footnote2][^footnote2]: This exercise is freely rendered from cite{Drap1998} \mbox{ }
\\
Find the ridge regression solution for the data below for a general value of $\lambda$ and for the straight line model $Y = \beta_0 + \beta_1 X + \varepsilon$ (only apply the ridge penalty to the slope parameter, not to the intercept). Show that when $\lambda$ is chosen as 0.4, the ridge solution fit is $\hat{Y} = 40 + 1.75 X$. Data: $\mathbf{X}^{\top} = (X_1, X_2, \ldots, X_{8})^{\top} = (-2, -1, -1, -1, 0, 1, 2, 2)^{\top}$, and $\mathbf{Y}^{\top} = (Y_1, Y_2, \ldots, Y_{8})^{\top} = (35, 40, 36, 38, 40, 43, 45, 43)^{\top}$.
\end{question}

\begin{question}[^footnote3][^footnote3]: This exercise is freely rendered from cite{Hast2009, but can be found in many other places. The original source is unknown to the author.}  label{question:ridgeAugmentation} \mbox{ }
\\
Show that the ridge regression estimates can be obtained by ordinary least squares regression on an augmented data set. We augment the centered matrix $\mathbf{X}$ with $p$ additional row $\sqrt{\lambda} \mathbf{I}$, and augment $\mathbf{y}$ with $p$ zeros.
\end{question}

\begin{question} label{question.ridgeResidualsProjection} \mbox{ }
\\
The coefficients $\beta$ of a linear regression model, $\mathbf{Y} = \mathbf{X} \beta + \vvarepsilon$, are estimated by $\hat{\beta} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}$. The associated fitted values then given by $\widehat{\mathbf{Y}} = \mathbf{X} \, \hat{\beta} = \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y} = \mathbf{H} \mathbf{Y}$, where $\mathbf{H} =\mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}$ referred to as the hat matrix. The matrix $\mathbf{P}$ is a projection matrix and satisfies $\mathbf{H} = \mathbf{H}^ 2$. Hence, linear regression projects the response $\mathbf{Y}$ onto the vector space spanned by the columns of $\mathbf{Y}$. Consequently, the residuals $\hat{\vvarepsilon}$ and $\hat{\mathbf{Y}}$ are orthogonal. Now consider the ridge estimator of the regression coefficients: $\hat{\beta}(\lambda) = (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top} \mathbf{Y}$. Let $\hat{\mathbf{Y}}(\lambda) =  \mathbf{X} \hat{\beta}(\lambda)$ be the vector of associated fitted values.

\begin{compactitem}
\item[*a)*] Show that the matrix $\mathbf{H}(\lambda) = \mathbf{X} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top}$, associated with ridge regression, is not a projection matrix (for any $\lambda > 0$).

\item[*b)*] Show that the `ridge fit' $\widehat{\mathbf{Y}}(\lambda)$ is not orthogonal to the associated `ridge residuals' $\hat{\vvarepsilon}(\lambda)$  (for any $\lambda > 0$).

\item[*c)*] Derive the distribution of the `ridge residuals'.
\end{compactitem}
\end{question}


\begin{question} label{question.MSEofLinearPredictor} \mbox{ }
\\
Recall that there exists $\lambda >  0$ such  that $MSE(\hat{\beta}) > MSE[\hat{\beta}(\lambda)]$. Verify that this carries over to the linear predictor. That is, there exists a $\lambda > 0$ such that$MSE(\widehat{\mathbf{Y}}) = MSE(\mathbf{X} \hat{\beta}) > MSE[\mathbf{X}\hat{\beta}(\lambda)]$.
\end{question}

\begin{question} \mbox{ }
\\
Consider a 3-gene pathway. Expression levels of these three genes have been measured in an observational study involving hundred individuals. In order to assess how the expression levels of gene A are affect by that of genes B and C, a medical researcher fits the
\begin{align*}
Y_{i}^{(A)} &=  \beta_b Y_{i}^{(B)} + \beta_c Y_{i}^{(C)} +
\varepsilon_{i},
\end{align*}
with $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$. This model fitted by means of ridge regression, but with a separate penalty parameter, $\lambda_{2,b}$ and $\lambda_{2,c}$, for the two regression coefficient, $\beta_b$ and $\beta_c$, respectively.

\begin{compactitem}
\item[*a)*] Write down the ridge penalized loss function employed by the researcher.

\item[*b)*] Does a different choice of penalty parameter for the second regression coefficient affect the estimation of the first regression coefficient? Motivate your answer.

\item[*c)*] The researcher decides that the second covariate $Y_i^{(C)}$ is irrelevant. Instead of removing the covariate from model, the researcher decides to set $\lambda_{2,c} =
\infty$. Show that this results in the same ridge estimate for $\beta_b$ as when fitting (again by means of ridge regression) the model without the second covariate.
\end{compactitem}
\end{question}

\begin{question} \mbox{ }
\\
The expression levels of the $j$-the gene are explained by a linear regression model in terms of those of all other genes. Consider the following two ridge regression estimators of the regression parameter of this model, defined as:
\begin{align*}
\arg \max_\beta \sum_{i=1}^n (Y_{i,j} - \mathbf{Y}_{i,\setminus j} \beta_j)^2 + \lambda \| \beta_j \|_ 2^2
\quad 
\mbox{and} \quad 
\arg \max_\beta \sum_{i=1}^n (Y_{i,j} - \mathbf{Y}_{i,\setminus j} \beta_j)^2 + n \lambda \| \beta_j \|_ 2^2.
\end{align*}
Which do you prefer? Motivate.
\end{question}




========= Generalizing ridge regression ========= label{chap:genRidge}
#  ======= Generalized ridge regression =======

The expos\'{e} on ridge regression may be generalized in many ways. Among others different generalized linear models may be considered (confer Section ref{sect.ridgeLogistic}). In this section we stick to the linear regression model $\mathbf{Y} = \mathbf{X} \beta + \vvarepsilon$ with the usual assumptions, but fit it in weighted fashion and generalize the common, spherical penalty. The loss function corresponding to this scenario is:

!bt

!bt
\begin{align} label{form:generalizedRidgeLoss}
(\mathbf{Y} - \mathbf{X} \beta)^{\top} \mathbf{W} (\mathbf{Y} - \mathbf{X} \beta) + (\beta - \beta_0)^{\top} \mathbf{\Delta}
(\beta - \beta_0),
\end{align}
!et
!et
which comprises a weighted least squares criterion and a generalized ridge penalty. In this $\mathbf{W}$ is a $(n \times n)$-dimensional, diagonal matrix with $(\mathbf{W})_{ii} \in [0,1]$ representing the weight of the $i$-th observation. The penalty is now a quadratic form with penalty parameter $\mathbf{\Delta}$, a $(p \times p)$-dimensional, positive definite, symmetric matrix. When $\mathbf{\Delta} = \lambda \mathbf{I}_{pp}$, one regains the spherical penalty of `regular ridge regression'. This penalty shrinks each element of the regression parameter $\beta$ equally along the unit vectors $\mathbf{e}_j$. Generalizing $\mathbf{\Delta}$ to the class of symmetric, positive definite matrices $\mathcal{S}_{++}$ allows for *i)* different penalization per regression parameter, and *ii)* joint (or correlated) shrinkage among the elements of $\beta$. The penalty parameter $\mathbf{\Delta}$ determines the speed and direction of shrinkage.  The $p$-dimensional column vector $\beta_0$ is a user-specified, non-random target towards which $\beta$ is shrunken as the penalty parameter increases. When recasting generalized ridge estimation as a constrained estimation problem, the implications of the penalty may be visualized (Figure ref{fig:generalizedRidgeIllustration}, left panel). The generalized ridge penalty is a quadratic form centered around $\beta_0$. In Figure ref{fig:generalizedRidgeIllustration} the parameter constraint clearly is ellipsoidal (and not spherical). Moreover, the center of this ellipsoid is not at zero.

# original latex figure with scale=0.40, angle=0

FIGURE: [generalizedRidgeIllustration_penalty.eps, width=400 frac=1.0] Left panel: the contours of the likelihood (grey solid ellipsoids) and the parameter constraint implied by the generalized penalty (black dashed ellipsoid. Right panel: generalized (fat coloured lines) and `regular' (thin coloured lines) regularization paths  of four regression coefficients. The dotted grey (straight) lines indicated the targets towards the generalized ridge penalty shrinks regression coefficient estimates.} label{fig:generalizedRidgeIllustration


The addition of the generalized ridge penalty to the sum-of-squares ensures the existence of a unique regression estimator in the face of super-collinearity. The generalized penalty is a non-degenerated quadratic form in $\beta$ due to the positive definiteness of the matrix $\mathbf{\Delta}$. As it is non-degenerate, it is strictly convex. Consequently, the generalized ridge regression loss function (ref{form:generalizedRidgeLoss}), being the sum of a convex and strictly convex function, is also strictly convex. This warrants the existence of a unique global minimum and, thereby, a unique estimator.


Like for the `regular' ridge loss function (ref{form.ridgeLossFunction}), there is an explicit expression for the optimum of the generalized ridge loss function (ref{form:generalizedRidgeLoss}). To see this, obtain the estimating equation of $\beta$ through equating its derivative with respect to $\beta$ to zero:
\begin{align*}
2 \mathbf{X}^{\top} \mathbf{W} \mathbf{Y} - 2 \mathbf{X}^{\top} \mathbf{W} \mathbf{X} \beta - 2 \mathbf{\Delta} \beta  + 2 \mathbf{\Delta} \beta_0 & =  \mathbf{0}_{p}.
\end{align*}
This is solved by:

!bt

!bt
\begin{align}
\hat{\beta}(\mathbf{\Delta}) & =  (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \mathbf{\Delta})^{-1} (\mathbf{X}^{\top} \mathbf{W} \mathbf{Y} + \mathbf{\Delta} \beta_0).
\end{align}
!et
!et
Clearly, this reduces to the `regular' ridge estimator by setting $\mathbf{W} = \mathbf{I}_{nn}$,  $\beta_0 = \mathbf{0}_{p}$, and $\mathbf{\Delta} = \lambda \mathbf{I}_{pp}$. The effects of the generalized ridge penalty on the estimates can be seen in the regularization paths of the estimates. Figure ref{fig:generalizedRidgeIllustration} (right panel) contains an example of the regularization paths for coefficients of a linear regression model with four explanatory variables. Most striking is the limiting behaviour of the estimates of $\beta_3$ and $\beta_4$ for large values of the penalty parameter $\lambda$: they convergence to a non-zero value (as was specified by a nonzero $\beta_0$). More subtle is the (temporary) convergence of the regularization paths of the estimates of $\beta_2$ and $\beta_3$. That of $\beta_2$ is pulled away from zero (its true value and approximately its unpenalized estimate) towards the estimate of $\beta_3$. In the regularization path of $\beta_3$ this can be observed in a delayed convergence to its nonzero target value (for comparison consider that of $\beta_4$). For reference the corresponding regularization paths of the `regular' ridge estimates (as thinner lines of the same colour) are included in Figure ref{fig:generalizedRidgeIllustration}.



\begin{example} *Fused ridge estimation* label{example.gRidge2orthonormalDesign}
\\
An example of a generalized ridge penalty is the *fused ridge penalty*. Consider the standard linear model $\mathbf{Y} = \mathbf{X} \beta + \vvarepsilon$. The fused ridge estimator of $\beta$ then minimizes:

!bt

!bt
\begin{align} label{form:fusedRidgeLoss}
\| \mathbf{Y} - \mathbf{X} \beta \|_2^2 + \lambda \sum_{j=2}^p \| \beta_{j} - \beta_{j-1} \|_2^2.
\end{align}
!et
!et
The penalty in the loss function above can be written as a generalized ridge penalty:
\begin{align*}
\lambda \sum_{j=2}^p \| \beta_{j} - \beta_{j-1} \|_2^2 & =   \beta^{\top}
\left(
\begin{array}{rrrrrr}
\lambda & -\lambda & 0 & \ldots & \ldots &  0
# #if FORMAT in ("latex", "pdflatex")
\vspace{-4pt}
# #endif
\\
-\lambda & 2 \lambda & -\lambda & \ddots & & \vdots
# #if FORMAT in ("latex", "pdflatex")
\vspace{-4pt}
# #endif
\\
0 & -\lambda & 2 \lambda & \ddots & \ddots   & \vdots
# #if FORMAT in ("latex", "pdflatex")
\vspace{-4pt}
# #endif
\\
\vdots & \ddots & \ddots & \ddots & \ddots & 0
# #if FORMAT in ("latex", "pdflatex")
\vspace{-4pt}
# #endif
\\
\vdots &  & \ddots & \ddots & \ddots & -\lambda
#  # #if FORMAT in ("latex", "pdflatex")
\vspace{-4pt}
# #endif
\\
0 & \ldots & \ldots & 0 & -\lambda &  \lambda
\end{array}\right)
\beta.
\end{align*}
The matrix $\mathbf{\Delta}$  employed above is semi-positive definite and therefore the loss function (ref{form:fusedRidgeLoss}) is not strictly convex. Hence, often a regular ridge penalty $\| \beta \|_2^2$ is added (with its own penalty parameter).

To illustrate the effect of the fused ridge penalty on the estimation of the linear regression model $\mathbf{Y} = \mathbf{X} \beta + \vvarepsilon$, let $\beta_j = \phi_{0,1}(z_j)$ with $z_j =-30 + \tfrac{6}{50} j$ for $j=1, \ldots, 500$. Sample the elements of the design matrix $\mathbf{X}$ and those of the error vector $\vvarepsilon$ from the standard normal distribution, then form the response $\mathbf{Y}$ from the linear model. The regression parameter is estimated through fused ridge loss minimization with $\lambda=1000$. The estimate is shown in Figure ref{fig:fusedRidgeIllustration} (red line). For reference the figure includes the true $\beta$ (black line) and the `regular ridge' estimate with $\lambda=1$ (blue line). Clearly, the fused ridge estimate yields a nice smooth vector of $\beta$ estimates
\end{example}

# original latex figure with scale=0.40, angle=0

FIGURE: [fusedRidgeIllustration_simulation.eps, width=400 frac=1.0] Left panel: illustration of the fused ridge estimator (in simulation). The true parameter $\beta$ and its ridge and fused ridge estimates against their spatial order. Right panel:  Ridge vs. fused ridge estimates of the DNA copy effect on KRAS expression levels. The dashed, grey vertical bar indicates the location of the KRAS gene. } label{fig:fusedRidgeIllustration




======= Moments =======

The expectation and variance of $\hat{\beta}(\mathbf{\Delta})$ are obtained through application of the same matrix algebra and expectation and covariance rules used in the derivation of their counterparts of the `regular' ridge regression estimator. This leads to:
\begin{align*}
\mathbb{E}[\hat{\beta}(\mathbf{\Delta})] & =  (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \mathbf{\Delta})^{-1} (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} \beta + \mathbf{\Delta} \beta_0),
\\
\mbox{Var}[\hat{\beta}(\mathbf{\Delta})] & =  \sigma^2 (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \mathbf{\Delta})^{-1} \mathbf{X}^{\top} \mathbf{W}^2 \mathbf{X} (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \mathbf{\Delta})^{-1}.
\end{align*}
From these expressions similar limiting behaviour as for the `regular' ridge regression case can be deduced. To this end let $\mathbf{V}_{\delta} \mathbf{D}_{\delta} \mathbf{V}_{\delta}^{\top}$ be the eigendecomposition of $\mathbf{\Delta}$ and $d_{\delta,j} = (\mathbf{D}_{\delta})_{jj}$. Furthermore, define (with some abuse of notation) $\lim_{\mathbf{\Delta} \rightarrow \infty}$ as the limit of all $d_{\delta,j}$ simultaneously tending to infinity. Then, $\lim_{\mathbf{\Delta} \rightarrow \infty} \mathbb{E}[\hat{\beta}(\mathbf{\Delta})] = \beta_0$ and $\lim_{\mathbf{\Delta} \rightarrow \infty} \mbox{Var}[\hat{\beta}(\mathbf{\Delta})] = \mathbf{0}_{pp}$.


\begin{example} \mbox{ } label{example.gRidge2orthonormalDesign}
\\
Let $\mathbf{X}$ be an $(n \times p)$-dimensional, orthonormal design matrix. Contrast the regular and generalized ridge regression estimator, the latter with $\mathbf{W} = \mathbf{I}_{pp}$, $\beta_0 = \mathbf{0}_p$ and $\mathbf{\Delta} = \lambda \mathbf{R}$ where $\mathbf{R} = (1-\rho) \mathbf{I}_{pp} + \rho \mathbf{1}_{pp}$ for $\rho \in (-(p-1)^{-1}, 1)$. For $\rho =0$ the two estimators coincide. The variance of the generalized ridge regression estimator then is $\mbox{Var}[ \hat{\beta}(\mathbf{\Delta})] = (\mathbf{I}_{pp} + \mathbf{\Delta})^{-2}$. The efficiency of this estimator, measured by its generalized variance, is:
\begin{align*}
\det \{ \mbox{Var}[ \hat{\beta}(\mathbf{\Delta})] \} & =  \{ [1 + \lambda + (p-1) \rho]  (1 + \lambda-\rho)^{p-1} \}^{-2}.
\end{align*}
This efficiency attains its minimum at $\rho = 0$. In the present case, the regular ridge regression estimator is thus more efficient than its generalized counterpart.
\end{example}


\begin{example} *(MSE with perfect target)*
\\ 
Set $\beta_0 = \beta$, i.e. the target is equal to the true value of the regression parameter. Then:
\begin{align*}
\mathbb{E}[\hat{\beta}(\Delta)] & =  (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \Delta)^{-1} (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} \beta + \Delta \beta) \, \, \, = \, \, \, \beta.
\end{align*}
Hence, irrespective of the choice of $\Delta$, the generalized ridge is then unbiased. Thus:
\begin{align*}
\mbox{MSE}[\hat{\beta}(\Delta)] & =  \mbox{tr} \{
\mbox{Var}[\hat{\beta}(\Delta)] \}
\\
& =  \mbox{tr}[ \sigma^{2} (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \mathbf{\Delta})^{-1} \mathbf{X}^{\top} \mathbf{W}^2 \mathbf{X} (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \mathbf{\Delta})^{-1}]
\\
& =  \sigma^2 \mbox{tr}[ \mathbf{X}^{\top} \mathbf{W}^2 \mathbf{X} (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \mathbf{\Delta})^{-2}].
\end{align*}
When $\mathbf{\Delta} = \lambda \mathbf{I}_{pp}$, this MSE is smaller than that of the ML regression estimator, irrespective of the choice of $\lambda$. 
\end{example}


======= The Bayesian connection} label{sect:genRidgeBayes =======
This generalized ridge estimator can, like the regular ridge estimator, be viewed as a Bayesian estimator. It requires to replace the conjugate prior on $\beta$ by a more general normal law, $\beta \sim \mathcal{N}(\beta_0, \sigma^2 \mathbf{\Delta}^{-1})$, but retains the gamma prior on $\sigma^2$. The joint posterior distribution of $\beta$ and $\sigma^2$ is then obtained analogously (the details are left as Exercise ref{question.generalizedRidgeAndBayes}) to Section ref{sect:Bayes}:
\begin{align*}
f_{\beta, \sigma^2} (\beta, \sigma^2 \, | \, \mathbf{Y}, \mathbf{X}) & =  f_Y (\mathbf{Y} \, | \, \mathbf{X}, \beta, \sigma^2) \, f_{\beta}(\beta | \sigma^2) \, f_{\sigma}(\sigma^2)
\\
& \propto & g_{\beta} (\beta \, | \, \sigma^2, \mathbf{Y}, \mathbf{X}) \,
g_{\sigma^2} (\sigma^2 \, | \, \mathbf{Y}, \mathbf{X})
\end{align*}
with
\begin{align*}
g_{\beta} (\beta \, | \, \sigma^2, \mathbf{Y}, \mathbf{X})
& \propto & \exp \Big\{ - \frac{1}{2\sigma^2} \big[ \beta - \hat{\beta}(\mathbf{\Delta}) \big]^{\top} (\mathbf{X}^{\top} \mathbf{X} +  \mathbf{\Delta}) \big[ \beta - \hat{\beta}(\mathbf{\Delta}) \big] \Big\}.
\end{align*}
This implies $\mathbb{E}(\beta  \, | \, \sigma^2, \mathbf{Y}, \mathbf{X}) = \hat{\beta}(\mathbf{\Delta})$. Hence, the generalized ridge regression estimator too can be viewed as the Bayesian posterior mean estimator of $\beta$ when imposing a multivariate Gaussian prior on the regression parameter.





======= Application =======

An illustration involving omics data can be found in the explanation of a gene's expression levels in terms of its DNA copy number. The latter is simply the number of gene copies encoded in the DNA. For instance, for most genes on the autosomal chromosomes the DNA copy number is two, as there is a single gene copy on each chromosome and autosomal chromosomes come in pairs.  Alternatively, in males the copy number is one for genes that map to the X or Y chromosome, while in females it is zero for genes on the Y chromosome. In cancer the DNA replication process has often been compromised leading to a (partially) reshuffled and aberrated DNA. Consequently, the cancer cell may exhibit gene copy numbers well over a hundred for classic oncogenes. A faulted replication process does - of course - not nicely follow the boundaries of gene encoding regions. This causes contiguous genes to commonly share aberrated copy numbers. With genes being transcribed from the DNA and a higher DNA copy number implying an enlarged availability of the gene's template, the latter is expected to lead to elevated expression levels. Intuitively, one expects this effect to be localized (a so-called *cis*-effect), but some suggest that aberrations elsewhere in the DNA may directly affect the expression levels of distant genes (referred to as a *trans*-effect).

The *cis*- and *trans*-effects of DNA copy aberrations on the expression levels of the KRAS oncogene in colorectal cancer are investigated. Data of both molecular levels from the TCGA (The Cancer Genome Atlas) repository are downloaded \citep{TCGA2012colon}. The gene expression data are limited to that of KRAS, while for  the DNA copy number data only that of chromosome 12, which harbors KRAS, is retained. This leaves genomic profiles of 195 samples comprising 927 aberrations. Both molecular data types are zero centered feature-wise. Moreover, the data are limited to ten - conveniently chosen? - samples. The KRAS expression levels are explained by the DNA copy number aberrations through the linear regression model. The model is fitted by means of ridge regression, with $\lambda \mathbf{\Delta}$ and $\mathbf{\Delta} = \mathbf{I}_{pp}$ and a single-banded $\mathbf{\Delta}$ with unit diagonal and the elements of the first off-diagonal equal to the arbitrary value of $-0.4$. The latter choice appeals to the spatial structure of the genome and encourages similar regression estimates for contiguous DNA copy numbers. The penalty parameter is chosen by means of leave-one-out cross-validation using the squared error loss. 

\lstinputlisting{fusedRidge_KRASexample.r}

The right panel of Figure ref{fig:fusedRidgeIllustration} shows the ridge regression estimate with both choices of $\mathbf{\Delta}$ and optimal penalty parameters plotted against the chromosomal order. The location of KRAS is indicated by a vertical dashed bar. The ordinary ridge regression estimates show a minor peak at the location of KRAS but is otherwise more or less flat. In the generalized ridge estimates the peak at KRAS is emphasized. Moreover, the region close to KRAS exhibits clearly elevated estimates, suggesting co-abberated DNA. For the remainder the generalized ridge estimates portray a flat surface, with the exception of a single downward spike away from KRAS. Such negative effects are biologically nonsensible (more gene templates leading to reduced expression levels?). On the whole the generalized ridge estimates point towards the *cis*-effect as the dominant genomic regulation mechanism of KRAS expression. The isolated spike may suggest the presence of a *trans*-effect, but its sign is biological nonsensible and the spike is fully absent in the ordinary ridge estimates. This leads us to ignore the possibility of a genomic *trans*-effect on KRAS expression levels in colorectal cancer.

The sample selection demands justification. It yields a clear illustrate-able difference between the ordinary and ridge estimates. When all samples are left in, the *cis*-effect is clearly present, discernable from both estimates that yield a virtually similar profile. 


======= Generalized ridge regression =======

What is generally referred to as `generalized ridge regression' (cf. \citealp{Hoer1970, Hemm1975}) is the particular case of loss function (ref{form:generalizedRidgeLoss}) in which $\mathbf{W} = \mathbf{I}_{nn}$,  $\beta_0 = \mathbf{0}_{p}$, and $\mathbf{\Delta} = \mathbf{V}_{x} \mathbf{\Lambda} \mathbf{V}_x^{\top}$, where $\mathbf{V}_x$ is obtained from the singular value decomposition of $\mathbf{X}$ (i.e., $\mathbf{X} = \mathbf{U}_{x} \mathbf{D}_x \mathbf{V}_x^{\top}$ with its constituents endowed with the usual interpretation) and $\mathbf{\Lambda}$ a positive definite diagonal matrix. This gives the estimator:
\begin{align*}
\hat{\beta}(\mathbf{\Lambda}) & =  (\mathbf{X}^{\top} \mathbf{X} + \mathbf{\Delta})^{-1} \mathbf{X}^{\top} \mathbf{Y}
\\
& =   (\mathbf{V}_x \mathbf{D}_x \mathbf{U}_x^{\top} \mathbf{U}_x \mathbf{D}_x \mathbf{V}_x^{\top} + \mathbf{V}_x \mathbf{\Lambda} \mathbf{V}_x^{\top})^{-1} \mathbf{V}_x \mathbf{D}_x \mathbf{U}_x \mathbf{Y}
\\
& =  \mathbf{V}_x (\mathbf{D}_x^2 + \mathbf{\Lambda})^{-1} \mathbf{D}_x \mathbf{U}_x \mathbf{Y}.
\end{align*}
From this last expression it becomes clear how this estimator generalizes the `regular ridge estimator'. The latter shrinks all eigenvalues, irrespectively of their size, in the same manner through a common penalty parameter. The `generalized ridge estimator', through differing penalty parameters (i.e. the diagonal elements of $\mathbf{\Lambda}$), shrinks them individually.

The generalized ridge estimator coincides with the Bayesian linear regression estimator with the normal prior $\mathcal{N}[\mathbf{0}_p, (\mathbf{V}_x \mathbf{\Lambda} \mathbf{V}_x^{\top})^{-1}]$ on the regression parameter $\beta$ (and preserving the inverse gamma prior on the error variance). Assume $\mathbf{X}$ to be of full column rank and choose $\mathbf{\Lambda} = g^{-1} \mathbf{D}_x^2$ with $g$ a positive scalar. The prior on $\beta$ then - assuming $(\mathbf{X}^{\top} \mathbf{X})^{-1}$ exits - reduces to Zellner's $g$-prior: $\beta \sim \mathcal{N}[\mathbf{0}_p, g (\mathbf{X}^{\top} \mathbf{X})^{-1}]$ \citep{Zell1986}. The corresponding estimator of the regression coefficient is: $\hat{\beta}(g)  = g (1+g)^{-1} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}$, which is proportional to the unpenalized ordinary least squares estimator of $\beta$.

For convenience of notation in the analysis of the generalized ridge estimator the linear regression model is usually rewritten as:
\begin{align*}
\mathbf{Y} & =  \mathbf{X} \beta + \vvarepsilon \, \, \, = \, \, \,
\mathbf{X} \mathbf{V}_x  \mathbf{V}_x^{\top} \beta + \vvarepsilon \, \, \, = \, \, \, \tilde{\mathbf{X}} \aalpha + \vvarepsilon,
\end{align*}
with $\tilde{\mathbf{X}} = \mathbf{X} \mathbf{V}_x = \mathbf{U}_x
\mathbf{D}_x$ (and thus $\tilde{\mathbf{X}}^{\top} \tilde{\mathbf{X}} = \mathbf{D}_x^ 2$) and $\aalpha = \mathbf{V}_x^{\top} \beta$ with loss function $(\mathbf{Y} - \tilde{\mathbf{X}} \aalpha)^{\top} (\mathbf{Y} - \tilde{\mathbf{X}} \aalpha) + \aalpha^{\top} \mathbf{\Lambda} \aalpha$. In the notation above the generalized ridge estimator is then:
\begin{align*}
\hat{\aalpha}(\mathbf{\Lambda}) & =  (\tilde{\mathbf{X}}^{\top} \tilde{\mathbf{X}} + \mathbf{\Lambda})^ {-1} \tilde{\mathbf{X}}^{\top} \mathbf{Y} = (\mathbf{D}_x^ 2 + \mathbf{\Lambda})^{-1} \tilde{\mathbf{X}}^{\top} \mathbf{Y},
\end{align*}
from which one obtains $\hat{\beta}(\mathbf{\Lambda}) = \mathbf{V}_x \hat{\aalpha}(\mathbf{\Lambda})$. Using $\mathbb{E}[\hat{\aalpha}(\mathbf{\Lambda})] = (\mathbf{D}_x^2 + \mathbf{\Lambda})^{-1} \mathbf{D}_x^2 \aalpha$ and $\mbox{Var}[\hat{\aalpha}(\mathbf{\Lambda})] = \sigma^2 (\mathbf{D}_x^2 + \mathbf{\Lambda})^{-1} \mathbf{D}_x^2 (\mathbf{D}_x^2 + \mathbf{\Lambda})^{-1}$, the MSE for the generalized ridge estimator can be written as:
\begin{align*}
\mbox{MSE}[\hat{\aalpha}(\mathbf{\Lambda})] & =  \sum_{j=1}^p ( \sigma^2 d_{x,j}^2 + \alpha_j^2 \lambda_{j}^2 ) (d_{x,j}^2 +  \lambda_{j} )^{-2},
\end{align*}
where $d_{x,j} = (\mathbf{D}_x)_{jj}$ and $\lambda_j = (\mathbf{\Lambda})_{jj}$. Having $\aalpha$ and $\sigma^ 2$ available, it is easily seen (equate the derivative w.r.t. $\lambda_j$ to zero and solve) that the MSE of $\hat{\aalpha}(\mathbf{\Lambda})$ is minimized by $\lambda_j = \sigma^2 / \alpha_j^2$ for all $j$. With $\aalpha$ and $\sigma^2$ unknown, cite{Hoer1970} suggest an iterative procedure to estimate the $\lambda_j$'s. Initiate the procedure with the OLS estimates of $\aalpha$ and $\sigma^2$, followed by sequentially updating the $\lambda_j$'s and
the estimates of $\aalpha$ and $\sigma^2$. An analytic expression of the limit of this procedure exists (\citealp{Hemm1975}). This limit, however, still depends on the observed $\mathbf{Y}$ and as such it does not necessarily yield the minimal attainable value of the MSE. This limit may nonetheless still yield a potential gain in MSE. This is investigated in cite{Lawl1981}. Under a variety of cases it seems to indeed outperform the OLS estimator, but there are exceptions.

======= Conclusion =======

To conclude: a note of caution. The generalized ridge penalty is extremely flexible. It can incorporate any prior knowledge on the parameter values (through specification of $\beta_0$) and the relations among these parameters (via $\mathbf{\Delta}$). While a pilot study or literature may provide a suggestion for $\beta_0$, it is less obvious how to choose an informative $\mathbf{\Delta}$ (although a spatial structure is a nice exception). In general, exact knowledge on the parameters should not be incorporated implicitly via the penalty (read: prior) but preferably be used explicitly in the model - the likelihood - itself. Though this may be the viewpoint of a prudent frequentist and a subjective Bayesian might disagree.


======= Exercises =======
\begin{question} \mbox{ }
\\
Consider the linear regression model $Y_i = \beta_1 X_{i,1} + \beta_2 X_{i,2} + \varepsilon_i$ for $i=1, \ldots, n$.
Suppose estimates of the regression parameters $(\beta_1, \beta_2)$ of this model are obtained through the minimization of the sum-of-squares augmented with a ridge-type penalty:
\begin{align*}
\Big[ \sum_{i=1}^n (Y_i - \beta_1 X_{i,1} - \beta_2 X_{i,2})^2 \Big] + \lambda (\beta_1^2 + \beta_2^2 + 2 \nu \beta_1 \beta_2),
\end{align*}
with penalty parameters $\lambda \in \mathbb{R}_{> 0}$ and $\nu \in (-1, 1)$.

\begin{compactitem}
\item[*a)*] Sketch (for both $\nu=0$ and $\nu=0.9$) the shape of the parameter constraint induced by the penalty above and describe in words the qualitative difference between both shapes.

\item[*b)*] When $\nu  = -1$ and $\lambda \rightarrow \infty$ the estimates of $\beta_1$ and $\beta_2$ (resulting from minimization of the penalized loss function above) converge towards each other:
$\lim_{\lambda \rightarrow \infty} \hat{\beta}_1(\lambda, -1) = \lim_{\lambda \rightarrow \infty} \hat{\beta}_2(\lambda, -1)$. Motivated by this observation a data scientists incorporates the equality constraint $\beta_1 = \beta = \beta_2$ explicitly into the model, and s/he estimates the `joint regression parameter' $\beta$ through the minimization (with respect to $\beta$) of:
\begin{align*}
\Big[ \sum_{i=1}^n (Y_i - \beta  X_{i,1} - \beta X_{i,2})^2 \Big] + \delta \beta^2,
\end{align*}
with penalty parameter $\delta \in \mathbb{R}_{> 0}$. The data scientist is surprised to find that resulting estimate $\hat{\beta}(\delta)$ does not have the same limiting (in the penalty parameter) behavior as the $\hat{\beta}_1(\lambda, -1)$, i.e. $\lim_{\delta \rightarrow \infty} \hat{\beta} (\delta) \not= \lim_{\lambda \rightarrow \infty} \hat{\beta}_1(\lambda, -1)$. Explain the misconception of the data scientist.

\item[*c)*] Assume that *i)* $n \gg 2$, *ii)* the unpenalized estimates $(\hat{\beta}_1(0, 0), \hat{\beta}_2(0, 0))$ equal $(-2,2)$, and *iii)* that the two covariates $X_1$ and $X_2$ are zero-centered, have equal variance, and are strongly negatively correlated. Consider $(\hat{\beta}_1(\lambda, \nu), \hat{\beta}_2(\lambda, \nu))$ for both $\nu=-0.9$ and $\nu=0.9$. For which value of $\nu$ do you expect the sum of the absolute value of the estimates to be largest? *Hint:* Distinguish between small and large values of $\lambda$ and think geometrically!
\end{compactitem}
\end{question}

\begin{question} label{question.generalizedRidgeAndBayes} \mbox{ }
\\ 
Consider the linear regression model $\mathbf{Y} = \mathbf{X} \beta + \vvarepsilon$ with $\vvarepsilon \sim \mathcal{N}(\mathbf{0}_p, \sigma^2 \mathbf{I}_{pp})$. Assume $\beta \sim \mathcal{N}(\beta_0, \sigma^2 \mathbf{\Delta}^{-1})$ with $\beta_0 \in \mathbb{R}^p$ and $\mathbf{\Delta} \succ 0$ and a gamma prior on the error variance. Verify (i.e., work out the details of the derivation) that the posterior mean coincides with the generalized ridge estimator defined as: 
\begin{align*}
\hat{\beta} & =  (\mathbf{X}^{\top} \mathbf{X} + \mathbf{\Delta})^{-1} (\mathbf{X}^{\top} \mathbf{Y} + \mathbf{\Delta} \beta_0). 
\end{align*}
\end{question}



\begin{question} label{question.ridgeAR1penalty} \mbox{ }
\\
The ridge penalty may be interpreted as a multivariate normal prior on the regression coefficients: $\beta \sim \mathcal{N}(\mathbf{0}, \lambda^{-1} \mathbf{I}_{pp})$. Different priors may be considered. In case the covariates are spatially related in some sense (e.g. genomically), it may of interest to assume a first-order autoregressive prior: $\beta \sim \mathcal{N}(\mathbf{0}, \lambda^{-1} \SSigma_A)$, in which $\SSigma_A$ is a $p \times p$-correlation matrix with $(\SSigma_A)_{j_1, j_2} = \rho^{ | j_1 - j_2 | } $ for some correlation coefficient $\rho \in [0, 1)$. Hence,
\begin{align*}
\SSigma_A \, \, \, = \, \, \,
\left(
\begin{array}{cccc}
1      & \rho      & \ldots    &   \rho^{p-1}
\\
\rho    & 1         & \ldots      &  \rho^{p-2}
\\
\vdots  & \vdots      & \ddots      & \vdots
\\
\rho^{p-1}        &  \rho^{p-2}          & \ldots    & 1
\end{array}
\right).
\end{align*}

\begin{compactitem}
\item[*a)*] The penalized loss function associated with this AR(1) prior is:
\begin{align*}
\mathcal{L}(\beta; \lambda, \SSigma_A) & =  \| \mathbf{Y} - \mathbf{X} \beta \|_2^2 + \lambda \beta^{\top} \SSigma_A^{-1} \beta.
\end{align*}
Find the minimizer of this loss function.

\item[*b)*] What is the effect of $\rho$ on the ridge estimates? Contrast this to the effect of $\lambda$. Illustrate this on (simulated) data.

\item[*c)*] Instead of an AR(1) prior assume a prior with a uniform correlation between the elements of $\beta$. That is, replace $\SSigma_A$ by $\SSigma_U$, given by:
\begin{align*}
\SSigma_U \, \, \, = \, \, \, \left(
\begin{array}{ccc}
1      & \rho      & \rho
\\
\rho    & 1         & \rho
\\
\rho        &  \rho          &  1
\end{array}
\right)
\end{align*}
\\
Investigate (again on data) the effect of changing from the AR(1) to the uniform prior on the ridge regression estimates.
\end{compactitem}
\end{question}




========= Ridge logistic regression ========= label{sect.ridgeLogistic}
#  ======= Ridge logistic regression =======

Ridge penalized estimation is not limited to the standard linear regression model, but may be used to estimate (virtually) any model. Here we illustrate how it may be used to fit the logistic regression model. To this end we first recap this model and the (unpenalized) maximum likelihood estimation of its parameters. After which the model is estimated by means of ridge penalized maximum likelihood, which will turn out to be a relatively straightforward modification of unpenalized estimation.

======= Logistic regression =======

The logistic regression model explains a binary response variable (through some transformation) by a linear combination of a set of covariates (as in the linear regression model). Denote this response of the $i$-th sample by $Y_i$ with  $Y_i \in \{ 0, 1 \}$ for $i=1, \ldots, n$. The $n$-dimensional column vector $\mathbf{Y}$ stacks these $n$ responses. For each sample information on the $p$ explanatory variables $X_{i,1}, \ldots, X_{i,p}$ is available. In row vector form this information is denoted $\mathbf{X}_{i,\ast} =  (X_{i,1}, \ldots, X_{i,p})$. Or, in short, $\mathbf{X}_i$ when the context tolerates no confusion. The $(n \times p)$-dimensional matrix $\mathbf{X}$ aggregates these vectors, such that $\mathbf{X}_i$ is the $i$-th row vector.

The binary response cannot be modelled as in the linear model like $Y_i = \mathbf{X}_i \beta + \varepsilon_i$. With each element of $\mathbf{X}_i$ and $\beta$ assuming a value in $\mathbb{R}$, the linear predictor is not restricted to the domain of the response. This is resolved by modeling $p_i = P(Y_i = 1)$ instead. Still the linear predictor may exceed the domain of the response ($p_i \in [0,1]$). Hence, a transformation is applied to map $p_i$ to $\mathbb{R}$, the range of the linear predictor.
# original latex figure with scale=0.40, angle=0

FIGURE: [logistic_effectOfBeta0.eps, width=400 frac=1.0] Top row, left panel: the response curve for various choices of the intercept $\beta_0$. Top row, right panel: the response curve for various choices of the regression coefficent $\beta_1$. Bottom row, left panel: the responce curve for various choices of the link function. Bottom panel, right panel: observations, fits and their deviations. } label{fig.logisticRidge_modelAndFitIllustration
The transformation associated with the logistic regression model is the logarithm of the odds, with the odds defined as: $\mbox{*odds*} = P(\mbox{succes}) / P(\mbox{failure})  = p_i/ (1-p_i)$. The logistic model is then written as $\log[ p_i / (1-p_i)] = \mathbf{X}_i \beta$ for all $i$. Or, expressed in terms of the response:
\begin{align*}
p_i & =  P(Y_i = 1) \, \, \, = \, \, \, g^{-1}(\mathbf{X}_i; \beta) \, \, \, = \, \, \,\frac{\exp(\mathbf{X}_i \beta) }{1 + \exp(\mathbf{X}_i \beta) }.
\end{align*}
The function $g(\cdot; \cdot)$ is called the *link function*. It links the response to the explanatory variables. The one above is called the logistic link function. Or short, logit. The regression parameters have tangible interpretations. When the first covariate represents the intercept, i.e. $X_{i,j} = 1$ for all $i$, then $\beta_1$ determines where the link function equals a half when all other covariates fail to contribute to the linear predictor (i.e. where $P (Y_i = 1 \, | \, \mathbf{X}_{i}) = 0.5$ when $\mathbf{X}_{i} \beta = \beta_1$). This is illustrated in the top-left panel of Figure ref{fig.logisticRidge_modelAndFitIllustration} for various choices of the intercept. On the other hand, the regression parameters are directly related to the odds ratio: $\mbox{*odds ratio*} = \mbox{odds}(X_{i,j}+1) / \mbox{odds}(X_{i,j}) = \exp(\beta_j)$. Hence, the effect of a unit change in the $j$-th covariate on the odds ratio is $\exp(\beta_j)$ (see Figure ref{fig.logisticRidge_modelAndFitIllustration}, top-right panel). Other link functions (depicted in Figure ref{fig.logisticRidge_modelAndFitIllustration}, bottom-left panel) are common, e.g. the *probit*:  $p_i = \Phi_{0,1}(\mathbf{X}_i \beta)$; the *cloglog*: $p_i = \frac{1}{\pi} \arctan(\mathbf{X}_i \beta) + \frac{1}{2}$; the *Cauchit*: $p_i = \exp[ - \exp(\mathbf{X}_i \beta)]$. All these link function are invertible. Irrespective of the choice of the link function, the binary data are thus modelled as $Y_i \sim \mathcal{B}[g^{-1}(\mathbf{X}_i; \beta), 1]$. That is, as a single draw from the  Binomial distribution with success probability $g^{-1}(\mathbf{X}_i; \beta)$.


Let us now estimate the parameter of the logistic regression model by means of the maximum likelihood method. The likelihood of the experiment is then:
\begin{align*}
L(\mathbf{Y} \, | \, \mathbf{X}; \beta) & =  \prod_{i=1}^n \big[ P(Y_i = 1 \, | \, \mathbf{X}_i) \big]^{Y_i} \big[ P(Y_i = 0 \, | \, \mathbf{X}_i) \big]^{1-Y_i}.
\end{align*}
After taking the logarithm and some ready algebra, the log-likelihood is found to be:
\begin{align*}
\mathcal{L}(\mathbf{Y} \, | \, \mathbf{X}; \beta) & =  \sum_{i=1}^n  \big\{ Y_i \mathbf{X}_i \beta - \log [ 1 + \exp(\mathbf{X}_i \beta) ] \big\}.
\end{align*}
Differentiate the log-likelihood with respect to $\beta$, equate it zero, and obtain the estimating equation for $\beta$:

!bt

!bt
\begin{align} label{form:logisticRidge_estimatingEquationOfBeta}
\frac{\partial \mathcal{L}}{\partial \beta } & =  \sum_{i=1}^n \Big[ Y_i  - \frac{\exp(\mathbf{X}_i \beta)}{ 1 + \exp(\mathbf{X}_i \beta)} \Big] \mathbf{X}_i^{\top} \, \, \, = \, \, \, \mathbf{0}_p.
\end{align}
!et
!et
The ML estimate of $\beta$ strikes a (weighted by the $\mathbf{X}_i$) balance between observation and model. Put differently (and illustrated in the bottom-right panel of Figure ref{fig.logisticRidge_modelAndFitIllustration}), a curve is fit through data by minimizing the distance between them: at the ML estimate of $\beta$ a weighted average of their deviations is zero.

The maximum likelihood estimate of $\beta$ is evaluated by solving Equation (ref{form:logisticRidge_estimatingEquationOfBeta}) with respect to $\beta$ by means of the Newton-Raphson algorithm. The Newton-Raphson algorithm iteratively finds the zeros of a smooth enough function $f(\cdot)$. Let $x_0$ denote an initial guess of the zero. Then, approximate $f(\cdot)$ around $x_0$ by means of a first order Taylor series: $f(x) \approx x_0 + (x - x_0) \, (d f / d x) |_{x=x_0}$. Solve this for $x$ and obtain: $x = x_0 -  [ (d f / d x) |_{x=x_0} ]^{-1} f(x_0)$. Let $x_1$ be the solution for $x$, use this as the new guess and repeat the above until convergence. When the function $f(\cdot)$ has multiple arguments, is vector-valued and denoted by $\vec{\mathbf{f}}$, and the Taylor approximation becomes: $\vec{\mathbf{f}}(\mathbf{x}) \approx \mathbf{x}_0 + J \vec{\mathbf{f}} \big|_{\mathbf{x}=\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0)$ with
\begin{align*}
J \vec{\mathbf{f}}  = \left(
\begin{array}{llll}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \ldots & \frac{\partial f_1}{\partial x_p}
\\
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \ldots & \frac{\partial f_2}{\partial x_p}
\\
\vdots & \vdots & \ddots & \vdots
\\
\frac{\partial f_q}{\partial x_1} & \frac{\partial f_q}{\partial x_2} & \ldots & \frac{\partial f_q}{\partial x_p}
\end{array}
\right),
\end{align*}
the Jacobi matrix. An update of $x_0$ is now readily constructed by solving (the approximation for) $\vec{\mathbf{f}}(\mathbf{x}) = \mathbf{0}$ for $\mathbf{x}$.

When applied here to the maximum likelihood estimation of the regression parameter $\beta$ of the logistic regression model, the Newton-Raphson update is:
\begin{align*}
\hat{\beta}^{\mbox{{\scriptsize new}}} & =   \hat{\beta}^{\mbox{{\scriptsize old}}} -  \Big( \frac{\partial^2 \mathcal{L}}{\partial \beta \partial \beta^{\top}} \Big)^{-1} \Big|_{\beta =  \hat{\beta}^{\mbox{{\tiny old}}} } \, \, \frac{\partial \mathcal{L}}{\partial \beta } \Big|_{\beta =  \hat{\beta}^{\mbox{{\tiny old}}} }
\end{align*}
where the Hessian of the log-likelihood equals:
\begin{align*}
\frac{\partial^2 \mathcal{L}}{\partial \beta \partial \beta^{\top}} & =  - \sum_{i=1}^n \frac{\exp(\mathbf{X}_i \beta)}{ [1 + \exp(\mathbf{X}_i \beta)]^2} \mathbf{X}_i^{\top} \mathbf{X}_i.
\end{align*}
Iterative application of this updating formula converges to the ML estimate of $\beta$.

The Newton-Raphson algorithm is often reformulated as an iteratively re-weighted least squares algorithm. Hereto, first write the gradient and Hessian in matrix notation:
\begin{align*}
\frac{\partial \mathcal{L}}{\partial \beta } \, \, \, = \, \, \, \mathbf{X}^{\top} [\mathbf{Y} - \vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta)]
& \mbox{ and } &
\frac{\partial^2 \mathcal{L}}{\partial \beta \partial \beta^{\top}} \, \,  \, = \, \, \,  - \mathbf{X}^{\top} \mathbf{W} \mathbf{X},
\end{align*}
where $\vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta)  = [g^{-1}( \mathbf{X}_{1, \ast}; \beta), \ldots, g^{-1}( \mathbf{X}_{n, \ast}; \beta)]^{\top}$ with $g^{-1}(\cdot; \cdot) = \exp(\cdot; \cdot) / [1 + \exp(\cdot; \cdot)]$ and $\mathbf{W}$ diagonal with $(\mathbf{W})_{ii} = \exp(\mathbf{X}_i \hat{\beta}^{\mbox{{\scriptsize old}}}  ) [ 1 + \exp(\mathbf{X}_i \hat{\beta}^{\mbox{{\scriptsize old}}}  ) ]^{-2}$. The updating formula of the estimate then becomes:
\begin{align*}
\hat{\beta}^{\mbox{{\scriptsize new}}} & =    \hat{\beta}^{\mbox{{\scriptsize old}}} + (\mathbf{X}^{\top} \mathbf{W} \mathbf{X})^{-1}  \mathbf{X}^{\top} [\mathbf{Y} - \vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta^{\mbox{{\scriptsize old}}})]
\\
& =   (\mathbf{X}^{\top} \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{W} \{ \mathbf{X} \hat{\beta}^{\mbox{{\scriptsize old}}} +  \mathbf{W}^{-1} [\mathbf{Y} - \vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta^{\mbox{{\scriptsize old}}})] \}
\\
& =  (\mathbf{X}^{\top} \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{W} \mathbf{Z},
\end{align*}
where $\mathbf{Z} = \{ \mathbf{X} \hat{\beta}^{\mbox{{\scriptsize old}}} +  \mathbf{W}^{-1} [\mathbf{Y} - \vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta^{\mbox{{\scriptsize old}}})] \}$. The Newton-Raphson update is thus the solution to the following weighted least squares problem:
\begin{align*}
\hat{\beta}^{\mbox{{\scriptsize new}}} & =   \arg \min_{\beta}    (\mathbf{Z} - \mathbf{X} \beta)^{\top} \mathbf{W} (\mathbf{Z} - \mathbf{X} \beta).
\end{align*}
Effectively, at each iteration the *adjusted response* $\mathbf{Z}$ is regressed on the covariates that comprise $\mathbf{X}$. For more on logistic regression confer the monograph of cite{Hosm2013}.


======= Ridge estimation =======

High-dimensionally, the linear predictor $\mathbf{X} \beta$ may be uniquely defined, but the maximum likelihood estimate of the logistic regression parameter is not. Assume $p > n$ and an estimate $\hat{\beta}$ available. Due to the high-dimensionality, the null space of $\mathbf{X}$ is non-trivial. Hence, let $\ggamma \in \mbox{null}(\mbox{span}(\mathbf{X}))$. Then: $\mathbf{X} \hat{\beta} = \mathbf{X} \hat{\beta} + \mathbf{X} \ggamma = \mathbf{X} (\hat{\beta} + \ggamma)$. As the null space is a $p-n$-dimensional subspace, $\ggamma$ need not equal zero. Hence, an infinite number of estimates of the logistic regression parameter exists that yield the same log-likelihood. Augmentation of the loss function with a ridge penalty resolves the matter, as their sum is strictly concave in $\beta$ (not convex as a maximum rather than a minimum is sought here) and thereby has a unique maximum.

# original latex figure with scale=0.40, angle=0

FIGURE: [logisticPenLL2ridgePenalty.eps, width=400 frac=1.0] Top row, left panel: contour plot of the penalized log-likelihood of a logistic regression model with the ridge constraint (red line). Top row, right panel: the regularization paths of the ridge estimator of the logistic regression parameter. Bottom row, left panel: variance of the ridge estimator of the logistic regression parameter against the logarithm of the penalty parameter. Bottom panel, right panel: the predicted success probability versus the linear predictor for various choices of the penalty parameter. } label{fig.logisticRidge_effectOfPenalization
\mbox{ }


Ridge maximum likelihood estimates of the logistic model parameters are found by the maximization of the ridge penalized loglikelihood (cf. \citealt{Scha1984,LeCe1992}):
\begin{align*}
\mathcal{L}^{\mbox{{\tiny pen}}}(\mathbf{Y}, \mathbf{X}; \beta, \lambda) & =  \mathcal{L} (\mathbf{Y}, \mathbf{X}; \beta) - \tfrac{1}{2} \lambda \| \beta \|_2^2
\\
& =  \sum_{i=1}^n \big\{ Y_i \mathbf{X}_i \beta - \log [ 1 + \exp(\mathbf{X}_i \beta) ] \big\} - \tfrac{1}{2} \lambda \beta^{\top} \beta,
\end{align*}
where the second summand is the ridge penalty (the sum of the square of the elements of $\beta$) with $\lambda$ the penalty parameter. Note that as in Section ref{sect.constrainedEstimation} maximization of this penalized loss function can be reformulated as a constrained estimation problem. This is illustrated by the top left panel of Figure ref{fig.logisticRidge_effectOfPenalization}, which depicts the contours (black lines) of the log-likelihood and the spherical domain of the parameter (red line). The optimization of the above loss function proceeds, due to the differentiability of the penalty, fully analogous to the unpenalized case and uses the Newton-Raphson algorithm for solving the (penalized) estimating equation. Hence, the unpenalized ML estimation procedure is modified straightforwardly by replacing gradient and Hessian by their `penalized' counterparts:
\begin{align*}
\frac{\partial \mathcal{L}^{\mbox{{\tiny pen}}}}{\partial \beta } \, \, \,  = \, \, \, \frac{\partial \mathcal{L}}{\partial \beta } - \lambda \beta & \mbox{ and } & \frac{\partial^2 \mathcal{L}^{\mbox{{\tiny pen}}}}{\partial \beta \partial \beta^{\top}} \, \, \, = \, \, \, \frac{\partial^2 \mathcal{L}}{\partial \beta \partial \beta^{\top}} - \lambda \mathbf{I}_{pp}.
\end{align*}
With these at hand, the Newton-Raphson algorithm is (again) reformulated as an iteratively re-weighted least squares algorithm with the updating step changes accordingly to:
\begin{align*}
\hat{\beta}^{\mbox{{\scriptsize new}}} & =   \hat{\beta}^{\mbox{{\scriptsize old}}} + \mathbf{V}^{-1}  \{   \mathbf{X}^{\top} [\mathbf{Y} - \vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta^{\mbox{{\scriptsize old}}}) ] - \lambda \beta^{\mbox{{\scriptsize old}}} \}
\\
& =   \mathbf{V}^{-1} \mathbf{V} \hat{\beta}^{\mbox{{\scriptsize old}}} -  \lambda \mathbf{V}^{-1}  \hat{\beta}^{\mbox{{\scriptsize old}}} + \mathbf{V}^{-1} \mathbf{X}^{\top} \mathbf{W}   \mathbf{W}^{-1} [\mathbf{Y} - \vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta^{\mbox{{\scriptsize old}}})]
\\
& =  \mathbf{V}^{-1} \mathbf{X}^{\top} \mathbf{W}  \{ \mathbf{X} \hat{\beta}^{\mbox{{\scriptsize old}}} +  \mathbf{W}^{-1} [\mathbf{Y} - \vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta^{\mbox{{\scriptsize old}}})] \}
\\
& =  [\mathbf{X}^{\top} \mathbf{W}  \mathbf{X} +  \lambda \mathbf{I}_{pp} ]^{-1} \mathbf{X}^{\top} \mathbf{W}  \mathbf{Z},
\end{align*}
where $\mathbf{V} = \mathbf{X}^{\top} \mathbf{W}  \mathbf{X} + \lambda \mathbf{I}_{pp}$ and $\mathbf{W}$ and $\mathbf{Z}$ as before. Hence, use this to update the estimate of $\beta$ until convergence, which yields the desired ridge ML estimate.

Obviously, the ridge estimate of the logistic regression parameter tends to zero as $\lambda \rightarrow \infty$. Now consider a linear predictor with an intercept that is left unpenalized. When $\lambda$ tends to infinity, all regression coefficients but the intercept vanish. The intercept is left to model the success probability. Hence, in this case $\lim_{\lambda \rightarrow \infty} \hat{\beta}_0 (\lambda) = \log [ \tfrac{1}{n} \sum_{i=1}^n Y_i / \tfrac{1}{n} \sum_{i=1}^n (1-Y_i)]$.


# original latex figure with scale=0.40, angle=0

FIGURE: [logisticRidge_effectOnPrediction_lambda0.eps, width=400 frac=1.0] The realized design as scatter plot ($X_1$ vs $X_2$ overlayed by the success (\textcolor{red}{RED}) and failure regions (\textcolor{green}{GREEN}) for various choices of the penalty parameter: $\lambda = 0$ (top row, left panel), $\lambda = 10$ (top row, right panel) $\lambda = 40$ (bottom row, left panel), $\lambda = 100$ (bottom row, right panel).} label{fig.logisticRidge_effectOnPrediction



The effect of the ridge penalty on parameter estimates propagates to the predictor $\hat{p}_i$. The linear predictor of the linear regression model involving the ridge estimator $\mathbf{X}_i \hat{\beta}(\lambda)$ shrinks towards a common value for each $i$, leading to a scale difference between observation and predictor (as seen before in Section ref{sect.ridgeRegressionDataIllustration}). This behaviour transfers to the ridge logistic regression predictor, as is illustrated on simulated data. The dimension and sample size of these data are $p=2$ and $n=200$, respectively. The covariate data are drawn from the standard normal, while that of the response is sampled from a Bernoulli distribution with success probability $P(Y_i=1) = \exp(2 X_{i,1} - 2 X_{i,2}) / [ 1 + \exp(2 X_{i,1} - 2 X_{i,2})]$. The logistic regression model is estimated from these data by means of ridge penalized likelihood maximization with various choices of the penalty parameter. The bottom right plot in Figure ref{fig.logisticRidge_effectOfPenalization} shows the predicted success probability versus the linear predictor for various choices of the penalty parameter. Larger values of the penalty parameter $\lambda$ flatten the slope of this curve. Consequently, for larger $\lambda$ more excessive values of the covariates are needed to achieve the same predicted success probability as those obtained with smaller $\lambda$ at more moderate covariate values. The implications for the resulting classification may become clearer when studying the effect of the penalty parameter on the `failure' and `success regions' respectively defined by:
\\
\indent $\{(x_1, x_2) : P({\color{green}{\mathbf{Y=0}}} \, | \, X_1=x_1, X_2=x_2, \hat{\beta}(\lambda)) > 0.75 \}$,
\\
\indent $\{(x_1, x_2) : P({\color{red}{\mathbf{Y=1}}} \, | \, X_1=x_1, X_2=x_2, \hat{\beta}(\lambda)) > 0.75 \}$.
\\
This separates the design space in a light red (`failure') and light green (`success') domain. The white bar between them is the domain where samples cannot be classified with high enough certainty. As $\lambda$ grows, so does the white area that separates the failure and success regions. Hence, as stronger penalization shrinks the logistic regression parameter estimate towards zero, it produces a predictor that is  less outspoken in its class assignments.




======= Moments =======

The $1^{\mbox{{\tiny st}}}$ and $2^{\mbox{{\tiny nd}}}$ order moment of the ridge ML parameter of the logistic model may be approximated by the final update of the Newton-Raphson estimate. Assume the one-to-last update $\hat{\beta}^{\mbox{{\scriptsize old}}}$ to be non-random and proceed as for the ridge estimator of the linear regression model parameter to arrive at:
#  \begin{align*}
#  & & \hat{\beta}^{\mbox{{\scriptsize new}}} = [\mathbf{X}^{\top} \mathbf{W}  \mathbf{X} + 2 \lambda \mathbf{I}_{p \times p} ]^{-1} \mathbf{X}^{\top} \mathbf{W}  \mathbf{z}
#  \end{align*}
\begin{align*}
\mathbb{E} \big( \hat{\beta}^{\mbox{{\scriptsize new}}} \big) & =  [\mathbf{X}^{\top} \mathbf{W}  \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1} \mathbf{X}^{\top} \mathbf{W} \mathbb{E}( \mathbf{Z}),
\\
\mbox{Var} \big( \hat{\beta}^{\mbox{{\scriptsize new}}} \big) &  =    [\mathbf{X}^{\top} \mathbf{W}  \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1} \mathbf{X}^{\top} \mathbf{W} \big[  \mbox{Var} ( \mathbf{Z} ) \big] \mathbf{W}  \mathbf{X} [\mathbf{X}^{\top} \mathbf{W}  \mathbf{X} + \lambda \mathbf{I}_{pp} ]^{-1},
\end{align*}
with
\begin{align*}
\mathbb{E}(\mathbf{Z}) &  =  \{ \mathbf{X} \hat{\beta}^{\mbox{{\scriptsize old}}} +  \mathbf{W}^{-1} [\mathbb{E}(\mathbf{Y}) - \vec{\mathbf{g}}^{-1}( \mathbf{X}; \beta^{\mbox{{\scriptsize old}}})] \},
\\
\mbox{Var}(\mathbf{Z}) & =  \mathbf{W}^{-1} \mbox{Var}(\mathbf{Y}) \mathbf{W}^{-1} = \mathbf{W}^{-1},
\end{align*}
where the identity $\mbox{Var}(\mathbf{Y}) = \mathbf{W}$ follows from the variance of a Binomial distributed random variable. From these expressions similar properties as for the ridge ML estimate of the regression parameter of the linear model may be deduced. For instance, the ridge ML estimate of the logistic regression parameter converges to zero as the penalty parameter tends to infinity (confer the top right panel of Figure ref{fig.logisticRidge_effectOfPenalization}). Similarly, their variances vanish as $\lambda \rightarrow \infty$ (illustrated in the bottom left panel of Figure ref{fig.logisticRidge_effectOfPenalization}).

======= The Bayesian connection =======

All penalized estimators can be formulated as Bayesian estimators, including the ridge logistic estimator. In particular, ridge estimators correspond to Bayesian estimators with a multivariate normal prior on the regression coefficients. Thus, assume $\beta \sim \mathcal{N}(\mathbf{0}_p, \mathbf{\Delta}^{-1})$. The posterior distribution of $\beta$ then is:
\begin{align*}
f_{\beta}(\beta \, | \, \mathbf{Y}, \mathbf{X}) & \propto & \Big\{ \prod_{i=1}^n \big[ P(Y_i = 1 \, | \, \mathbf{X}_i) \big]^{Y_i} \big[ P(Y_i = 0 \, | \, \mathbf{X}_i) \big]^{1-Y_i}  \Big\} \exp( - \tfrac{1}{2} \beta \mathbf{\Delta} \beta).
\end{align*}
This does not coincide with any standard distribution. But, under appropriate conditions, the posterior distribution is asymptotically normal. This invites a (multivariate) normal approximation to the posterior distribution above. The Laplace's method provides (cf. \citealp{Bish2006}).

# original latex figure with scale=0.40, angle=0

FIGURE: [logisticRidge_laplace2posterior.eps, width=400 frac=1.0] Right panel: Laplace approximation to the posterior density of the Bayesian logistic regression parameter.} label{fig.logisticRidge_MSEandLaplace2posterior

Laplace's method *i)* centers the normal approximation at the mode of the posterior, and *ii)* chooses the covariance to match the curvature of the posterior at the mode. The posterior mode is the location of the maximum of the posterior distribution. The location of this maximum coincides with that of the logarithm of the posterior. The latter is the log-likelihood augmented with a ridge penalty. Hence, the posterior mode, which is taken as the mean of the approximating Gaussian, coincides
with the ridge logistic estimator. For the covariance of the approximating Gaussian, the logarithm of the posterior is approximated by a second order Taylor series around the posterior mode and limited to second order terms:
\begin{align*}
\log[f_{\beta}(\beta \, | \, \mathbf{Y}, \mathbf{X})] & \propto &
\left. \log[f_{\beta}(\beta \, | \, \mathbf{Y}, \mathbf{X})]
\right|_{\beta = \hat{\beta}_{\mbox{{\tiny MAP}}}}
\\
&  &  + \tfrac{1}{2} (\beta - \hat{\beta}_{\mbox{{\tiny MAP}}})^{\top} \left. \frac{\partial^2}{\partial \beta \partial \beta^{\top}}
\log[f_{\beta}(\beta \, | \, \mathbf{Y}, \mathbf{X})]
 \right|_{\beta = \hat{\beta}_{\mbox{{\tiny MAP}}}} (\beta - \hat{\beta}_{\mbox{{\tiny MAP}}})^{\top},
\end{align*}
in which the first order term cancels as the derivative of $f_{\beta}(\beta \, | \, \mathbf{Y}, \mathbf{X})$ with respect to $\beta$ vanishes at the posterior mode - its maximum. Take the exponential of this approximation and match its arguments to that of a multivariate Gaussian $\exp[-\tfrac{1}{2} (\beta - \mmu_{\beta})^{\top} \mathbf{\Sigma}_{\beta}^{-1} (\beta - \mmu_{\beta})]$. The covariance of the sought Gaussian approximation is thus the inverse of the Hessian of the negative penalized log-likelihood. Put together the posterior is approximated by:
\begin{align*}
\beta \,  | \, \mathbf{Y}, \mathbf{X} \sim \mathcal{N} \Big( \hat{\beta}_{\mbox{{\tiny MAP}}}, \Big\{ \mathbf{\Delta} + \sum_{i=1}^n \frac{\exp(\mathbf{X}_i \beta)}{ [1+ \exp(\mathbf{X}_i \beta) ]^2 } \mathbf{X}_i \mathbf{X}_i^{\top} \Big\}^{-1} \Big).
\end{align*}
The Gaussian approximation is convenient but need not be good. Fortunately, the Bernstein-Von Mises Theorem \citep{VdVa2000} tells it is very accurate when the model is regular, the prior smooth, and the sample size sufficiently large. The quality of the approximation for an artificial example data set is shown in Figure ref{fig.logisticRidge_MSEandLaplace2posterior}.

======= Penalty parameter selection =======

As before the penalty parameter may be chosen through $K$-fold cross-validation. For the $K=n$ case cite{Meij2013} describe a computationally efficient approximation of the leave-one-out cross-validated loglikelihood. It is based on the exact evaluation of the LOOCV loss, discussed in Section ref{subsect.crossvalidation}, that avoided resampling. The approach of cite{Meij2013} hinges upon the first-order Taylor expansion of the left-out penalized loglikelihood of the left-out estimate  $\hat{\beta}_{-i} (\lambda)$ around $\hat{\beta} (\lambda)$, which yields an approximation of the former:
\begin{align*}
\hat{\beta}_{-i} (\lambda) & \approx  \hat{\beta} (\lambda) - 
\left( \left. \frac{\partial^2 \mathcal{L}_{-i}^{\mbox{{\tiny pen}}}}{\partial \beta \partial \beta^{\top}} \right|_{\beta = \hat{\beta}(\lambda)} \right)^{-1} \left. \frac{\partial \mathcal{L}_{-i}^{\mbox{{\tiny pen}}}}{\partial \beta } \right|_{\beta = \hat{\beta}(\lambda)}
\\
& =  \hat{\beta} (\lambda) + (\mathbf{X}_{- i, \ast}^{\top} \mathbf{W}_{-i, -i} \mathbf{X}_{- i, \ast} + \lambda \mathbf{I}_{pp})^{-1} \{ \mathbf{X}_{- i, \ast}^{\top} [\mathbf{Y}_{-i} - \vec{\mathbf{g}}^{-1}(\mathbf{X}_{-i, \ast}; \hat{\beta}(\lambda))] - \lambda \hat{\beta}(\lambda) \}.
\end{align*}
This approximation involves the inverse of a $p \times p$ dimensional matrix, which amounts to the evaluation of $n$ such inverses for the LOOCV loss. As in Section ref{subsect.crossvalidation} this may be avoided. Rewrite both the gradient and the Hessian of the left-out loglikelihood in the approximation of the preceding display:
\begin{align*}
\mathbf{X}_{-i, \ast}^{\top} \{ \mathbf{Y}_{-i} - \vec{\mathbf{g}}^{-1}(\mathbf{X}_{-i, \ast}; \hat{\beta}(\lambda)]\} - \lambda \hat{\beta}(\lambda)
& =  \mathbf{X}^{\top} \{ \mathbf{Y} - \vec{\mathbf{g}}^{-1}[\mathbf{X}; \hat{\beta}(\lambda)]\} - \lambda \hat{\beta}(\lambda)
- \mathbf{X}_{i, \ast}^{\top} \{Y_{i} - g^{-1}[\mathbf{X}_{i, \ast}; \hat{\beta}(\lambda)]\}
\\
& =  - \mathbf{X}_{i, \ast}^{\top} \{Y_{i} - g^{-1}[\mathbf{X}_{i, \ast}; \hat{\beta}(\lambda)]\}
\end{align*}
and
\begin{align*}
(\mathbf{X}_{- i, \ast}^{\top} \mathbf{W}_{-i, -i} \mathbf{X}_{- i, \ast} + \lambda \mathbf{I}_{pp})^{-1} & =  (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} + \mathbf{W}_{ii} (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top}  
\\
& & \qquad \qquad \qquad \qquad \qquad \quad [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} \mathbf{X}_{i, \ast} (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1},
\end{align*}
where the Woodbury identity has been used and now $\mathbf{H}_{ii}(\lambda) = \mathbf{W}_{ii} \mathbf{X}_{i, \ast}(\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1}  \mathbf{X}_{i, \ast}^{\top}$. Substitute both in the approximation of the left-out ridge logistic regression estimator and manipulate as in Section ref{subsect.crossvalidation} to obtain:
\begin{align*}
\hat{\beta}_{- i}(\lambda) & \approx  \hat{\beta}(\lambda) - (\mathbf{X}^{\top} \mathbf{W} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}_{i, \ast}^{\top} [ 1 - \mathbf{H}_{ii}(\lambda)]^{-1} [ Y_i - g^{-1}(\mathbf{X}_{i, \ast}; \hat{\beta}(\lambda)) ].
\end{align*}
Hence, the leave-one-out cross-validated loglikelihood $\sum_{i=1}^n \mathcal{L} [Y_i \, | \, \mathbf{X}_{i, \ast}, \hat{\beta}_{-i}(\lambda)]$ can now be evaluated by means of a single inverse of a $p \times p$ dimensional matrix and some matrix multiplications. For the performance of this approximation in terms of accuracy and speed confer cite{Meij2013}.









