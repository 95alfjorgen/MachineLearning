TITLE: Data Analysis and Machine Learning: Introduction and Representing data
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University
DATE: today


!split
===== Introduction =====

Statistics, data science and machine learning form important fields of
research in modern science.  They describe how to learn and make
predictions from data, as well allowing us to extract important
correlations about physical process and the underlying laws of motion
in large data sets. The latter, big data sets, appear
frequently in essentially all disciplines, from the traditional Science,
Technology, Mathematics and Engineering fields to Life Science, Law, education research, 
the  Humanities and
the Social Sciences. It has become more and more common to see
research projects on big data in for example the Social
Sciences where extracting patterns from complicated survey data is one of many research directions. 
Having a solid grasp of data analysis and machine learning
is thus becoming central to scientific computing in many
fields, and competences and skills within the fields of machine learning
and scientific computing are nowadays strongly requested by many
potential employers. The latter cannot be overstated, familiarity with
machine learning has almost become a prerequisite for many of the most
exciting employment opportunities, whether they are in bioinformatics,
life science, physics or finance, in the private or the public
sector. This author has had several students or met students who have
been hired recently based on their skills and competences in
scientific computing and data science, often with marginal knowledge
of machine learning.

Machine learning is a subfield of computer science, and is closely
related to computational statistics.  It evolved from the study of
pattern recognition in artificial intelligence (AI) research, and has
made contributions to AI tasks like computer vision, natural language
processing and speech recognition. 
Machine learning represents the
science of giving computers the ability to learn without being
explicitly programmed.  The idea is that there exist generic
algorithms which can be used to find patterns in a broad class of data
sets without having to write code specifically for each problem. The
algorithm will build its own logic based on the data.  

Machine learning is an extremely rich field, in spite of its young age. The
increases we have seen during the last three decades in computational
capabilities have been followed by developments of methods and
techniques for analyzing and handling large date sets, relying heavily
on statistics, computer science and mathematics.  The field is rather
new and developing rapidly. Popular software packages written in
Python for machine learning like "Scikit-learn":"http://scikit-learn.org/stable/", "Tensorflow":"https://www.tensorflow.org/",
"PyTorch":"http://pytorch.org/" and "Keras":"https://keras.io/", all freely available at their respective GitHub sites,
encompass communities of developers in the thousands or more. And the number
of code developers and contributors keeps increasing. Not all the
algorithms and methods can be given a rigorous mathematical
justification, opening up thereby large rooms for experimenting
and trial and error and thereby exciting new developments. 
However, a solid command of linear algebra, multivariate theory, 
probability theory, statistical data analysis,
understanding errors and Monte Carlo methods are central elements in a proper understanding of many of 
algorithms and methods we will discuss. 

!split 
===== Learning outcomes =====

These lectures aim at giving you an overview of central aspects of
statistical data analysis as well as some of the central algorithms
used in machine learning.  We will introduce a variety of central
algorithms and methods essential for studies of data analysis and
machine learning. 

Hands-on projects and experimenting with data and algorithms plays a central role in
these lectures, and our hope is, through the various
projects and exercies, to  expose you to fundamental
research problems in these fields, with the aim to reproduce state of
the art scientific results. You  will learn to develop and
structure large codes for studying these systems, get acquainted with
computing facilities and learn to handle large scientific projects. A
good scientific and ethical conduct is emphasized throughout the
course. More specifically, you will

o learn about basic data analysis, Bayesian statistics, Monte Carlo methods, data optimization and machine learning;
o be capable of extending the acquired knowledge to other systems and cases;
o Have an understanding of central algorithms used in data analysis and machine learning;
o Gain knowledge of central aspects of Monte Carlo methods, Markov chains, Gibbs samplers and their possible applications, from numerical integration to simulation of stock markets;
o Understand methods for regression and classification;
o Learn about neural network, genetic algorithms and Boltzmann machines;
o Work on numerical projects to illustrate the theory. The projects play a central role and you are expected to know modern programming languages like Python or C++, in addition to a basic knowledge of linear algebra (typically taught during the first one or two years of undergraduate studies).

There are several topics we will cover here, spanning from a
statistical data analysis and its basic concepts such expectation
values, variance, covariance, correlation functions and errors, via
well-known probability distribution functions like uniform
distribution, the binomial distribution, the Poisson distribution and
simple and multivariate normal distributions to central elements of
Bayesian statistics and modeling. We will also remind the reader about
central elements from linear algebra and standard methods based on
linear algebra used to fit functions such Cubic splines and gradient
methods for data optimization and the Singular-value decomposition and
least square methods for parameterizing data.

We will also cover Monte Carlo methods, Markov chains, well-known
algorithms for sampling stochastic events like the Metropolis-Hastings
and Gibbs sampling methods. An important aspect of all our
calculations is a proper estimation of errors. Here we will also
discuss famous resampling techniques like the blocking, bootstrapping
and jackknife methods.

The second part of the material covers several algorithms used in
machine learning.





!split
===== Types of Machine Learning =====


The approaches to machine learning are many, but are often split into two main categories. 
In *supervised learning* we know the answer to a problem,
and let the computer deduce the logic behind it. On the other hand, *unsupervised learning*
is a method for finding patterns and relationship in data sets without any prior knowledge of the system.
Some authours also operate with a third category, namely *reinforcement learning*. This is a paradigm 
of learning inspired by behavioral psychology, where learning is achieved by trial-and-error, 
solely from rewards and punishment.

Another way to categorize machine learning tasks is to consider the desired output of a system.
Some of the most common tasks are:

  * Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.

  * Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.

  * Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.


The methods we cover have three main topics in common, irrespective of
whether we deal with supervised or unsupervised learning. The first
ingredient is normally our data set, the second is a model which is
normally a function of some parameters. The last ingredient is a
so-called _cost_ function which allows us to present an estimate on
how good our model is in reproducing the data it is supposed to train.

Here we will build our machine learning approach on elements of the
statistical foundation discussed above, with elements from data
analysis, stochastic processes etc. We will discuss the following
machine learning algorithms

o Linear regression and its variants, in essence polynomial regression
o Decision tree algorithms, from simpler to more complex ones
o Nearest neighbors models
o Bayesian statistics and regression
o Support vector machines and finally various variants of
o Artifical neural networks and deep learning


Before we proceed however, there are several practicalities with data
analysis and software tools we would like to present. These tools will
help us in our understanding of various machine learning algorithms.

Our emphasis here is on understanding the mathematical aspects of
different algorithms, however, where possible we will emphasize the
importance of using available software. We start thus with a hands-on
and top-down approach machine learning. The aim is thus to start with
relevant data and use these to introduce statistical data analysis
concepts and machine learning algorithms before we delve into the
algorithms themselves. The examples we will use start with a simple
third-order polynomial with random noise added, and using the Python
software package "Scikit-learn":"http://scikit-learn.org/stable/" we
will introduce various machine learning algorithm s to make fits of
the data data and predictions. We move thereafter to more interesting
cases such as the simulation of financial transactions or disease
models. These are examples where we can easily set up the data and
then use machine learning algorithms using included in for example _scikit-learn_. Another model we
will consider is the so-called Ising model. Here we will use this
model to produce data for selected spin configurations and attempt to classify the data. 
Finally, our last example consists of economic data from the OECD. 


!split
===== Software and needed installations =====

We will make intensive use of python as programming language and the myriad of available libraries. 
Furthermore, you will find IPython/Jupyter notebooks invaluable in your work. 
You can run _R_ codes in the Jupyter/IPython notebooks, with the immediate benefit of visualizing your data.


If you have Python installed (we recommend Python3) and you feel pretty familiar with installing different packages, 
we recommend that you install the following Python packages via _pip_ as
o pip install numpy scipy matplotlib ipython scikit-learn mglearn sympy pandas pillow
For Python3, replace _pip_ with _pip3_.

For OSX users we recommend also, after having installed Xcode, to install _brew_. Brew allows 
for a seamless installation of additional software via for example
o brew install python3

For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution
you can use _pip_ as well and simply install Python as 
o sudo apt-get install python3  (or python for pyhton2.7)
etc etc. 

!split
===== Python installers =====
If you don't want to perform these operations separately, we recommend two widely used distrubutions which set up 
all relevant dependencies for Python, namely
o "Anaconda":"https://docs.anaconda.com/" Anaconda is an open source distribution of the Python and R programming languages for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. Package versions are managed by the package management system _conda_
o "Enthought canopy":"https://www.enthought.com/product/canopy/"  is a Python distribution for scientific and analytic computing distribution and analysis environment, available for free and under a commercial license.


!split
===== Installing R, C++, cython or Julia =====

You will also find it convenient to utilize R. 
Jupyter/Ipython notebook allows you run _R_ code interactively in your browser. The software library _R_ is 
tuned to statistically analysis and allows for an easy usage of the tools we will discuss in these texts.

To install _R_ with Jupyter notebook "following the link here":"https://mpacer.org/maths/r-kernel-for-ipython-notebook"



!split
===== Installing R, C++, cython or Julia =====


For the C++ aficionados, Jupyter/IPython notebook allows you also to install C++ and run codes written in this language 
interactively in the browser. Since we will emphasize writing many of the algorithms yourself, you can thus opt for
either Python or C++ as programming languages. 

To add more entropy, _cython_ can also be used when running your notebooks. It means that Python with the Jupyter/IPython notebook 
setup allows you to integrate widely popular softwares and tools for scientific computing. With its versatility, 
including symbolic operations, Python offers a unique computational environment. Your Jupyter/IPython notebook 
can easily be converted into a nicely rendered _PDF_ file or a Latex file for further processing. For example, convert to latex as
!bc pycod
jupyter nbconvert filename.ipynb --to latex
!ec

If you use the light mark-up language _doconce_ you can convert a standard ascii text file into various HTML 
formats, ipython notebooks, latex files, pdf files etc. 



!split
===== Introduction to Jupyter notebook and available tools =====

!bblock
!bc pycod 
import numpy as np
import matplotlib.pyplot as plt
from scipy import sparse
import pandas as pd
from IPython.display import display
eye = np.eye(4)
print(eye)
sparse_mtx = sparse.csr_matrix(eye)
print(sparse_mtx)
x = np.linspace(-10,10,100)
y = np.sin(x)
plt.plot(x,y,marker='x')
plt.show()
data = {'Name': ["John", "Anna", "Peter", "Linda"], 'Location': ["Nairobi", "Napoli", "London", "Buenos Aires"], 'Age':[51, 21, 34, 45]}
data_pandas = pd.DataFrame(data)
display(data_pandas)
!ec
!eblock

!split
===== Representing data, more examples  =====
!bblock
!bc pycod 
import numpy as np
import matplotlib.pyplot as plt
from scipy import sparse
import pandas as pd
from IPython.display import display
import mglearn
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
x, y = mglearn.datasets.make_wave(n_samples=100)
line = np.linspace(-3,3,1000,endpoint=False).reshape(-1,1)
reg = DecisionTreeRegressor(min_samples_split=3).fit(x,y)
plt.plot(line, reg.predict(line), label="decision tree")
regline = LinearRegression().fit(x,y)
plt.plot(line, regline.predict(line), label= "Linear Regression")
plt.show()
!ec
!eblock


!split
===== Simple regression model =====
Add info about the equations
!bc pycod
# Importing various packages
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt

x = 2*np.random.rand(100,1)
y = 4+3*x+np.random.randn(100,1)

xb = np.c_[np.ones((100,1)), x]
theta = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)
xnew = np.array([[0],[2]])
xbnew = np.c_[np.ones((2,1)), xnew]
ypredict = xbnew.dot(theta)

plt.plot(xnew, ypredict, "r-")
plt.plot(x, y ,'ro')
plt.axis([0,2.0,0, 15.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Linear Regression')
plt.show()

!ec






!split
===== Simple regression model, now using _scikit-learn_ =====
Add info about the equations
!bc pycod
# Importing various packages
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = 2*np.random.rand(100,1)
y = 4+3*x+np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
xnew = np.array([[0],[2]])
ypredict = linreg.predict(xnew)

plt.plot(xnew, ypredict, "r-")
plt.plot(x, y ,'ro')
plt.axis([0,2.0,0, 15.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Random numbers ')
plt.show()
!ec


!split
===== Simple regression model with gradient descent  =====
Add info about the equations, play around with different learning rates
!bc pycod
# Importing various packages
from math import exp, sqrt
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt

x = 2*np.random.rand(100,1)
y = 4+3*x+np.random.randn(100,1)

xb = np.c_[np.ones((100,1)), x]
theta_linreg = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)
print(theta_linreg)
theta = np.random.randn(2,1)

eta = 0.1
Niterations = 1000
m = 100

for iter in range(Niterations):
    gradients = 2.0/m*xb.T.dot(xb.dot(theta)-y)
    theta -= eta*gradients

print(theta)
xnew = np.array([[0],[2]])
xbnew = np.c_[np.ones((2,1)), xnew]
ypredict = xbnew.dot(theta)
ypredict2 = xbnew.dot(theta_linreg)
plt.plot(xnew, ypredict, "r-")
plt.plot(xnew, ypredict2, "b-")
plt.plot(x, y ,'ro')
plt.axis([0,2.0,0, 15.0])
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.title(r'Random numbers ')
plt.show()
!ec


!split
===== Simple regression model with stochastic gradient descent  =====
Add info about the equations, play around with different learning rates
!bc pycod
# Importing various packages
from math import exp, sqrt
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import SGDRegressor

x = 2*np.random.rand(100,1)
y = 4+3*x+np.random.randn(100,1)

xb = np.c_[np.ones((100,1)), x]
theta_linreg = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)
print(theta_linreg)
sgdreg = SGDRegressor(n_iter = 50, penalty=None, eta0=0.1)
sgdreg.fit(x,y.ravel())
print(sgdreg.intercept_, sgdreg.coef_)

!ec

!split
===== Polynomial regression =====
!bc pycod 

!ec



!split
===== Predator-Prey model from ecology =====

!bblock
The population dynamics of a simple predator-prey system is a
classical example shown in many biology textbooks when ecological
systems are discussed. The system contains all elements of the
scientific method:

 * The set up of a specific hypothesis combined with
 * the experimental methods needed (one can study existing data or perform experiments)
 * analyzing and interpreting the data and performing further experiments if needed
 * trying to extract general behaviors and extract eventual laws or patterns
 * develop mathematical relations for the uncovered regularities/laws and test these by per forming new experiments
!eblock


!split
===== Case study from Hudson bay =====

!bblock
Lots of data about populations of hares and lynx collected from furs in Hudson Bay, Canada, are available. It is known that the populations oscillate. Why?
Here we start by

 o plotting the data
 o derive a simple model for the population dynamics
 o (fitting parameters in the model to the data)
 o using the model predict the evolution other predator-pray systems
!eblock

!split
===== Hudson bay data =====


!bblock

Most mammalian predators rely on a variety of prey, which complicates mathematical modeling; however, a few predators have become highly specialized and seek almost exclusively a single prey species. An example of this simplified predator-prey interaction is seen in Canadian northern forests, where the populations of the lynx and the snowshoe hare are intertwined in a life and death struggle.

One reason that this particular system has been so extensively studied is that the Hudson Bay company kept careful records of all furs from the early 1800s into the 1900s. The records for the furs collected by the Hudson Bay company showed distinct oscillations (approximately 12 year periods), suggesting that these species caused almost periodic fluctuations of each other's populations. The table here shows data from 1900 to 1920.


|------------------------------------------------------|
|      Year         |     Hares (x1000)  | Lynx (x1000)|
|---------l-----------------------r--------------r------|
|  1900 | 30.0  | 4.0  |
|  1901 | 47.2  | 6.1 |
|  1902 | 70.2  | 9.8  |
|  1903 | 77.4  | 35.2  |
|  1904 | 36.3  | 59.4  |
|  1905 | 20.6  | 41.7  |
|  1906 | 18.1  | 19.0  |
|  1907 | 21.4  | 13.0  |
|  1908 | 22.0  | 8.3  |
|  1909 | 25.4  | 9.1  |
|  1910 | 27.1  | 7.4 |
|  1911 | 40.3  | 8.0  |
|  1912 | 57  | 12.3  |
|  1913 | 76.6  | 19.5  |
|  1914 | 52.3  | 45.7  |
|  1915 | 19.5  | 51.1  |
|  1916 | 11.2  | 29.7  |
|  1917 | 7.6  | 15.8  |
|  1918 | 14.6  | 9.7  |
|  1919 | 16.2  | 10.1  |
|  1920 | 24.7  | 8.6  |
|------------------------------------------------------|

!eblock



!split
===== Plotting the data =====

!bblock
@@@CODE src/plot_Hudson.py
!eblock

!split
===== Hares and lynx in Hudson bay from 1900 to 1920 =====

FIGURE: [fig/Hudson_Bay_data, width=700 frac=0.9]



!split
===== Why now create a computer model for the hare and lynx populations? =====
!bblock
We see from the plot that there are indeed fluctuations.
We would like to create a mathematical model that explains these
population fluctuations. Ecologists have predicted that in a simple
predator-prey system that a rise in prey population is followed (with
a lag) by a rise in the predator population. When the predator
population is sufficiently high, then the prey population begins
dropping. After the prey population falls, then the predator
population falls, which allows the prey population to recover and
complete one cycle of this interaction. Thus, we see that
qualitatively oscillations occur. Can a mathematical model predict
this? What causes cycles to slow or speed up? What affects the
amplitude of the oscillation or do you expect to see the oscillations
damp to a stable equilibrium? The models tend to ignore factors like
climate and other complicating factors. How significant are these?

 * We see oscillations in the data
 * What causes cycles to slow or speed up?
 * What affects the amplitude of the oscillation or do you expect to see the oscillations damp to a stable equilibrium?
 * With a model we can better *understand the data*
 * More important: we can understand the ecology dynamics of
   predator-pray populations
!eblock


!split
===== The traditional (top-down) approach =====

!bblock
The classical way (in all books) is to present the Lotka-Volterra equations:

!bt
\begin{align*}
\frac{dH}{dt} &= H(a - b L)\\
\frac{dL}{dt} &= - L(d - c  H)
\end{align*}
!et

Here,

 * $H$ is the number of preys
 * $L$ the number of predators
 * $a$, $b$, $d$, $c$ are parameters

Most books quickly establish the model and then use considerable space on
discussing the qualitative properties of this *nonlinear system of
ODEs* (which cannot be solved)
!eblock



!split
===== Basic  mathematics notation =====
!bblock
 * Time points: $t_0,t_1,\ldots,t_m$
 * Uniform distribution of time points: $t_n=n\Delta t$
 * $H^n$: population of hares at time $t_n$
 * $L^n$: population of lynx at time $t_n$
 * We want to model the changes in populations, $\Delta H=H^{n+1}-H^n$
   and $\Delta L=L^{n+1}-L^n$ during a general time interval $[t_{n+1},t_n]$
   of length $\Delta t=t_{n+1}-t_n$
!eblock

!split
===== Basic dynamics of the population of hares =====

!bblock
The population of hares evolves due to births and deaths exactly as a bacteria population:

!bt
\[
\Delta H = a \Delta t H^n
\]
!et
However, hares have an additional loss in the population because
they are eaten by lynx.
All the hares and lynx can form
$H\cdot L$ pairs in total. When such pairs meet during a time
interval $\Delta t$, there is some
small probablity that the lynx will eat the hare.
So in fraction $b\Delta t HL$, the lynx eat hares. This
loss of hares must be accounted for. Subtracted in the equation for hares:

!bt
\[ \Delta H = a\Delta t H^n - b \Delta t H^nL^n\]
!et
!eblock

!split
===== Basic dynamics of the population of lynx =====

!bblock
We assume that the primary growth for the lynx population depends on sufficient food for raising lynx kittens, which implies an adequate source of nutrients from predation on hares. Thus, the growth of the lynx population does not only depend of how many lynx there are, but on how many hares they can eat.
In a time interval $\Delta t HL$ hares and lynx can meet, and in a
fraction $b\Delta t HL$ the lynx eats the hare. All of this does not
contribute to the growth of lynx, again just a fraction of
$b\Delta t HL$ that we write as
$d\Delta t HL$. In addition, lynx die just as in the population
dynamics with one isolated animal population, leading to a loss
$-c\Delta t L$.
!eblock

!bblock
The accounting of lynx then looks like
!bt
\[ \Delta L = d\Delta t H^nL^n - c\Delta t L^n\]
!et
!eblock

!split
===== Evolution equations =====

!bblock
By writing up the definition of $\Delta H$ and $\Delta L$, and putting
all assumed known terms $H^n$ and $L^n$ on the right-hand side, we have

!bt
\[ H^{n+1} = H^n + a\Delta t H^n - b\Delta t H^n L^n \]
!et

!bt
\[ L^{n+1} = L^n + d\Delta t H^nL^n - c\Delta t L^n \]
!et

Note:

 * These equations are ready to be implemented!
 * But to start, we need $H^0$ and $L^0$ <linebreak>
   (which we can get from the data)
 * We also need values for $a$, $b$, $d$, $c$
!eblock

!split
===== Adapt the model to the Hudson Bay case =====

!bblock
 * As always, models tend to be general - as here, applicable
   to ``all'' predator-pray systems
 * The critical issue is whether the *interaction* between hares and lynx
   is sufficiently well modeled by $\hbox{const}HL$
 * The parameters $a$, $b$, $d$, and $c$ must be
   estimated from data
 * Measure time in years
 * $t_0=1900$, $t_m=1920$
!eblock

!split
===== The program =====

!bblock
@@@CODE src/Hudson_Bay.py
!eblock

!split
===== The plot =====

FIGURE: [fig/Hudson_Bay_sim, width=700 frac=0.9]

If we perform a least-square fitting, we can find optimal values for the parameters $a$, $b$, $d$, $c$. The optimal parameters are $a=0.4807$, $b=0.02482$, $d=0.9272$ and $c=0.02756$. These parameters result in a slightly modified initial conditions, namely $H(0) = 34.91$ and $L(0)=3.857$. With these parameters we are now ready to solve the equations and plot these data together with the experimental values.


!split
===== Linear regression in Python  =====
!bblock
!bc pycod
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import display
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor


data = np.loadtxt('src/Hudson_Bay.csv', delimiter=',', skiprows=1)
x = data[:,0]
y = data[:,1]
line = np.linspace(1900,1920,1000,endpoint=False).reshape(-1,1)
reg = DecisionTreeRegressor(min_samples_split=3).fit(x.reshape(-1,1),y.reshape(-1,1))
plt.plot(line, reg.predict(line), label="decision tree")
regline = LinearRegression().fit(x.reshape(-1,1),y.reshape(-1,1))
plt.plot(line, regline.predict(line), label= "Linear Regression")
plt.plot(x, y, label= "Linear Regression")
plt.show()
!ec
!eblock



!split
===== Linear Least squares in R  =====
!bblock
!bc r
HudsonBay = read.csv("src/Hudson_Bay.csv",header=T)
fix(HudsonBay)
dim(HudsonBay)
names(HudsonBay)
plot(HudsonBay$Year, HudsonBay$Hares..x1000.)
attach(HudsonBay)
plot(Year, Hares..x1000.)
plot(Year, Hares..x1000., col="red", varwidth=T, xlab="Years", ylab="Haresx 1000")
summary(HudsonBay)
summary(Hares..x1000.)
library(MASS)
library(ISLR)
scatter.smooth(x=Year, y = Hares..x1000.)
linearMod = lm(Hares..x1000. ~ Year)
print(linearMod)
summary(linearMod)
plot(linearMod)
confint(linearMod)
predict(linearMod,data.frame(Year=c(1910,1914,1920)),interval="confidence")
!ec
!eblock

!split
===== Non-Linear Least squares in R  =====
!bblock
!bc r
set.seed(1485)
len = 24
x = runif(len)
y = x^3+rnorm(len, 0,0.06)
ds = data.frame(x = x, y = y)
str(ds)
plot( y ~ x, main ="Known cubic with noise")
s  = seq(0,1,length =100)
lines(s, s^3, lty =2, col ="green")
m = nls(y ~ I(x^power), data = ds, start = list(power=1), trace = T)
class(m)
summary(m)
power = round(summary(m)$coefficients[1], 3)
power.se = round(summary(m)$coefficients[2], 3)
plot(y ~ x, main = "Fitted power model", sub = "Blue: fit; green: known")
s = seq(0, 1, length = 100)
lines(s, s^3, lty = 2, col = "green")
lines(s, predict(m, list(x = s)), lty = 1, col = "blue")
text(0, 0.5, paste("y =x^ (", power, " +/- ", power.se, ")", sep = ""), pos = 4)
!ec
!eblock



!split
===== Example: ecoli lab experiment =====


!bnotice Typical pattern:
The population grows faster and faster. "Why? Is there an underlying (general) mechanism":"http://www.zo.utexas.edu/courses/Thoc/PopGrowth.html"?
!enotice
!bblock
 o Cells divide after $T$ seconds on average (one generation)
 o $2N$ celles divide into twice as many new cells $\Delta N$ in a time
   interval $\Delta t$ as $N$ cells would: $\Delta N \propto N$
 o $N$ cells result in twice as many new individuals $\Delta N$ in
   time $2\Delta t$ as in time $\Delta t$: $\Delta N \propto\Delta t$
 o Same proportionality wrt death (repeat reasoning)
 o Proposed model: $\Delta N = b\Delta t N - d\Delta tN$ for some unknown
   constants $b$ (births) and $d$ (deaths)
 o Describe evolution in discrete time: $t_n=n\Delta t$
 o Program-friendly notation: $N$ at $t_n$ is $N^n$
 o Math model: $N^{n+1} = N^n + r\Delta t\, N$ (with $\ r=b-d$)
 o Program model: `N[n+1] = N[n] + r*dt*N[n]`
!eblock

!split
===== The program =====

!bblock
Let us solve the difference equation in as simple way as possible,
just to train some programming: $r=1.5$, $N^0=1$, $\Delta t=0.5$

@@@CODE ../../Programs/LifeScience/diffeq.py
!eblock

% if FORMAT != 'ipynb':
!split
===== The output =====

!bc
N[1]=1.2
N[2]=1.6
N[3]=2.0
N[4]=2.4
N[5]=3.1
N[6]=3.8
N[7]=4.8
N[8]=6.0
N[9]=7.5
N[10]=9.3
N[11]=11.6
N[12]=14.6
N[13]=18.2
N[14]=22.7
N[15]=28.4
N[16]=35.5
N[17]=44.4
N[18]=55.5
N[19]=69.4
N[20]=86.7
!ec
% endif

!split
===== Parameter estimation =====

!bblock
 * We do not know $r$
 * How can we estimate $r$ from data?

We can use the difference equation with the experimental data

!bt
\[ N^{n+1} = N^n + r\Delta t N^n\]
!et
Say $N^{n+1}$ and $N^n$ are known from data, solve wrt $r$:

!bt
\[ r = \frac{N^{n+1}-N^n}{N^n\Delta t} \]
!et

Use experimental data in the fraction, say $t_1=600$, $t_2=1200$,
$N^1=140$, $N^2=250$: $r=0.0013$.
!eblock


!split
===== A program relevant for the biological problem =====

# exact r = 0.000694

!bblock
@@@CODE ../../Programs/LifeScience/ecoli.py

Change `r` in the program and play around to make a better fit!
!eblock




!split
===== Simulating financial transcations =====

The aim here is to simulate financial transactions among financial agents
using Monte Carlo methods. The final goal is to extract a distribution of income  as function
of the income $m$.   From Pareto's work ("V.~Pareto, 1897":"http://www.institutcoppet.org/2012/05/08/cours-deconomie-politique-1896-de-vilfredo-pareto") it is known from empirical studies
that the higher end of the distribution of money follows a distribution 
!bt
\[
w_m\propto m^{-1-\alpha},
\]
!et
with $\alpha\in [1,2]$. We will here follow the analysis made by "Patriarca and collaborators":"http://www.sciencedirect.com/science/article/pii/S0378437104004327". 

Here we will study numerically the relation between the micro-dynamic relations among financial 
agents and the  resulting macroscopic money distribution.

We assume we have $N$ agents that exchange money in pairs $(i,j)$. We assume also that all agents
start with the same amount of money $m_0 > 0$. At a given 'time step', we choose randomly a pair
of agents $(i,j)$ and let a transaction take place. This means that agent $i$'s money $m_i$ changes
to $m_i'$ and similarly we have $m_j\rightarrow m_j'$. 
Money is conserved during a transaction, meaning that
!bt
\begin{equation}
  m_i+m_j=m_i'+m_j'.
  label{eq:conserve}
\end{equation}
!et
The change is done via a random reassignement (a random number) $\epsilon$, meaning that

!bt
\begin{equation*}
m_i' = \epsilon(m_i+m_j),
\end{equation*}
!et
leading to

!bt
\begin{equation*}
m_j'= (1-\epsilon)(m_i+m_j).
\end{equation*}
!et
The number $\epsilon$ is extracted from a uniform distribution.
In this simple model, no agents are left with a debt, that is $m\ge 0$.
Due to the conservation law above, one can show that the system relaxes toward an equilibrium
state given by a Gibbs distribution

!bt
\begin{equation*}
w_m=\beta \exp{(-\beta m)},
\end{equation*}
!et
with

!bt
\begin{equation*}
\beta = \frac{1}{\langle m\rangle},
\end{equation*}
!et
and $\langle m\rangle=\sum_i m_i/N=m_0$, the average money.
It means that after equilibrium has been reached that the majority of agents is left with a small
number of money, while the number of richest agents, those with $m$ larger than a specific value $m'$,
exponentially decreases with $m'$.

We assume that we have $N=500$ agents.   In each simulation, we need a sufficiently large number of transactions, say $10^7$. Our aim is find the final equilibrium distribution $w_m$. In order to do that we would need
several runs of the above simulations, at least $10^3-10^4$ runs (experiments).




=== Simulation of Transactions   ===
Our task is to first set up an algorithm which simulates the above transactions with an initial
  amount $m_0$.
  The challenge here is to figure out a Monte Carlo  simulation  based on the
  above equations.
  You will in particular need to make an algorithm which sets up a histogram as function of $m$.
  This histogram contains the number of times a value $m$ is registered and represents
  $w_m\Delta m$. You will need to set up a value for the interval $\Delta m$  (typically $0.01-0.05$).
  That means you need to account for the number of times you register an income in the interval
  $m,m+\Delta m$. The number of times you register this income, represents the value that enters the histogram.
  You will also need to find a criterion for when the equilibrium situation has been reached.

!bc pycod
#!/usr/bin/env python
import numpy as np
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import random

# initialize the rng with a seed
random.seed()
# Hard coding of input parameters
Agents  = 500
MCcounts = 1000
Transactions = 100000
startMoney = 1.0
Lambda = 0.0
FinancialAgents = startMoney*np.ones(Agents)
for i in range (1, MCcounts, 1):
    for j in range (1, Transactions, 1):
        agent_i = int(Agents*random.random())
        agent_j = int(Agents*random.random())
        epsilon = random.random()
        if agent_i != agent_j:
           m1 = Lambda*FinancialAgents[agent_i] + (1-Lambda)*epsilon*(FinancialAgents[agent_i] + FinancialAgents[agent_j])
           m2 = Lambda*FinancialAgents[agent_j] + (1-Lambda)*(1-epsilon)*(FinancialAgents[agent_i] + FinancialAgents[agent_j])
           FinancialAgents[agent_i] = m1
           FinancialAgents[agent_j] = m2

# the histogram of the data
n, bins, patches = plt.hist(FinancialAgents, 50, facecolor='green')

plt.xlabel('$x$')
plt.ylabel('Distribution of wealth')
plt.title(r'Money')
plt.axis([0, 10, 0, 500])
plt.grid(True)
plt.show()

!ec


We can then change our model to allow for a saving criterion, meaning that the agents save
  a fraction $\lambda$ of the money they have before the transaction is made. The final distribution will then no longer be given by Gibbs distribution. It could also include a taxation on financial transactions.

  The conservation law of Eq. (ref{eq:conserve}) holds, but the money to be shared in a transaction between
  agent $i$ and agent $j$ is now $(1-\lambda)(m_i+m_j)$. This means that we have

!bt
\begin{equation*}
  m_i' = \lambda m_i+\epsilon(1-\lambda)(m_i+m_j),
  \end{equation*}
!et
  and

!bt
\begin{equation*}
  m_j' = \lambda m_j+(1-\epsilon)(1-\lambda)(m_i+m_j),
  \end{equation*}
!et
  which can be written as

!bt
\begin{equation*}
  m_i'=m_i+\delta m
  \end{equation*}
!et
  and

!bt
\begin{equation*}
  m_j'=m_j-\delta m,
  \end{equation*}
!et
  with

!bt
\begin{equation*}
  \delta m=(1-\lambda)(\epsilon m_j-(1-\epsilon)m_i),
  \end{equation*}
!et
  showing how money is conserved during a transaction.
  Select values of $\lambda =0.25,0.5$ and $\lambda=0.9$ and try to extract the corresponding
  equilibrium distributions and compare these with the Gibbs distribution. Comment your results.
Extract a parametrization of the above curves, see for example "Patriarca and collaborators":"http://www.sciencedirect.com/science/article/pii/S0378437104004327" and see if you can parametrize the high-end tails of the distributions in terms of power laws. Comment your results.

In the studies above the agents were selected randomly, irrespective of whether we allowed for
saving or not during a transaction. What is often observed is that various agents tend to make preferences for for whom to interact with. We will now study the evolution of the distribution of wealth $w_m$ by assuming that there is a likelihood 
!bt
\[
p_{ij} \propto \vert m_i-m_j\vert^{-\alpha},
\]
!et
for an interaction between agents $i$ and $j$ with respective wealths $m_i$ and $m_j$. The parameter  $\alpha >  0$. For $\alpha=0$ we recover our model from part 5a). 
Perform the same analysis as previously with $N=500$ as well as with $N=1000$ agents and study the distribution of wealth for $\alpha =0.5$, $\alpha =1.0$, $\alpha =1.5$ and $\alpha =2.0$. 
You should try to reproduce Figure 1 of "Goswami and Sen":"http://www.sciencedirect.com/science/article/pii/S0378437114006967". 
Extract the tail of the distribution and see if it follows a Pareto distribution
!bt
\[
w_m\propto m^{-1-\alpha}. 
\]
!et
What happens if $\alpha \gg 1$?

Perform the analysis with and without a saving $\lambda$ on each transaction and comment your results. 
We add to the previous probability the possibility that two agents who interact have performed similar transactions earlier. That is, in addition to being financially close, we assume that the likelihood for interacting increases if two agents have interacted earlier. 
We add this feature by modifying the previous likelihood to
!bt
\[
p_{ij} \propto \vert m_i-m_j\vert^{-\alpha}\left(c_{ij}+1\right)^{\gamma},
\]
!et
where $c_{ij}$ represents the number of  previous interactions that have taken place between $i$ and $j$. The factor $1$ is added in order to ensure that if they have not interacted earlier they can still interact.  Perform similar studies as above with $N=1000$, $\alpha=1.0$ and $\alpha=2.0$ using $\gamma = 0.0, 1.0, 2.0, 3.0$ and $4.0$. Plot the wealth distributions for these cases and try to extract eventual power law tails with and without a saving $\lambda$ in each transaction. Comment your results and compare them with figures 5 and 6 of "Goswami and Sen":"http://www.sciencedirect.com/science/article/pii/S0378437114006967".  

!split
===== Particle in one dimension an velocity distribution =====
!bc pycod 
# Program to test the Metropolis algorithm with one particle at given temp in one dimension
import numpy as np
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import random
from math import sqrt, exp, log
# initialize the rng with a seed
random.seed()
# Hard coding of input parameters
MCcycles = 100000
Temperature = 2.0
beta = 1./Temperature
InitialVelocity = -2.0
CurrentVelocity = InitialVelocity
Energy = 0.5*InitialVelocity*InitialVelocity
VelocityRange = 10*sqrt(Temperature)
VelocityStep = 2*VelocityRange/10.
AverageEnergy = Energy
AverageEnergy2 = Energy*Energy
VelocityValues = np.zeros(MCcycles)
# The Monte Carlo sampling with Metropolis starts here
for i in range (1, MCcycles, 1):
    TrialVelocity = CurrentVelocity + (2.0*random.random() - 1.0)*VelocityStep
    EnergyChange = 0.5*(TrialVelocity*TrialVelocity -CurrentVelocity*CurrentVelocity);
    if random.random() <= exp(-beta*EnergyChange):
        CurrentVelocity = TrialVelocity
        Energy += EnergyChange
        VelocityValues[i] = CurrentVelocity
    AverageEnergy += Energy
    AverageEnergy2 += Energy*Energy
#Final averages
AverageEnergy = AverageEnergy/MCcycles
AverageEnergy2 = AverageEnergy2/MCcycles
Variance = AverageEnergy2 - AverageEnergy*AverageEnergy
print(AverageEnergy, Variance)
n, bins, patches = plt.hist(VelocityValues, 400, facecolor='green')

plt.xlabel('$v$')
plt.ylabel('Velocity distribution P(v)')
plt.title(r'Velocity histogram at $k_BT=2$')
plt.axis([-5, 5, 0, 600])
plt.grid(True)
plt.show()

!ec
